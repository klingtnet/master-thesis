# Introduction

## Scope of this Thesis

It's not required for the reader to have any prior knowledge of soft- and hardware tools used in music production environments.
However, a basic knowledge of signal processing and some familiarity with computer programming is beneficial to grasp the presented concepts.

## Objectives

<!-- TODO
Be more specific
-->

The objective of this thesis is to show how to build a real time audio software application at the example of an audio synthesizer with frequency modulated oscillators.

The amount of problems that arise when building such an application is quite large.
This includes timing and synchronization problems that occur because the computations made are required to be finished in short time windows.
Computations must be as fast as possible to ensure a responsive feedback for the player of the instrument, and are needed to avoid sound distortion when the generated signals could not be delivered fast enough to the sound card buffer.

Not every programming language is equally suitable for the development of real time applications, thus it is explained why Rust was chosen as the implementation language.
Libraries for the MIDI and Open Sound Control protocol were implemented and released as open-source software.
Both protocols are supported by the application and are used to play polyphonic\footnote{More than one at a time.} notes or remote control all parameters of the synthesizer.
Common synthesizer architectures and signal generation methods are discussed and evaluated.

Furthermore, sound shaping techniques and effects are presented in detail.
The precautions taken to reduce the amount of signal distortion are shown as well.
Lastly, an outlook on possible optimizations and enhancements, as well as a summary of the achieved results is given.

## Overview

<!-- TODO: provide a quick summary of each chapter. -->

Chapter 2 gives an overview about common synthesis concepts and evaluates why Rust was chosen as the implementation language.

## Why Rust?

Real time audio applications must finish their signal processing in tight time bounds to ensure that the sound card can output a continuous audio stream.
Missing such a bound will result in unpleasant sound glitches, and---in the worst case---renders a whole song recording useless.
Additionally, the time bounds must be as tight as possible to reduce the latency between the user input (playing a note) and the output of the calculated signal in the speaker.
Therefore, (memory-)managed languages like [Java](https://en.wikipedia.org/wiki/Java_(programming_language)), [Go](https://golang.org) or [C#](https://en.wikipedia.org/wiki/C_Sharp_(programming_language)) that use a garbage collector, which can produce non deterministic program stops while examining the state of variable references, will not receive any further consideration.
Interpreted languages like [Python](https://www.python.org/) or [Ruby](https://www.ruby-lang.org/en/) face the same problems as compiled managed languages by also having increased runtime costs.
Audio application development in those languages is still possible but commonly requires to write the signal processing as C modules and interface with them through the languages foreign function interface (FFI) (e.g. pyo \cite{pyo} uses this approach). 
Unmanaged languages like [C](https://en.wikipedia.org/wiki/C_(programming_language)) and [C++](https://en.wikipedia.org/wiki/C%2B%2B) offer the control over memory that is needed to make reliable claims about the runtime behaviour while avoiding the overhead of an additional runtime environment.
This comes with a downside in memory safety and introduces a whole new class of possible bugs compared to managed languages.
Those bugs are very likely to cause *undefined behavior* or to crash the program. Unfortunately, they are also very hard to debug.

One of the major selling points of [Rust](https://rust-lang.org) is guaranteed memory safety *without* garbage collection and *data race freedom* while providing the same level of control over memory as C/C++.
These goals are achieved through a variety of concepts like ownership, lifetimes and borrowing to know at compile when memory can be safely freed, and to enforce that there is only a single mutable access to any variable at any given time in the run of the program.
Explaining these concepts is outside the scope of this thesis and the official [Rust Book](https://doc.rust-lang.org/book/ownership.html) \cite{RustBook} does a great job doing this in detail, hence, this is left as an exercise for the interested reader.
The [Max Planck Institute for Software Systems](http://plv.mpi-sws.org/) has started the \cite{RustBelt} RustBelt research project in 2015 to develop formal foundations for the Rust programming language.
One of the main goals of the research group is to formally investigate if the claims made about data-race freedom, memory and type safety actually hold.
<!-- Furthermore, they want to provide formal tools for verifying safe encapsulation of `unsafe` code blocks. -->
As of the time of writing there is no evidence that the claims are untrue, therefore it's assumed that they're actually hold.

In the following I will describe a number of language features that were crucial in making the decision to use Rust for the implementation of the synthesizer:  

Generics (parametric polymorphism)
:	Generics allow to write a single implementation of a function or method so that it can be used with a number of different types.
	A *generic* type in the declaration is replaced by an *type parameter*.
	Such a parameter can also be constrained so it can only be replaced with types that implement certain traits.

	[Example](https://is.gd/Ob3eqr):
	```rust
	fn sum<T>(a: T, b: T) -> T::Output {
		a + b
	}

	println!(sum(3+4));
	```

Cross compilation
:	Cross compilation is the ability to compile statically linked binaries for different target platforms on a single machine, e.g. one can compile a windows executable on a Linux machine.

Abstractions without overhead
:	The Rust compiler will create a separate implementation for each type that a generic function is used with in the code.
	This is called *monomorphization* and allows to statically dispatch calls to generic functions but comes with the cost of slightly increased binary sizes.
	Iterators and other high-level features will be compiled down to simpler constructs like *for* loops.

FFI
:	A foreign function interface allows the calling of functions written in other languages.
	Rust's FFI allows to easily call functions from a variety of languages but, as a systems language, it focuses on C as the default target.
	This is of great value because most audio APIs and I/O libraries, e.g. [portaudio](http://www.portaudio.com/) or [libsoundio](http://libsound.io/), are C libraries.
	The opposite directions is possible as well, because a Rust function can be defined to use the C ABI (application binary interface) so it can be called from C or other languages that provide a C FFI.

Algebraic data types and pattern matching
:	An algebraic data type is formed by combination of other types.
	The term *algebraic* refers to the operations that are used to form the type, which is either *sum*, i.e., the type is $A$ or $B$, or *product*, i.e., it is a combination of $A$ and $B$, where $A$ and $B$ are different types.
	Rust's `enum`s are formed as *sum* types whereas `tuple`s are formed by a *product* of types.
	A classic example for the former kind is the `Option` type, which can either be `None` or have `Some` value of type $T$:

	```rust
	enum Option<T> {
	    None,
		Some(T),
	}
	```

	Destructuring the values of such types is done through *pattern matching* in Rust.
	In contrast to C-like enums, the compiler knows how much variants such an algebraic data type has and won't compile if they aren't matched exhaustively, i.e. there is at least one unmatched variant of the type.

Iterators
:	Rust provides conventional `for` loops but additionally has *iterators*.
	An iterator is something that provides a sequence of things where you can loop over.
	The great benefit of using iterators over index based access in traditional `for` loops is, that it avoids the, often unintended, access of elements that are outside the iterators range.
	In addition, iterators provide a vast amount of methods to combine them, filter elements, perform map-reduce like operations and in many cases even parallelize the computation \cite{Rayon}.

	Example: $\sum_{x\,\in\,v_1, v_2\;\wedge\;x > 3}$

	```rust
	let v1 = [1, 2, 3];
	let v2 = [3, 4, 5];
	let sum = v1.iter().chain(v2.iter())
				.filter(|val| **val > 3)
				.fold(|acc, val| acc + val, 0);
	assert_eq!(sum, 9)
	```

Toolchain
:	Rust has a standard package manager and build tool called *Cargo*, that is typically used to manage the dependencies of a project but also to run tests and benchmarks.

# User Interface

Interaction with the software synthesizer is done through its *user interface*.
The user interface serves three senses, which are sight, touch and hearing.
This section concentrates on the first two, the *visual* and the *haptic* component.
However, a synthesizer can be played solely through haptic controls and audio feedback.
Visual indicators for the synthesizer's parameters, e.g. through a display, are convenient, but not necessary for the playing musician.

The application supports the two most common musical control signal protocols, *MIDI* and *Open Sound Control* (OSC).
Adding MIDI support is highly beneficial, because it enables the synthesizer to be played with any---of the vast amount of available---MIDI hardware controllers (\cref{midi:edirol} shows an example of such a device).
On the other hand, Open Sound Control software like [liine's Lemur](https://liine.net/de/products/lemur/) \cite{LiineLemur} provides an editor to create or customize a software defined controller for a multi-touch device like a smartphone or tablet.

![Edirol PCR-300 MIDI controller keyboard\label{midi:edirol}](imgs/pcr_300_angle.jpg){ width=100% }

<!-- TODO
- OSC: custom controllers, more modern approach
- User interface must work in real time (latency)

MIDI support is a basic requirement for nearly all types of music hard- and software because it allows to use controller hardware
User input, if it is a played note or a parameter change, must be processed in real-time to give the player direct feedback without perceived *latency* (see \nameref{latency}).
-->

## MIDI

The Musical Instrument Digital Interface (MIDI) specification stipulates a hardware interconnection scheme and a method for data communication \cite[p.~972]{Roads:CMT}, but only the protocol specification is of interest for this work.
Most modern MIDI hardware is connected via USB anyway.
The *MIDI 1.0 Specification* \cite{MIDI10} provides a high level description of the MIDI protocol:

> The Musical Instrument Digital Interface (MIDI) protocol provides a standardized and efficient means of conveying musical performance information as electronic data.
> MIDI information is transmitted in \enquote{MIDI messages}, which can be thought of as instructions which tell a music synthesizer how to play a piece of music. \cite[p.~1]{MIDI10}

Transmitting *control data* is the purpose of the MIDI protocol, and not, like it is sometimes confused, to transmit audio data[^midi-audio].
Control data can be thought of as the press of a key, turning a knob, or an instruction to change the clock speed of a song.

[^midi-audio]: It is possible to transmit audio data over MIDI by using *System Exclusive* (SysEx) messages, but this can not be done in real-time and is often used to replace or update samples or wavetables in hardware synthesizers.

The work on the MIDI specification began in 1981 by a consortium of Japanese and American synthesizer manufacturers, the MIDI Manufacturers Association (MMA).
In August 1983, the version 1.0 was published \cite[p.~974]{Roads:CMT}.
This year, 2016, the MMA established The MIDI Association (TMA).
The TMA should support the global community of MIDI users and establish [midi.org](https://www.midi.org/) \cite{MidiOrg} as a central source for information about MIDI.
MIDI is used in nearly every music electronic device, like synthesizers, samplers, digital audio effects, and music software, due to its simple protocol structure and long time of existence.

### MIDI Protocol

The MIDI protocol specifies a standard transmission rate of 31.250 baud.
This may seem like an unusual choice for the transmission rate, but it was derived by dividing the common clock frequency of 1MHz by 32 \cite[p.~976]{Roads:CMT}.
It uses an 8b/10b encoding, i.e. 8 bits of data are transmitted as a 10 bit word.
A data byte is enclosed by a start- and stop bit which in turn results in the 10 bit encoding.
Asynchronous serial communication is used to transfer *MIDI messages*, thus the start and stop bit.

A MIDI message is composed of a *status byte* which is followed by up to two[^sysex] *data byte*s.
Both types are differentiated by their most significant bit (MSB), `1` for status- and `0` for data bytes.
Consequently, the usable payload size is reduced to 7 bit, in other words, values can range from 0 to 127.

[^sysex]: System Exclusive (SysEx) messages can be made up of more than two data-bytes, in fact they are build by a sequence of data bytes followed by an *End of Exclusive* (EOX) message to mark the end of the stream. This type of message does not contain any musical control data, in general it is used to upload binary data, like firmware updates or samples, to a MIDI device.

\Cref{fig:midi-status} shows the structure of a status byte.
The message type is denoted by three bits (`T`) and the remaining four bits are used to denote the channel number (`C`), hence there are sixteen different channels.
MIDI channels allow to route different logical streams over one physical MIDI connection, e.g. to reach a different, daisy-chained MIDI device or to control different timbres of a multitimbral synthesizer.

\begin{figure}
    \centering
	$0\quad\underbrace{T\quad{}T\quad{}T}_{\text{message type}}\quad\overbrace{C\quad{}C\quad{}C\quad{}C}^{\text{channel number}}$
	\caption{Structure of a MIDI status byte.}
	\label{fig:midi-status}
\end{figure}

MIDI messages are divided in two categories, *channel* and *system* messages.
Only the latter contain musical control information and therefore are of interest for this thesis.
\Cref{fig:midi-classification} illustrates the classification, status byte values are shown as edge labels where \texttt{x} illustrates *don't care*.
*Channel Mode Messages* define the instrument's response to Voice Messages \cite[p.~36]{MIDI10}, i.e. listen on all channels (omni mode), or switch between mono- and polyphonic mode (multiple simultaneous voices).

\begin{figure}
	\centering
	\begin{tikzpicture}
[
	grow = right,
	every node/.style = {font=\footnotesize},
	sibling distance = 1cm,
	level distance = 1cm,
	level 1/.style = {sibling distance = 5cm, level distance = 3cm},
	level 2/.style = {sibling distance = 2cm, level distance = 5cm},
	sloped
]
\node[msg]{MIDI Message}
	child { node [msg]{System Message}
		child { node [msg]{System Exclusive\\Message}
			edge from parent node [above] {\texttt{F0}}
		}
		child { node [msg]{System Common\\Message}
			edge from parent node [above] {\texttt{F1-F7}}
		}
		child { node [msg]{System Real-Time\\Message}
			edge from parent node [above] {\texttt{F8-FF}}
		}
		edge from parent node [above] {\texttt{F0-FF}}
	}
	child { node [msg]{Channel Message}
		child { node [msg]{Channel Voice\\Message}
			edge from parent node [above] {\texttt{8x-Ex}}
		}
		child { node [msg]{Channel Mode\\Message}
			edge from parent node [above] {\texttt{Bx}}
			edge from parent node [below] {\texttt{Data1: 79-7F}}
		}
		edge from parent node [above] {\texttt{8x-Ex}}
	}
	;
	\end{tikzpicture}
	\caption{Classification of MIDI messages.}
	\label{fig:midi-classification}
\end{figure}

### MIDI Pitch

\begin{table}[]
    \centering
    \caption{Types of MIDI Voice Messages.}
    \label{tab:voice-messages}
    \begin{tabular}{@{}llllp{3.5cm}@{}}
    \toprule
    Type                    & Status & Data1      & Data2    & Description                                                \\ \midrule
    Note-Off                & 8x     & Key \#     & Velocity & Key released.                                              \\
    Note-On                 & 9x     & Key \#     & Velocity & Key press from a triggering device.                        \\
    Polyphonic Key Pressure & Ax     & Key \#     & Pressure & Aftertouch event.                                          \\
    Control Change          & Bx     & Ctrl. \#	  & Value    & Move of a controller other than a key (e.g. Knob, Slider). \\
    Program Change          & Cx     & Program \# & ---      & Instruction to load specified preset.                      \\
    Channel Pressure        & Dx     & Pressure   & ---      & Aftertouch event.                                          \\
    Pitch Bend              & Ex     & MSB        & LSB      & Altering pitch (14-bit resolution).                        \\ \bottomrule
    \end{tabular}
\end{table}

\Cref{tab:voice-messages} gives an overview of the types on voice messages.
Corresponding Note-On and Off messages do not necessarily follow one after another, therefore, to relate associated messages, pitch information is contained in the Note-Off as well.
Pitch is encoded as a 7-bit value in note messages, hence there is a range of 128 pitches or about 10 octaves.
MIDI's pitch representation was designed with an *chromatic western music scale* in mind.
A *chromatic scale* has 12 pitches per octave with one semitone difference between each pitch, that is a ratio of $2^{1/12}$ between successive notes.
An interval of one octave is equivalent to a doubling or halving (in the negative case) in frequency.
Instruments in *western music* are usually *equal-tempered*, i.e. all semitones have the same size.
MIDI pitches are considered to be equal-tempered and range from C0 (*c* in the lowest octave) to a G10 (*g* in the 10th octave).
Middle C, pitch number 60 (C5), is used as reference.

\begin{align}
\begin{split} % creates only a single equation number
	f &= f_\text{tune}\cdot 2^{\displaystyle\left(p-p_\text{ref}\right)/12}\\
    p &= p_\text{ref}+12\cdot\log_2(f/f_\text{tune})
\end{split}
\label{eq:midi-pitch}
\end{align}

\Cref{eq:midi-pitch} shows how to calculate the frequency $f$ for a given MIDI pitch $p$, and vice versa, where $f_\text{tune}$ is the tuning frequency and $p_\text{ref}$ is the reference pitch number.
Musical instruments are commonly tuned to the *Concert A*, the note A above middle C or MIDI pitch 69.
The default tuning of Concert A is 440 Hz \cite{ISO16:1975}.
The following example shows how to calculate the frequency for middle C by using the *Concert A* tuned to 440 Hz as reference pitch in \cref{eq:midi-pitch}:

$$
\begin{aligned}
f	&= 440\,\text{Hz}\cdot 2^{(60-69)/12}\\
	&= 440\,\text{Hz}\cdot 2^{-9/12}\\
	&\approx 261.626\,\text{Hz}
\end{aligned}
$$

### Timing Problems

Playing two or more notes a the same time, i.e. playing a chord, can lead to timing problems because of MIDI's low bandwidth.

$$
\begin{aligned}
t_\text{Note-On}	&= 3\cdot\left(31250\,\frac{\text{bit}}{\text{s}}/10\,{\text{bit}}\right)^{-1}\\
					&= 0.0096\,\text{s} = 0.96\,\text{ms}
\end{aligned}
$$

The time to transmit a single note-on event $t_\text{Note-On}$ takes $\approx 1\,\text{ms}$, this means that the last transmitted note of an $n$-key chord arrives with $n \cdot 0.96\,\text{ms}$ delay, e.g. the last note of a pentachord (5 keys) will be received $5 \cdot 0.96\,\text{ms} = 4.8\,\text{ms}$ later than the first one.
This may result in a *comb filter*[^comb-filter] like distortion of the synthesized chord sound.

[^comb-filter]: A comb filter adds a delayed copy of the signal to itself causing addition or subtraction in the signal. The filters frequency response shows regularly spaced notches, might resemble the shape of a comb.

## Open Sound Control

The *UC Berkeley Center for New Music and Audio Technology* (CNMAT) originally developed, and continues to research, Open Sound Control.
In 2002, OSC's 1.0 specification was released.
It provides the following definition \cite{OSC:10}:

> Open Sound Control (OSC) is an open, transport-independent, message-based protocol developed for communication among computers, sound synthesizers, and other multimedia devices.

The protocol is not limited to being used with audio or multimedia devices, however, it is often used as a high-speed network replacement for MIDI.
Referring to OSC as a *message format* is more accurate, since error-handling, synchronization or negotiation methods are not specified.
Therefore, OSC can be compared to formats like JSON or XML.
A draft of the OSC 1.1 specification was published in a 2009 paper \cite{OSC:11} only adding minor, backward compatible changes.
UDP is often used as the transport layer to avoid the time required to establish a connection by TCP's three-way handshake.
A connection less transport is sufficient because OSC sender and receiver are almost always in physical proximity and connected through the same LAN.

### OSC Data Types

\begin{table}[]
    \centering
    \caption{Overview of OSC 1.0 and 1.1 data types.}
    \label{tab:osc-data-types}
    \begin{tabular}{@{}cp{7.5cm}cc@{}}
    \toprule
    Tag     & Description                                                      & 1.0 Required & 1.1 Required \\ \midrule
    i       & 32-bit two's complement integer                                  & ✔            & ✔            \\
    f       & IEEE 754 single precision (32-bit)                               & ✔            & ✔            \\
    s       & null-terminated sequence of ASCII characters                     & ✔            & ✔            \\
    b       & binary blob with size information                                & ✔            & ✔            \\
    h       & 64-bit big-endian two's complement integer                       &              &              \\
    t       & OSC-timetag in NTP format                                        &              & ✔            \\
    d       & IEEE 754 double precision (64-bit)                               &              &              \\
    S       & alternate string type                                            &              &              \\
    c       & ASCII character                                                  &              &              \\
    r       & RGBA color (8-bit per channel)                                   &              &              \\
    m       & 4 byte MIDI message. From MSB to LSB: port, status, data1, data2 &              &              \\
    T/F     & true, false boolean values                                       &              & ✔            \\
    N       & Nil                                                              &              & ✔            \\
    I       & Infinitum (1.0)/Impulse(1.1) used as event trigger               &              & ✔            \\
    {[},{]} & Array delimiters                                                 &              &              \\ \bottomrule
    \end{tabular}
\end{table}

An overview of the predefined data types for both, OSC 1.0 and 1.1, is shown in \cref{tab:osc-data-types}.
The byte order of OSC's integer, float and timetags is big-endian.
OSC's unit of transmission is called *OSC Packet*.
The EBNF grammar for OSC packets is described by \cref{osc:grammar}.
Fields of an OSC packet have to be aligned to multiples of 4-byte and are zero-padded, thus the size of such a packet is also a multiple of four.
The packets contents can either be an *OSC Message* or *OSC Bundle*.
An OSC message starts with an *address pattern*  followed by zero or more *arguments* to be applied to the *OSC Method* matched by the pattern.
Address pattern can contain basic regular expression with single-/multi-character `?/*` wildcards, range `[A-Z]` and list matches `{foo, bar}`, hence multiple OSC Methods can be triggered with a single OSC Message.
An OSC Receiver's[^osc-naming] address space forms a tree structure with branch nodes called *OSC Containers* and leaves are named *OSC Methods*.
Methods are *`italicized`* in the tree structure example of \cref{fig:osc-address-space-example}.
The address of an OSC method starts with a `/`, followed by any container name along the path in order from the root of the tree, joined by forward slashes `/` and the method's name, e.g. `/oscillator/1/phase`.

[^osc-naming]:
	The term *OSC Receiver* and *OSC Server* is interchangeable.
	This also applies to *OSC Sender* and *OSC Client*.
	OSC applications often act as server and receiver, hence no clear distinction between those roles can be made.

\begin{figure}
	\centering
	\begin{tikzpicture}
[
	every node/.style = {font=\footnotesize\ttfamily},
	level 1/.style = {sibling distance = 5cm},
	level 2/.style = {sibling distance = 2cm},
	sloped
]
\node{/}
	child { node {oscillator/}
		child { node {1/}
			child { node [osc-method]{freq} }
			child { node [osc-method]{phase} }
		}
		child [sibling distance=20mm]{ node {2/}
			child { node {\ldots} }
		}
	}
	child { node {filter/}
		child { node [osc-method]{mode} }
		child { node [osc-method]{cutoff} }
		child { node [osc-method]{resonance} }
	}
	;
	\end{tikzpicture}
	\caption{OSC Address Space example.}
	\label{fig:osc-address-space-example}
\end{figure}

\begin{figure}
\caption{Grammar of an OSC packet described as EBNF (ISO14977 syntax \cite[p.~14]{ISO14977})}
\label{osc:grammar}
\begin{verbatim}
packet			= size, content ;
size			= (* 4-byte aligned packet content field length *) ;
content			= message | bundle ;
message			= address, ",", { type-tag }, { argument } ;
address			= "/", osc-string - ( "'" | "#" | "*" | "," | "/" |
									  "?" | "[" | "]" | "{" | "}" ) ;
osc-string		= { ASCII }, "0" ;
type-tag		= "i" | "f" | "s" | "b" | "h" | "t" | "d" | "S" |
				  "c" | "r" | "m" | "T" | "F" | "N" | "I" | 
				  "{", {type-tag}, "}" ;
argument		= (* binary representation of the argument *) ;
bundle			= "#bundle", OSC-timetag , { bundle-element } ;
bundle-element	= size, content ;
\end{verbatim}
\end{figure}

### Comparison to MIDI

Both protocols provide a number of benefits and limitations in comparison to each other.
The following list shows them for OSC compared to MIDI:

\begin{enumerate}
	\item[$+$] OSC's data-types allow a much higher resolution for control values.
	They also provide symbolic types like booleans or \emph{Nil} to represent an empty value.
	\item[$+$] The definition of \emph{custom data-types} is allowed, therefore OSC applications must be made robust against unknown ones.
	\item[$+$] The \emph{bandwidth} is orders of magnitudes larger than MIDI's, but it depends on the type of network used.
	A common choice are ad-hoc WiFi connections between OSC receiver and sender because the player (sender) and the instrument (receiver) are in local proximity to each other.
	This, in turn, results in an acceptable network \emph{latency} in the single digit millisecond range.
	\item[$+$] Control events can be send simultaneously as an OSC bundle, e.g. note events of a chord.
	\item[$+$] Events can be timed with an resolution of $\approx 200$ picoseconds \cite{OSC:10}.
	\item[$+$] OSC can be used to tunnel MIDI messages over a network connection.
	\item[$-$] There is no standard for discovering OSC devices in a network, thus addresses must be configured manually which is cumbersome.
	\item[$-$] Unlike MIDI, there is no standard namespace for interfacing with an OSC device, although, a proposal for a standard exists \cite{synoscopy}.
	\item[$-$] The number of applications that support OSC is very limited.
\end{enumerate}

# Synthesizer Fundamentals 

This chapter outlines the fundamental elements of a synthesizer and briefly describes the fundamental methods of sound generation.

## Oscillator

Oscillators are the fundamental building blocks of a synthesizer's sound generation engine.
They serve the purpose of emitting a periodic waveform.
An oscillator is controlled through its *frequency* and *amplitude* parameters.
In the context of a synthesizer there are additional controls for starting *phase*, the point at which the waveforms begins, and *type of waveform* to emit.

The amplitude parameter sets the *peak amplitude* for the signal, i.e. the absolute value of the waveforms highest amplitude.
Frequency is usually specified as number of waveform cycles per second (Hz) but in the software implementation stored as *phase increment* (angular frequency) for each sample step.
The software oscillators output is a sequence of samples at equidistant intervals $T$.
Let $f_s=1/T$ be the sample rate and $f$ the frequency in Hz ($s^{-1}$), then the phase increment $\omega$ is calculated like this:

\begin{equation}
\omega = \frac{2\pi\,f}{f_s}
\label{eq:phase-incr}
\end{equation}

The oscillators highest frequency is limited to $f_s/2$ or $\omega = \pi$, which is called *Nyquist frequency*.
In general, the (Nyquist-Shannon) *sampling-theorem* states that a signal can be exactly reconstructed from its digitization if its entire frequency spectrum lies below the Nyquist frequency \cite[p.~244]{Benson2008}.
In other words, it ensures that there are at least two sample points for any frequency contained in the sampled signal.

### Aliasing
\label{sec:aliasing}

\begin{figure}
\resizebox{\textwidth}{!}{
\includegraphics[width=\textwidth]{imgs/aliasing.pdf}}
\caption{Two sinusoids with angular frequencies $\omega_1 = \pi/2,\,\omega_2 = 3/2\,\pi$ sampled in intervals of $T=\pi/2$. Both sinusoids produce the same sampled signal due to aliasing.}
\label{fig:aliasing}
\end{figure}

\Cref{fig:aliasing} shows two sinusoids with frequencies $\omega_1=\pi/2$ and $\omega_2=3/2\,\pi$ that are sampled at sample rate $\omega_s=\pi/2$.
Clearly, $\omega_2$ is above the Nyquist frequency $\omega_{\text{Ny}}=\pi$, thus $\omega_2$ is *foldover* at $\omega_{\text{Ny}}$ which results in a frequency of $\omega_2\mod\omega_{\text{Ny}} = \pi/2$ that is equal to $\omega_1$, therefore $\omega_2$ is an *alias* of $\omega_1$, so both signals are indistinguishable after the sampling process.
This effect is called *aliasing* or *foldover* and is an inevitable result of sampling or sample rate conversion, hence signal components with frequencies above Nyquist must be removed or reduced before fed into the sampling process.
The effect of foldover in the frequency spectrum is shown in \cref{fig:foldover}.

The alias $f_a$ for a frequency $f$ and a sampling frequency $f_s$ can be calculated as shown in \cref{eq:aliasing-frequency}\footnote{\citeauthor{Dashow1978} presented a method for generating non-harmonic spectra using foldover frequencies \cite[p.~82]{Dashow1978}.}.

\begin{equation}
f_a =
\begin{cases}
|N\,f_s - f|,& \text{if }N\text{ is even}\\
|(N+1)f_s - f|,& \text{otherwise}
\end{cases}
\text{, where }N = \lfloor{}f/f_s\rfloor{}
\label{eq:aliasing-frequency}
\end{equation}

\begin{figure}
\includegraphics[width=\textwidth]{imgs/foldover}
\caption{Spectra for waveforms sampled at a) $f_{\text{Ny}}=30$ and b) $f_{\text{Ny}}=10$ which is $1/3$ of the highest frequency contained, hence foldover (aliasing) occurs.}
\label{fig:foldover}
\end{figure}

### Waveforms
\label{sec:waveforms}

Synthesis techniques like subtractive or FM synthesis (see \cref{sec:synthesis-techniques}) require a source signal with rich harmonic content, hence providing only sine wave oscillators is not sufficient.
Oscillator waveforms that are commonly available in most synthesizers are *triangle*, *sawtooth* and *square wave*.
Non band limited versions of those waveforms are shown in \cref{fig:waveforms}.
To prevent aliasing artifacts it is required to create band limited versions of those waveforms.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/basic-waveforms}
\caption{Common (non band-limited) waveforms supported by most synthesizers: a) sine wave, b) triangle wave, c) ramp/sawtooth, d) square wave}
\label{fig:waveforms}
\end{figure}

#### Fourier Synthesis
\label{sec:fourier-synthesis}

*Fourier synthesis* is used to create band limited versions of those complex waveforms.
It uses the properties of *Fourier series* representation of arbitrary but periodic waveforms \cite[p.~103]{LoyMusimathics2}:

> Any periodic vibration, \ldots, can be built up from sinusoids whose frequencies are integer multiplies of a fundamental frequency, by choosing the proper amplitudes and phases.

This means that any periodic waveform can be constructed by specifying the power of each its *harmonics*, where a *harmonic* is an integer multiple of the waveforms fundamental frequency.
The Fourier series is defined as (\cref{eq:fourier-series})

\begin{equation}
f(t) = a_0/2 + \sum^\infty_{k=1}a_k \cos(2\pi k t + \phi_k) + \sum^\infty_{k=1}b_k \sin(2\pi k t + \phi_k)
\label{eq:fourier-series}
\end{equation}

where $a_k$ and $b_k$ are coefficients for the strength of the k-th harmonic. The harmonics phase is specified $\phi_k$ and $\omega_k=2\pi{}k$ sets its angular frequency.
By substituting\footnote{Eulers identities: $\cos = (e^{i\phi} + e^{-i\phi})/2$ and $\sin = (e\,i^{i\phi} + e\,i^{-i\phi})/2$} eulers equation (\cref{eq:eulers-equation})

\begin{equation}
e^{i\phi} = \cos(\phi)+i\sin(\phi)
\label{eq:eulers-equation}
\end{equation}

into \cref{eq:fourier-series} it can be written in complex form:

\begin{equation}
f(t) = \sum_{k=-\infty}^\infty c_n e^{i\,2\pi\,k\,t+\phi_k}.
\label{eq:fourier-series-complex}
\end{equation}

The summation range has changed for the complex Fourier series \cref{eq:fourier-series-complex} to $-\infty$ and $\infty$, thus there are now negative frequencies.
To retrieve a real-valued signal from the complex Fourier series it is required to take the conjugate transpose of each value, i.e. to specify the coefficients $c_k$ as pairs with a $-c_k$ for each positive coefficient.

The complex coefficients $c_k$ can be converted from the $a_k$, $b_k$'s with the following equation:

\begin{equation}
c_{\pm{}k}=1/2\left(a_k\pm{}i\,b_k\right)
\label{eq:fourier-coeff-conversion}
\end{equation}

<!-- conversion from complex to real
a_n = c_k + c_{-k} = 2\Re(c_k)
b_n = i\,(c_k + c_{-k}) = 2\Im(c_k)
-->

#### Band limited Waveforms

Fourier Synthesis is used to create one band limited cycle versions for sawtooth, triangle and square waves.
Sawtooth, in contrast to square and triangle waves, contains odd and even harmonics which makes them a great source signal because of their rich harmonic content.
Square and triangle waves consist solely of odd harmonic partials.
The complex Fourier series for sawtooth, triangle and square waves is shown in \cref{eq:sawtooth-series}, \cref{eq:squarewave-series} and \cref{eq:triangle-series} where the sum is zero for $n=0$.
The number of harmonics contained in the waveform is determined by summation limits and \cref{fig:waveforms} illustrates evaluated Fourier series for sawtooth (a) and square wave (b) at increasing numbers of harmonics partials.

\begin{equation}
x_\text{saw}(t) = \sum^{\infty}_{n=-\infty,\;n\neq{}0} -1^n \frac{e^{-i\,2\pi\,n\,t}}{n\pi}
\label{eq:sawtooth-series}
\end{equation}

\begin{equation}
x_\text{square}(t) = 2 \sum^{\infty}_{n=-\infty,\;n\neq{}0}\frac{e^{-i\,2\pi\,(2n - 1)\,t}}{n\pi}
\label{eq:squarewave-series}
\end{equation}

\begin{equation}
x_\text{triangle}(t) = 4\sum^{\infty}_{n=-\infty,\;n\neq{}0} \frac{e^{-i\,2\pi\,(2n-1)\,t}}{(n\pi)^2}
\label{eq:triangle-series}
\end{equation}

\begin{figure}
\includegraphics[width=\textwidth]{imgs/fourier-series-saw.pdf}
\includegraphics[width=\textwidth]{imgs/fourier-series-sqr.pdf}
\caption{One cycle of band limited a) sawtooth and b) square waveforms with increasing number of harmonics.}
\label{fig:waveforms}
\end{figure}

## Non linearity of hearing
\label{sec:hearing}

The intensity of a sound is perceived logarithmically by human hearing \cite[p.~27]{West1986}.
Therefore, the *ratio* between two sound intensities is important and *not* the difference as in the case of linear perceived phenomenons.
The ratio of two physical quantities, e.g. signal amplitudes, is measured in \si{\decibel} (decibel), a dimensionless logarithmic unit.
Distinction should be made between the ratio of signal energy which is expressed by

\begin{equation}
10\log_{10}{}\left(\frac{a}{b}\right)
\label{eq:db-energy}
\end{equation}

and the ratio of signal power \cref{eq:db}.

\begin{equation}
20\log_{10}{}\left(\frac{a}{b}\right)
\label{eq:db}
\end{equation}
The difference in the scaling factors is based on the definition of signal energy where the square of the signals amplitude is taken, see \cref{eq:signal-energy}.
\begin{equation}
\int^{\infty}_{-\infty} \left|x(t)\right|^2 dt
\label{eq:signal-energy}
\end{equation}

\begin{figure}
\includegraphics[width=\textwidth]{imgs/db.pdf}
\caption{Relationship between \si{\decibel} and corresponding amplitude ratios on a logarithmically scaled y-axis.}
\label{fig:db}
\end{figure}
\Cref{fig:db} illustrates the relationship between \si{\decibel} levels (x-axis) and corresponding amplitude ratios on a logarithmically scaled y-axis.
The pressure levels audible by human ears range from 0.00002 \si{\newton\per\meter\squared} to 200 \si{\newton\per\meter\squared} \cite[p.~26]{West1986}, which is seven orders of magnitude larger than the audibility threshold and clearly shows that logarithmic scale is better suited than a linear one to represent sound level ratios.

## Envelope Generators

The sound produced by an musical instrument is usually not static and changes in amplitude or spectral content over time.
To simulate these *time-varying* waveforms a function of time, the envelope generator, is used to controls parameters of an oscillator or other parts of a synthesizers sound engine, e.g. the cutoff of a frequency filter.
In \cref{fig:piano-c4} the time-varying behavior of sounds produced by musical instruments is illustrated by an example of waveform plot of a C\textsubscript{4} note played on piano.

<!-- Replace this by the high resolution image -->
\begin{figure}
\includegraphics[width=\textwidth]{imgs/piano_c4_small.png}
\caption{Waveform plot of sampled C\textsubscript{4} note played on a piano \cite{freesound:piano}.}
\label{fig:piano-c4}
\end{figure}

There are various types of envelope generators that range from simple two stage models, for fading the sound in and out, to ones which have an arbitrary number of stages and envelope shapes.
A commonly used model with a reasonable amount of controllable parameters is the so called ADSR envelope, which stands for the four different stages of the envelope which are *Attack*, *Decay*, *Sustain* and *Release*.
Because of the non-linearity of human hearing, as discussed in \cref{sec:hearing}, it is not sufficient to linearly ramp values between those four stages because this would not yield a smooth change in perceived loudness.

\Citeauthor{Puckette2006a} proposes three different amplitude envelope transfer functions \cite[p.~94]{Puckette2006a} where $10^{2(x-1)}$ converts from \si{\decibel} to linear and the quartic curve $x^4$ approximates the exponential \si{\decibel} curve while being computationally less expensive and reaching true zero at $x=0$.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/adsr-transfer-functions}
\caption{Three amplitude envelope transfer functions for input values in the range of [0, 1] as proposed by \cite[p.~94]{Puckette2006a}.}
\label{fig:transfer-functions}
\end{figure}

An ADSR generator's output is fully determined by five parameters, that are the output *level* and *duration* of the attack stage, decay duration, sustain level and duration of release.
\Cref{fig:adsr} shows the output and stages for an envelope generator with exponential transfer function.
The generator will start the output on an event like a key press and will reside in the sustain stage as long as the key is still pressed.
If the key is released the generator will switch to the release stage independent of its state at the time of the event.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/adsr}
\caption{An ADSR envelope with equal duration of 0.5\si{\second} for each stage and attack and sustain levels of 1 and 0.5.}
\label{fig:adsr}
\end{figure}


## Synthesis Techniques
\label{sec:synthesis-techniques}

Since the invention of the first electrical organ in 1894 \cite[p.~83]{Roads:CMT}, the *Telharmonium* built by Thaddeus Cahill, a lot of different synthesizing techniques have been developed.
Those techniques can be roughly divided into two broad categories, techniques for *mimicking the sound of traditional instruments*, like

- Karplus-Strong synthesis \cite{KarplusStrong}, a simple technique for simulating plucked-string or drum sounds
- physical modeling synthesis which uses a mathematical model of an instrument to generate sounds

or techniques for generating arbitrary sounds, possibly not reproducible by a physical instrument, like *Additive*, *Subtractive* and *FM* synthesis.
Developing a physical simulation of a traditional instrument is not the aim of this thesis, therefore the latter techniques will be described in this section.

### Additive Synthesis

Additive synthesis is one of the oldest sound synthesizing techniques.
It uses separate sinusoidal oscillators to generate a complex sound from its partials.
As the name suggests, the output of each oscillator is added up to obtain the resulting output signal.
The basic structure of an additive synthesizer is shown in \cref{fig:additive-synthesizer}, where $∿_i$ denotes a sinusoidal oscillator with frequency input $f_i$ and amplitude input $a_i$.

An advantage of additive synthesis is its great versatility, because virtually any sound can be synthesized, given a sufficient amount of oscillators.
This comes with two major downsides:

1. this method is computationally expensive.
2. it is hard to control because there are at least as twice as much parameters as there are oscillators.

Additionally, to be able to simulate real or time-varying artificial sounds there must be functions that control those parameters over time, e.g. to reduce the amplitude of higher frequency partials when the sound decays.
The coefficients obtained by a the Fourier analysis of a real sound (e.g. a sample of played key on a piano) can be used as parameters to reconstruct this sound through additive synthesis, this process is sometimes called *Fourier recomposition* \cite[p.~88]{West1986}.

\begin{figure}
	\centering
	\begin{tikzpicture}
[
    auto,
    >=latex,
    node distance = 1cm
]
\node[block](osc1){$∿_1$};
\node[input, above=of osc1, xshift=-4mm](f1){$f_1$};
\node[input, above=of osc1, xshift=4mm](a1){$a_1$};
\draw[->](f1)--([xshift=-4mm]osc1.north);
\draw[->](a1)--([xshift=4mm]osc1.north);

\node[block, right=of osc1](osc2){$∿_2$};
\node[input, above=of osc2, xshift=-4mm](f2){$f_2$};
\node[input, above=of osc2, xshift=4mm](a2){$a_2$};
\draw[->](f2)--([xshift=-4mm]osc2.north);
\draw[->](a2)--([xshift=4mm]osc2.north);

\node[right=of osc2](oscX){\ldots};

\node[block, right=of oscX](oscN1){$∿_{n-1}$};
\node[input, above=of oscN1, xshift=-4mm](fN1){$f_{n-1}$};
\node[input, above=of oscN1, xshift=4mm](aN1){$a_{n-1}$};
\draw[->](fN1)--([xshift=-4mm]oscN1.north);
\draw[->](aN1)--([xshift=4mm]oscN1.north);

\node[block, right=of oscN1](oscN){$∿_n$};
\node[input, above=of oscN, xshift=-4mm](fN){$f_n$};
\node[input, above=of oscN, xshift=4mm](aN){$a_n$};
\draw[->](fN)--([xshift=-4mm]oscN.north);
\draw[->](aN)--([xshift=4mm]oscN.north);

\node[sum, below=of oscX](sum){$+$};
\draw[->](osc1.south)--(sum);
\draw[->](osc2.south)--(sum);
\draw[->](oscN1.south)--(sum);
\draw[->](oscN.south)--(sum);
\node[output, below=of sum](out){$y_n$};
\draw[->](sum)--(out);
	\end{tikzpicture}
	\caption{Basic structure of an additive synthesizer.}
	\label{fig:additive-synthesizer}
\end{figure}

### Frequency Modulation (FM) Synthesis

Frequency Modulation (FM) was originally used in telecommunications to encode information on a carrier wave by modulating the waves instantaneous frequency, e.g. for radio broadcast.
In \citeyear{Chowning:FM}, \citeauthor{Chowning:FM} presented a new application of this well-known process to control spectral components of an audio signal with great simplicity \cite[p.~1]{Chowning:FM}.
Contrary to its well-understood use for radio transmission, both, the *carrier* and the *modulating frequency* are inside the audio band.
The audio spectrum is formed by the carrier wave and side frequencies which are introduced through frequency modulation.
Modulation of the carrier wave is determined by two factors:

- the *frequency of the modulating wave* $m_f$ sets rate at which the instantaneous frequency of the carrier varies.
- the *amount of modulation* $m_a$ which is equal to the modulating waves amplitude.

If both the carrier as well as the modulator, are sinusoids then the instantaneous frequency maybe be calculated as follows \cite[p.~2]{Chowning:FM}:

\begin{equation}
y(t) = A \sin\left(c_f t + I \sin(m_f t)\right)
\label{eq:fm}
\end{equation}

where $A$ is the *peak amplitude*, $c_f$ is the carrier wave's frequency and $I = m_a/m_f$ is the ratio of modulation amount to modulation frequency also called *modulation index*. 
A table of waveforms generated by different values of $m_f$ and $m_a$ is shown in \cref{fig:fm-table}.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/fm-grid}
\caption{One cycle of a FM modulated sine wave for different modulation intensities $a_m$ and modulator frequencies $f_m$}
\label{fig:fm-table}
\end{figure}

For $I=0$ there is no modulation but non-zero values will result in frequencies occurring below and above the carrier frequency at intervals of the modulating frequency.
\citeauthor{Chowning:FM} describes the relation of modulation index and occurrence of side frequencies like this \cite[p.~2]{Chowning:FM}:

> The number of side frequencies which occur is related to the modulation index in such a way that as $I$ increases from zero, energy is \enquote{stolen} from the carrier and distributed among an increasing number of side frequencies.

This behavior is shown in \cref{fig:fm-bandwidth} for different modulation indices by constant modulation and carrier frequency. Negative amplitudes for frequency components indicate *phase inversion*\footnote{$-\sin(\phi)=\sin(-\phi)$}.

\begin{figure}
\begin{center}
\includegraphics[height=.8\textheight]{imgs/fm-bandwidth}
\end{center}
\caption{Magnitude spectra for frequency modulated carrier frequency $c_f$ at different modulation indices $I$ and constant modulation frequency $m_f$. Bandwidth increases symmetrically around $c_f$ with $I$.}
\label{fig:fm-bandwidth}
\end{figure}

Specific carrier and modulation frequency ratios and modulation index values will produce sideband frequencies that fall into the negative spectrum.
Those negative frequency components will be reflected (aliased) around $\SI{0}{\hertz}$.
Reflected sideband components will either increase or---if they are phase inverted---decrease the energy the energy in the spectrum.

Harmonic spectra\footnote{Overtones are an integer multiple of waves the fundamental frequency.} will be generated if the ratio of carrier and modulation frequency is a rational number.
Ratios that are irrational numbers, e.g. $1/\sqrt{2}$, will result in inharmonic spectra because the reflected sideband frequencies will fall in-between the positive frequency components.

Carrier and sideband component amplitudes can be determined analytically by evaluating n-th order Bessel functions $J_i$ of the first kind with the modulation index as argument.
A quick estimation for the resulting bandwidth of different modulation indices is shown in \cref{fig:fm-bessel} by evaluating Bessel functions $J_0$ through $J_{15}$ for those indices.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/fm-bessel}
\caption{Bandwidth estimation for modulation indices $I$ ranging from 0 through 20 by evaluating Bessel functions $J_0$ through $J_{15}$ and showing resulting sideband frequency $s_f$ amplitudes $s_A$ \cite[p.~5]{Chowning:FM}.}
\label{fig:fm-bessel}
\end{figure}

The most basic FM synthesizer *algorithm* consists of two *operators* (which are just oscillators in FM terminology), a modulator and a carrier, where the modulators output is summed with the carriers fundamental frequency.
<!-- TODO: untereinander verbunden -->
An FM algorithm is described by how its operators are connected among each other.
\Cref{fig:fm-operator} shows the structure of the most basic FM algorithm, a simple pair of operators.

\begin{figure}
	\centering
	\begin{tikzpicture}
[
    auto,
    >=latex,
    node distance = 5mm
]
\node[block](modulator){$∿$};
\node[input, above=of modulator, xshift=-4mm](fm){$f_m$};
\node[input, above=of modulator, xshift=4mm](am){$a_m$};
\draw[->](fm)--([xshift=-4mm]modulator.north);
\draw[->](am)--([xshift=4mm]modulator.north);
\node[sum, below=of modulator](sum){$+$};
\node[input, right=of sum](fc){$f_c$};
\draw[->](modulator)--(sum);
\draw[->](fc)->(sum);
\node[block, below left=of sum](carrier){$∿$};
\draw[->](sum)-|([xshift=4mm]carrier.north);
\node[input, above=of carrier, xshift=-4mm](An){$A(n)$};
\draw[->](An)--([xshift=-4mm]carrier.north);
\node[output, below=of carrier](out){$y_n$};
\draw[->](carrier)--(out);
	\end{tikzpicture}
	\caption{Most basic FM algorithm, a pair of operators with one modulator and carrier.}
	\label{fig:fm-operator}
\end{figure}

FM's advantages lies in the simplicity of control, the small computational effort that is required and the great amount of flexibility that is given arranging operators in different algorithms.
On the other hand, FM synthesis is likely to introduce undesirable aliasing of higher frequencies which must be taken into account when implementing the algorithm.

### Subtractive Synthesis



\begin{figure}
	\centering
    \resizebox{\textwidth}{!}{
	\begin{tikzpicture}
[
    auto,
    >=latex',
    node distance = 1cm
]
\node[align=center](control){Control\\Input};
\node[matrix, row sep=5mm, right=of control](voices){
    \node[draw](voice1){
        $\text{voice}_1$
        \begin{tikzpicture}
            \node[block](osc){$∿$};
            \node[left=of osc, yshift=4mm](freq){freq};
            \node[draw, left=of osc, yshift=-4mm](adsr){envelope};
            \draw[->](freq)--([yshift=4mm]osc.west);
            \draw[->](adsr)--node[above, midway]{A}([yshift=-4mm]osc.west);
        \end{tikzpicture}
    };\\
    \node[draw](voice2){
        $\text{voice}_2$
        \begin{tikzpicture}
            \node[block](osc){$∿$};
            \node[left=of osc, yshift=4mm](freq){freq};
            \node[draw, left=of osc, yshift=-4mm](adsr){envelope};
            \draw[->](freq)--([yshift=4mm]osc.west);
            \draw[->](adsr)--node[above, midway]{A}([yshift=-4mm]osc.west);
        \end{tikzpicture}
    };\\
    \node[](voiceX){\vdots};\\
    \node[draw](voiceN1){
        $\text{voice}_{n-1}$
        \begin{tikzpicture}
            \node[block](osc){$∿$};
            \node[left=of osc, yshift=4mm](freq){freq};
            \node[draw, left=of osc, yshift=-4mm](adsr){envelope};
            \draw[->](freq)--([yshift=4mm]osc.west);
            \draw[->](adsr)--node[above, midway]{A}([yshift=-4mm]osc.west);
        \end{tikzpicture}
    };\\
    \node[draw](voiceN){
        $\text{voice}_n$
        \begin{tikzpicture}
            \node[block](osc){$∿$};
            \node[left=of osc, yshift=4mm](freq){freq};
            \node[draw, left=of osc, yshift=-4mm](adsr){envelope};
            \draw[->](freq)--([yshift=4mm]osc.west);
            \draw[->](adsr)--node[above, midway]{A}([yshift=-4mm]osc.west);
        \end{tikzpicture}
    };\\
};
\draw[->](control.north)|-(voice1.west);
\draw[->](control.north)|-(voice2.west);
\draw[->](control.south)|-(voiceN1.west);
\draw[->](control.south)|-(voiceN.west);
\node[sum, right=of voices](sum){$+$};
\draw[->](voice1.east)-|(sum.north);
\draw[->](voice2.east)-|(sum.north);
\draw[->](voiceN1.east)-|(sum.south);
\draw[->](voiceN.east)-|(sum.south);
\node[block, right=of sum, align=center](filter){Multi mode\\Filter};
\draw[->](sum)--(filter);
\node[right=of filter,align=center](out){Output\\Device};
\draw[->](filter)--(out);
	\end{tikzpicture}}
	\caption{Building blocks/structure of a basic polyphonic subtractive synthesizer.}
	\label{fig:basic-synth-structure}
\end{figure}

- pitch (see MIDI pitch, log. scale)
- periodic signals $x[n+\tau] = x[n]$
- angular frequency $\omega = 2\pi f = 2\pi \frac{1}{N}$

# Oscillators Methods

- primitive waveforms not suitable because they are not band limited
- example: ramp for sawtooth and -1,1 for perfect square, unlimited harmonics to generate hard step (discontinuity) in waveform
- aliasing problems
- anti-aliasing not possible after sampling
- simple: oversample -> antialias (low-pass, brickwall) -> resample
- fold back: sr = 48kHz, harmonic 30kHz -> 24kHz - (30kHz - 24khz) = 18kHz ...

## BLITs

## Wavetables

- one cycle of a periodic waveform
- pitch: read wavetable at different speeds (index increments)
- increment = (length * frequency)/sampling_frequency
- zero-degree polynomial floor (round), linear interpolation (first-degree polynomial)
     - $y_\text{Lin}(x) = y[x_0] + \left(y[x_0 + 1]-y[x_0]\right)\cdot(x-x_0)$
     - two error components: change in amplitude (barely perceptible) and distortion of waveshape (very perceptible)

\begin{figure}
\includegraphics[width=\textwidth]{imgs/table-lookup.pdf}
\caption{Wavetable for a single period of a sine wave sampled at 24 equidistant points. The bottom x-axis shows the index of the sample in the wavetable and the top shows angle in radians.}
\label{fig:table-lookup}
\end{figure}

\begin{figure}
\includegraphics[width=.9\textwidth]{imgs/saw-table.pdf}
\caption{Harmonic spectra and time domain representation for a sawtooth waveform}
\label{fig:saw-table}
\end{figure}

\begin{figure}
\includegraphics[width=.9\textwidth]{imgs/sqr-table.pdf}
\caption{Harmonic spectra and time domain representation for a square waveform}
\label{fig:sqr-table}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{imgs/interpolation-error.pdf}
\caption{Error of zero-degree (floor) and first-degree (linear) interpolation for a wavetable of 64 samples}
\label{fig:interpolation-error}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{imgs/harmonic-spectrum.pdf}
\includegraphics[width=\textwidth]{imgs/inharmonic-spectrum.pdf}
\caption{Spectrum of a signal with only harmonic content (a) and solely inharmonic content (b)}
\label{fig:spectra}
\end{figure}

## Polyphony

## Latency
<!-- TODO: unsure where to put this chapter -->

The *responsiveness* of an electronic musical instrument is mainly determined by its latency.

\theoremstyle{definition}
\begin{definition}
\label{def:latency}
Latency is the time delay between two causally connected events.
\end{definition}

An instrument is the more responsive, the less latency between an input event and the corresponding sound output it has.
The latency is imperceptible for the user if the delay between the input event and audio output stimuli is $< 24ms$ \cite{NasaLatency}.
Furthermore, there is an even stronger latency limit of $\approx 2$ms, that is the *temporal resolution* of the human hearing, as shown by \cite[p.\~294]{FastlZwicker} using psycho acoustic measurements.
However, it is not realistic to use this as an upper limit for the synthesizer's system latency, considering that the sound propagation delay from a speaker to a listener at a speed of sound $v = 343.2 m/s$ and a common listening distance of $d = 2m$ is $\approx$ 3 times larger than the temporal resolution: $d/v = 2m/343.2\frac{m}{s} = 5.82ms$.
Therefore, achieving a system latency of less than $24ms$ is favorable.

Latency (see \nameref{latency}) is the time difference between an input action and the corresponding output from a system, in this case the synthesizer.

- reduce latency with custom kernel (linux-ck)

\begin{definition}{Latency} is a time interval between the stimulation and response, or, from a more general point of view, a time delay between the cause and the effect of some physical change in the system being observed
\end{definition}

> For any audio system, may it be, for example, an analogue electronic circuit, a digital circuit, a whole computer or a physical wave-guide, there will be some time lag between the instant at which the signal enters the audio system and the one at which the signal exits.
> For a lot of reasons:
> - finite propagation speed of sound waves
> - AD/DA conversion times

- temporal resolution of human hearing is $\approx 2\,ms$ \cite{FastlZwicker}
- only control over some aspects, buffer sizes of the application or the sound backend
- user of the application should not be able to hear any possible delay between his input action/event and the generated output signal

Spectral artifacts can occur for very low latencies and only when the signal is monitored (comb filter).
Can be ignored because there is no source signal that is monitored.
Temporal issues are to be concerned.
\cite{AES:Latency}

The propagation delay, i.e. time $t$ it takes for a wave of sound to travel from the speaker to the ear of the listener, over a distance of $d = 1.5\,m$, and by a speed of sound of $v = 343.2\,m/s$ at normal temperature [^stdTemp] can be calculated like this:

[^stdTemp]: The *normal temperature* is defined by the National Institute of Standards and Technology (NIST) as $20°C$ at $1\,atm$ absolute pressure.

$$
\begin{aligned}
t &= \frac{d}{v}\cdot 1\,s\\
  &= \frac{1.5\,m}{343.2\,m}\cdot 1\,s\\
  &= 0.00437\,s\\
  &= 4.37\,ms
\end{aligned}
$$

The overall latency of the synthesizer is the sum of different latency portions, introduced through a variety of components in the audio output chain.
At first there is the input latency, that is either $l_{midi}$ or $l_{osc}$ depending on the used input protocol, where the amount of $l_{osc}$ depends highly on the network connection.
Usually a wifi connection is used to connect an OSC client, like a tablet running [liine's lemur](https://liine.net/de/products/lemur/) or some other OSC capable controller application.
An ad-hoc wifi network or tethering over USB can be used to get reliable network latencies with wireless clients.

The next latency portion, $l_{dsp}$ is introduced through the synthesizers internal audio output buffer.
At the time of writing, a small buffer size of [64 samples](https://github.com/klingtnet/ytterbium/blob/master/src/main.rs#L172), or $\approx 1.3\,ms$, was used.

The audio backends output buffer adds a portion $t_{backend}$ of latency, as well.
It depends on the users buffer size settings how large the added latency is.

At last there is the propagation delay which can be neglected for usual listening distances or is near zero if headphones are used.

\resizebox{\textwidth}{!}{
\begin{tikzpicture}[auto]
\draw
	node [block](Engine){DSP Engine}
	node [block, above left=of Engine, node distance=3cm](MIDI){MIDI}
	node [block, below left=of Engine, node distance=3cm](OSC){OSC}
	node [block, right=of Engine, node distance=4cm](Soundcard){Audio Backend}
	node [block, right=of Soundcard, node distance=5cm](Speaker){Speaker}
	node [block, right=of Speaker, node distance=4cm](Listener){Listener}
;
	\draw[->](MIDI) -- node{$l_{midi}$}(Engine);
	\draw[->](OSC) -- node{$l_{osc}$}(Engine);
	\draw[->](Engine) -- node{$l_{dsp}$}(Soundcard);
	\draw[->](Soundcard) -- node{$l_{backend}$}(Speaker);
	\draw[->](Speaker) -- node{$l_{wave}$}(Listener);
\end{tikzpicture}}

## Filter

\begin{figure}
\includegraphics[width=\textwidth]{imgs/filter-bands.pdf}
\caption{Terminology for describing the frequency response of a low-pass filter.}
\label{fig:bands}
\end{figure}

\begin{equation}
H(z) = \frac{Y(z)}{X(z)} = \frac{\sum^M_{k=0}b_k z^{-k}}{\sum^N_{k=0}a_k z^{-k}}
\label{eq:iir-transfer-function}
\end{equation}

<!-- http://www.musicdsp.org/files/Audio-EQ-Cookbook.txt -->
\begin{align}
H(z) &= \frac{b_0 + b_1\,z^{-1} + b_2\,z^{-2}}{a_0 + a_1\,z^{-1} + a_2\,z^{-2}}\\
    &= \frac{b_0}{a_0}\cdot\frac{1 + (b_1/b_0)\,z^{-1} + (b_2/b_0)\,z^{-2}}{1 + (a_1/a_0)\,z^{-1} + (a_2/a_0)\,z^{-2}}
\label{eq:biquad}
\end{align}

## Waveshaper

# Effects

## Delay

## Reverb

# Implementation Details

\begin{figure}
\includegraphics[width=.8\textwidth,angle=180]{imgs/ytterbium-spectrum-legend.png}
\includegraphics[width=.9\textheight,angle=90,origin=c]{imgs/ytterbium-0.1.0-Saw-sweep.png}
\includegraphics[width=.9\textheight,angle=90,origin=c]{imgs/ytterbium-0.1.0-Square-sweep.png}
\caption{Frequency sweep from 20Hz to 20kHz in the sawtooth and square (below wavetable}
\label{fig:sweep}
\end{figure}

![Sweep using sine modulator and carrier with increasing modulation amount](imgs/ytterbium-0.1.0-fm.png){ width=100% }

## Ring Buffer

## Control Input

### Lemur

![View of the piano section](imgs/lemur-piano.png){ width=100% }

![View of the oscillator contol section](imgs/lemur-oscillator.png){ width=100% }

![View of the fm section](imgs/lemur-fm.png){ width=100% }

![View of the mixer section](imgs/lemur-mix.png){ width=100% }

![View of the mixer section](imgs/lemur-filter.png){ width=100% }

## Audio Output

# Outlook
<!-- TODO: rename this to future work? -->

## Optimizations

- LFOs
- modulation for oscillator pitch via envelopes
- general amount of modulation sources

# Conclusion
