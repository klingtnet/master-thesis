# Introduction
\label{ch:introduction}

Electronic musical synthesizers have a long history that started in 1906 with the *Telharmonium* \cite[p.~83]{Roads:CMT}, a very large device with rotating tone generators that emitted pure sinusoidal waves.
It took nearly 50 years until the first experiments with digital sound synthesis were made in 1957 at Bell Telephone Laboratories with the development of Music I and II programs by Max V. Mathews \cite[p.~88]{Roads:CMT}.
Another 20 years later, in the late 70s, the first real-time FM synthesizers became commercially available, e.g. the Fairlight CMI in 1979.
The first affordable digital hardware FM synthesizer, Yamaha's DX7, was introduced in 1983 and caused the decline of analog synthesizers because digital hardware became cheaper from year to year.
Due to increasing computing power, real-time synthesis was made possible on general purpose computers in the early 2000s without the need for special DSP processors or other expensive audio hardware.
Since then, a large number of vastly different software synthesizers was developed, both commercial as well as free or open source projects.

Despite the huge amount of computing power that is recently available even on commodity hardware, the development is still a challenging task because musicians expect an nearly instantaneous response from their instrument and dropped audio frames cause click sound which could make a whole recording useless.
Therefore, audio software must make very efficient use of processing and power while achieving the best possible amount of audio quality.

The set of problems that arise when developing a software synthesizer is quite large.
It includes timing and synchronization problems that occur because the short time windows in which computations are required to finish.
Another class of problems are sound artifacts like aliasing which must be avoided by choosing appropriate algorithms.

Audio software is usually implemented in programming languages with manual memory managment (unmanaged languages) like C or C++ for which a great number of frameworks already exist.
Rust, on the other hand, is a promising new programming language that claims to be thread-safe, guarantees memory safety (unlike C and C++) and has little runtime cost because a garbage collector is not required.
This makes Rust a great candidate for the development of modern audio software.
But, due to the language's ecosystem still being in its early days, a lot of libraries had to be implemented from scratch.

The aim of this thesis is to evaluate available methods and algorithms for the use in a real-time polyphonic FM synthesizer and to implement a prototype that can be played with conventional audio controller hardware.
Every piece of code that was implemented for this thesis is being open sourced, see \cref{ch:implementation-evaluation} for details.

## Scope of this Thesis
\label{sec:scope}

It's not required for the reader to have any prior knowledge of soft- and hardware tools used in music production environments.
However, a basic knowledge of signal processing and some familiarity with computer programming is beneficial to grasp the presented concepts.

# User Interface
\label{ch:user-interface}

Interaction with the software synthesizer is done through its *user interface*.
The user interface serves three senses, which are sight, touch and hearing.
This section concentrates on the first two, the *visual* and the *haptic* component.
However, a synthesizer can be played solely through haptic controls and audio feedback.
Visual indicators for the synthesizer's parameters, e.g. through a display, are convenient, but not necessary for the playing musician.

The application supports the two most common musical control signal protocols, *MIDI* and *Open Sound Control* (OSC).
Adding MIDI support is highly beneficial, because it enables the synthesizer to be played with any---of the vast amount of available---MIDI hardware controllers (\cref{midi:edirol} shows an example of such a device).
On the other hand, Open Sound Control software like [liine's Lemur](https://liine.net/de/products/lemur/) \cite{LiineLemur} provides an editor to create or customize a software defined controller for a multi-touch device like a smartphone or tablet.

![Edirol PCR-300 MIDI controller keyboard\label{midi:edirol}](imgs/pcr_300_angle.jpg){ width=100% }

## MIDI
\label{sec:midi}

The Musical Instrument Digital Interface (MIDI) specification stipulates a hardware interconnection scheme and a method for data communication \cite[p.~972]{Roads:CMT}, but only the protocol specification is of interest for this work.
Most modern MIDI hardware is connected via USB anyway.
The *MIDI 1.0 Specification* \cite{MIDI10} provides a high level description of the MIDI protocol:

> The Musical Instrument Digital Interface (MIDI) protocol provides a standardized and efficient means of conveying musical performance information as electronic data.
> MIDI information is transmitted in \enquote{MIDI messages}, which can be thought of as instructions which tell a music synthesizer how to play a piece of music. \cite[p.~1]{MIDI10}

Transmitting *control data* is the purpose of the MIDI protocol, and not, like it is sometimes confused, to transmit audio data[^midi-audio].
Control data can be thought of as the press of a key, turning a knob, or an instruction to change the clock speed of a song.

The work on the MIDI specification began in 1981 by a consortium of Japanese and American synthesizer manufacturers, the MIDI Manufacturers Association (MMA).
In August 1983, the version 1.0 was published \cite[p.~974]{Roads:CMT}.
This year, 2016, the MMA established The MIDI Association (TMA).
The TMA should support the global community of MIDI users and establish [midi.org](https://www.midi.org/) \cite{MidiOrg} as a central source for information about MIDI.
MIDI is used in nearly every music electronic device, like synthesizers, samplers, digital audio effects, and music software, due to its simple protocol structure and long time of existence.

### MIDI Protocol
\label{subsec:midi-protocol}

The MIDI protocol specifies a standard transmission rate of $\SI{31250}{\bit\per\second}$.
This may seem like an unusual choice for the transmission rate, but it was derived by dividing the common clock frequency of $\SI{1}{\mega\hertz}$ by 32 \cite[p.~976]{Roads:CMT}.
It uses an 8b/10b encoding, i.e. $\SI{8}{\bit}$ of data are transmitted as a $\SI{10}{\bit}$ word.
A data byte is enclosed by a start and stop bit which in turn results in the $\SI{10}{\bit}$ encoding.
Asynchronous serial communication is used to transfer *MIDI messages*, thus the start and stop bit.

A MIDI message is composed of a *status byte* which is followed by up to two[^sysex] *data byte*s.
Both types are differentiated by their most significant bit (MSB), `1` for status- and `0` for data bytes.
Consequently, the usable payload size is reduced to $\SI{7}{\bit}$, in other words, values can range from 0 to 127.

\Cref{fig:midi-status} shows the structure of a status byte.
The message type is denoted by three bits (`T`) and the remaining four bits are used to denote the channel number (`C`), hence there are sixteen different channels.
MIDI channels allow to route different logical streams over one physical MIDI connection, e.g. to reach a different, daisy-chained MIDI device or to control different timbres of a multitimbral synthesizer.

\begin{figure}
    \centering
	$0\quad\underbrace{T\quad{}T\quad{}T}_{\text{message type}}\quad\overbrace{C\quad{}C\quad{}C\quad{}C}^{\text{channel number}}$
	\caption{Structure of a MIDI status byte.}
	\label{fig:midi-status}
\end{figure}

MIDI messages are divided in two categories, *channel* and *system* messages.
Only the latter contain musical control information and therefore are of interest for this thesis.
\Cref{fig:midi-classification} illustrates the classification, status byte values are shown as edge labels where \texttt{x} illustrates *don't care*.
*Channel Mode Messages* define the instrument's response to Voice Messages \cite[p.~36]{MIDI10}, i.e. listen on all channels (omni mode), or switch between mono- and polyphonic mode (multiple simultaneous voices).

\begin{figure}
	\centering
	\begin{tikzpicture}
[
	grow = right,
	every node/.style = {font=\footnotesize},
	sibling distance = 1cm,
	level distance = 1cm,
	level 1/.style = {sibling distance = 5cm, level distance = 3cm},
	level 2/.style = {sibling distance = 2cm, level distance = 5cm},
	sloped
]
\node[msg]{MIDI Message}
	child { node [msg]{System Message}
		child { node [msg]{System Exclusive\\Message}
			edge from parent node [above] {\texttt{F0}}
		}
		child { node [msg]{System Common\\Message}
			edge from parent node [above] {\texttt{F1-F7}}
		}
		child { node [msg]{System Real-Time\\Message}
			edge from parent node [above] {\texttt{F8-FF}}
		}
		edge from parent node [above] {\texttt{F0-FF}}
	}
	child { node [msg]{Channel Message}
		child { node [msg]{Channel Voice\\Message}
			edge from parent node [above] {\texttt{8x-Ex}}
		}
		child { node [msg]{Channel Mode\\Message}
			edge from parent node [above] {\texttt{Bx}}
			edge from parent node [below] {\texttt{Data1: 79-7F}}
		}
		edge from parent node [above] {\texttt{8x-Ex}}
	}
	;
	\end{tikzpicture}
	\caption{Classification of MIDI messages.}
	\label{fig:midi-classification}
\end{figure}

### MIDI Pitch
\label{subsec:midi-pitch}

\begin{table}[]
    \centering
    \caption{Types of MIDI Voice Messages.}
    \label{tab:voice-messages}
    \begin{tabular}{@{}llllp{3.5cm}@{}}
    \toprule
    Type                    & Status & Data1      & Data2    & Description                                                \\ \midrule
    Note-Off                & 8x     & Key \#     & Velocity & Key released.                                              \\
    Note-On                 & 9x     & Key \#     & Velocity & Key press from a triggering device.                        \\
    Polyphonic Key Pressure & Ax     & Key \#     & Pressure & Aftertouch event.                                          \\
    Control Change          & Bx     & Ctrl. \#	  & Value    & Move of a controller other than a key (e.g. Knob, Slider). \\
    Program Change          & Cx     & Program \# & ---      & Instruction to load specified preset.                      \\
    Channel Pressure        & Dx     & Pressure   & ---      & Aftertouch event.                                          \\
    Pitch Bend              & Ex     & MSB        & LSB      & Altering pitch (14-bit resolution).                        \\ \bottomrule
    \end{tabular}
\end{table}

\Cref{tab:voice-messages} gives an overview of the types on voice messages.
Corresponding Note-On and Off messages do not necessarily follow one after another, therefore, to relate associated messages, pitch information is contained in the Note-Off as well.
Pitch is encoded as a \SI{7}{\bit} value in note messages, hence there is a range of 128 pitches or about 10 octaves.
MIDI's pitch representation was designed with an *chromatic western music scale* in mind.
A *chromatic scale* has 12 pitches per octave with one semitone difference between each pitch, that is a ratio of $2^{1/12}$ between successive notes.
An interval of one octave is equivalent to a doubling or halving (in the negative case) in frequency.
Instruments in *western music* are usually *equal-tempered*, i.e. all semitones have the same size.
MIDI pitches are considered to be equal-tempered and range from C0 (*c* in the lowest octave) to a G10 (*g* in the 10th octave).
Middle C, pitch number 60 (C5), is used as reference.

\begin{align}
\begin{split} % creates only a single equation number
	f &= f_\text{tune}\cdot 2^{\displaystyle\left(p-p_\text{ref}\right)/12}\\
    p &= p_\text{ref}+12\cdot\log_2(f/f_\text{tune})
\end{split}
\label{eq:midi-pitch}
\end{align}

\Cref{eq:midi-pitch} shows how to calculate the frequency $f$ for a given MIDI pitch $p$, and vice versa, where $f_\text{tune}$ is the tuning frequency and $p_\text{ref}$ is the reference pitch number.
Musical instruments are commonly tuned to the *Concert A*, the note A above middle C or MIDI pitch 69.
The default tuning of Concert A is \SI{440}{\hertz} \cite{ISO16:1975}.
The following example shows how to calculate the frequency for middle C by using the *Concert A* tuned to \SI{440}{\hertz} as reference pitch in \cref{eq:midi-pitch}:

$$
\begin{aligned}
f	&= \SI{440}{\hertz}\cdot 2^{(60-69)/12}\\
	&= \SI{440}{\hertz}\cdot 2^{-9/12}\\
	&\approx \SI{261.626}{\hertz}
\end{aligned}
$$

### Timing Problems
\label{subsec:midi-timing-problems}

Playing two or more notes a the same time, i.e. playing a chord, can lead to timing problems because of MIDI's low bandwidth.

$$
\begin{aligned}
t_\text{Note-On}	&= 3\cdot\left(\SI{31250}{\bit\per\second}/\SI{10}{\bit}\right)^{-1}\\
					&= \SI{0.00096}{\second} = \SI{0.96}{\milli\second}
\end{aligned}
$$

The time to transmit a single note-on event $t_\text{Note-On}$ takes $\approx 1\,\text{ms}$, this means that the last transmitted note of an $n$-key chord arrives with $n \cdot 0.96\,\text{ms}$ delay, e.g. the last note of a pentachord (5 keys) will be received $5 \cdot 0.96\,\text{ms} = 4.8\,\text{ms}$ later than the first one.
This may result in a *comb filter*[^comb-filter] like distortion of the synthesized chord sound.

## Open Sound Control
\label{sec:osc}

The *UC Berkeley Center for New Music and Audio Technology* (CNMAT) originally developed, and continues to research, Open Sound Control.
In 2002, OSC's 1.0 specification was released.
It provides the following definition \cite{OSC:10}:

> Open Sound Control (OSC) is an open, transport-independent, message-based protocol developed for communication among computers, sound synthesizers, and other multimedia devices.

The protocol is not limited to being used with audio or multimedia devices, however, it is often used as a high-speed network replacement for MIDI.
Referring to OSC as a *message format* is more accurate, since error-handling, synchronization or negotiation methods are not specified.
Therefore, OSC can be compared to formats like JSON or XML.
A draft of the OSC 1.1 specification was published in a 2009 paper \cite{OSC:11} only adding minor, backward compatible changes.
UDP is often used as the transport layer to avoid the time required to establish a connection by TCP's three-way handshake.
A connection less transport is sufficient because OSC sender and receiver are almost always in physical proximity and connected through the same LAN.

### OSC Data Types
\label{subsec:osc-data-types}

\begin{table}[]
    \centering
    \caption{Overview of OSC 1.0 and 1.1 data types.}
    \label{tab:osc-data-types}
    \begin{tabular}{@{}cp{7.5cm}cc@{}}
    \toprule
    Tag     & Description                                                                & 1.0 Required & 1.1 Required \\ \midrule
    i       & \SI{32}{\bit} two's complement integer                                     & ✔           & ✔            \\
    f       & IEEE 754 single precision (\SI{32}{\bit})                                  & ✔           & ✔            \\
    s       & null-terminated sequence of ASCII characters                               & ✔           & ✔            \\
    b       & binary blob with size information                                          & ✔           & ✔            \\
    t       & OSC-timetag in NTP format                                                  &              & ✔            \\
    T/F     & boolean values: true, false                                                &              & ✔            \\
    N       & Nil                                                                        &              & ✔            \\
    I       & Infinitum (1.0)/Impulse(1.1) used as event trigger                         &              & ✔            \\
    d       & IEEE 754 double precision (\SI{64}{\bit})                                  &              &              \\
    h       & \SI{64}{\bit} big-endian two's complement integer                          &              &              \\
    S       & alternate string type                                                      &              &              \\
    c       & ASCII character                                                            &              &              \\
    r       & RGBA color (\SI{8}{\bit} per channel)                                      &              &              \\
    m       & $\SI{4}{\byte}$ MIDI message (from MSB to LSB): port, status, data1, data2 &              &              \\
    {[},{]} & Array delimiters                                                           &              &              \\ \bottomrule
    \end{tabular}
\end{table}

An overview of the predefined data types for both, OSC 1.0 and 1.1, is shown in \cref{tab:osc-data-types}.
The byte order of OSC's integer, float and timetags is big-endian.
OSC's unit of transmission is called *OSC Packet*.
The EBNF grammar for OSC packets is described by \cref{osc:grammar}.
Fields of an OSC packet have to be aligned to multiples of 4-byte and are zero-padded, thus the size of such a packet is also a multiple of four.
The packets contents can either be an *OSC Message* or *OSC Bundle*.
An OSC message starts with an *address pattern*  followed by zero or more *arguments* to be applied to the *OSC Method* matched by the pattern.
Address pattern can contain basic regular expression with single-/multi-character `?/*` wildcards, range `[A-Z]` and list matches `{foo, bar}`, hence multiple OSC Methods can be triggered with a single OSC Message.
An OSC Receiver's[^osc-naming] address space forms a tree structure with branch nodes called *OSC Containers* and leaves are named *OSC Methods*.
Methods are *`italicized`* in the tree structure example of \cref{fig:osc-address-space-example}.
The address of an OSC method starts with a `/`, followed by any container name along the path in order from the root of the tree, joined by forward slashes `/` and the method's name, e.g. `/oscillator/1/phase`.

\begin{figure}
	\centering
	\begin{tikzpicture}
[
	every node/.style = {font=\footnotesize\ttfamily},
	level 1/.style = {sibling distance = 5cm},
	level 2/.style = {sibling distance = 2cm},
	sloped
]
\node{/}
	child { node {oscillator/}
		child { node {1/}
			child { node [osc-method]{freq} }
			child { node [osc-method]{phase} }
		}
		child [sibling distance=20mm]{ node {2/}
			child { node {\ldots} }
		}
	}
	child { node {filter/}
		child { node [osc-method]{mode} }
		child { node [osc-method]{cutoff} }
		child { node [osc-method]{resonance} }
	}
	;
	\end{tikzpicture}
	\caption{OSC Address Space example.}
	\label{fig:osc-address-space-example}
\end{figure}

\begin{figure}
\caption{Grammar of an OSC packet described as EBNF (ISO14977 syntax \cite[p.~14]{ISO14977}).}
\label{osc:grammar}
\begin{verbatim}
packet			= size, content ;
size			= (* 4-byte aligned packet content field length *) ;
content			= message | bundle ;
message			= address, ",", { type-tag }, { argument } ;
address			= "/", osc-string - ( "'" | "#" | "*" | "," | "/" |
									  "?" | "[" | "]" | "{" | "}" ) ;
osc-string		= { ASCII }, "0" ;
type-tag		= "i" | "f" | "s" | "b" | "h" | "t" | "d" | "S" |
				  "c" | "r" | "m" | "T" | "F" | "N" | "I" | 
				  "{", {type-tag}, "}" ;
argument		= (* binary representation of the argument *) ;
bundle			= "#bundle", OSC-timetag , { bundle-element } ;
bundle-element	= size, content ;
\end{verbatim}
\end{figure}

### Comparison to MIDI
\label{subsec:osc-vs-midi}

Both protocols provide a number of benefits and limitations in comparison to each other.
The following list shows them for OSC compared to MIDI:

\begin{enumerate}
	\item[$+$] OSC's data-types allow a much higher resolution for control values.
	They also provide symbolic types like booleans or \emph{Nil} to represent an empty value.
	\item[$+$] The definition of \emph{custom data-types} is allowed, therefore OSC applications must be made robust against unknown ones.
	\item[$+$] The \emph{bandwidth} is orders of magnitudes larger than MIDI's, but it depends on the type of network used.
	A common choice are ad-hoc Wi-Fi connections between OSC receiver and sender because the player (sender) and the instrument (receiver) are in local proximity to each other.
	This, in turn, results in an acceptable network \emph{latency} in the single digit millisecond range.
	\item[$+$] Control events can be send simultaneously as an OSC bundle, e.g. note events of a chord.
	\item[$+$] Events can be timed with an resolution of $\approx 200$ picoseconds \cite{OSC:10}.
	\item[$+$] OSC can be used to tunnel MIDI messages over a network connection.
	\item[$-$] There is no standard for discovering OSC devices in a network, thus addresses must be configured manually which is cumbersome.
	\item[$-$] Unlike MIDI, there is no standard namespace for interfacing with an OSC device, although, a proposal for a standard exists \cite{synoscopy}.
	\item[$-$] The number of applications that support OSC is very limited.
\end{enumerate}

# Synthesizer Fundamentals
\label{ch:fundamentals}

This chapter outlines the fundamental elements of a synthesizer and briefly describes the fundamental methods of sound generation.

## Oscillator
\label{sec:oscillator}

Oscillators are the fundamental building blocks of a synthesizer's sound generation engine.
They serve the purpose of emitting a periodic waveform.
An oscillator is controlled through its *frequency* and *amplitude* parameters.
In the context of a synthesizer there are additional controls for starting *phase*, the point at which the waveforms begins, and *type of waveform* to emit.

The amplitude parameter sets the *peak amplitude* for the signal, i.e. the absolute value of the waveforms highest amplitude.
Frequency is usually specified as number of waveform cycles per second (Hz) but in the software implementation stored as *phase increment* (angular frequency) for each sample step.
The software oscillators output is a sequence of samples at equidistant intervals $T$.
Let $f_s=1/T$ be the sample rate and $f$ the frequency in Hz ($s^{-1}$), then the phase increment $\omega$ is calculated like this:

\begin{equation}
\omega = \frac{2\pi\,f}{f_s}
\label{eq:phase-incr}
\end{equation}

The oscillators highest frequency is limited to $f_s/2$ or $\omega = \pi$, which is called *Nyquist frequency*.
In general, the (Nyquist-Shannon) *sampling-theorem* states that a signal can be exactly reconstructed from its digitization if its entire frequency spectrum lies below the Nyquist frequency \cite[p.~244]{Benson2008}.
In other words, it ensures that there are at least two sample points for any frequency contained in the sampled signal.

### Aliasing
\label{subsec:aliasing}

\begin{figure}
\resizebox{\textwidth}{!}{
\includegraphics[width=\textwidth]{imgs/aliasing.pdf}}
\caption{Two sinusoids with angular frequencies $\omega_1 = \pi/2,\,\omega_2 = 3/2\,\pi$ sampled in intervals of $T=\pi/2$. Both sinusoids produce the same sampled signal due to aliasing.}
\label{fig:aliasing}
\end{figure}

\Cref{fig:aliasing} shows two sinusoids with frequencies $\omega_1=\pi/2$ and $\omega_2=3/2\,\pi$ that are sampled at sample rate $\omega_s=\pi/2$.
Clearly, $\omega_2$ is above the Nyquist frequency $\omega_{\text{Ny}}=\pi$, thus $\omega_2$ is *foldover* at $\omega_{\text{Ny}}$ which results in a frequency of $\omega_2\mod\omega_{\text{Ny}} = \pi/2$ that is equal to $\omega_1$, therefore $\omega_2$ is an *alias* of $\omega_1$, so both signals are indistinguishable after the sampling process.
This effect is called *aliasing* or *foldover* and is an inevitable result of sampling or sample rate conversion, hence signal components with frequencies above Nyquist must be removed or reduced before fed into the sampling process.
The effect of foldover in the frequency spectrum is shown in \cref{fig:foldover}.

The alias $f_a$ for a frequency $f$ and a sampling frequency $f_s$ can be calculated as shown in \cref{eq:aliasing-frequency}[^foldover-ref].

\begin{equation}
f_a =
\begin{cases}
|N\,f_s - f|,& \text{if }N\text{ is even}\\
|(N+1)f_s - f|,& \text{otherwise}
\end{cases}
\label{eq:aliasing-frequency}
\end{equation}

, where $N = \lfloor{}f/f_s\rfloor{}$.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/foldover}
\caption{Spectra for waveforms sampled at a) $f_{\text{Ny}}=30$ and b) $f_{\text{Ny}}=10$ which is $1/3$ of the highest frequency contained, hence foldover (aliasing) occurs.}
\label{fig:foldover}
\end{figure}

## Waveforms
\label{sec:waveforms}
Synthesis techniques like subtractive synthesis require a source signal with rich harmonic content, hence providing only sine wave oscillators is not sufficient.
Oscillator waveforms typically used in subtractive synthesis are *triangle*, *sawtooth* and *square wave* the so called *trivial* or *geometric waveforms* \cite[p.~5]{Pekonen2013} because of their well-defined shape consisting of piece-wise linear or constant segments.
Non bandlimited versions of those waveforms are shown in \cref{fig:waveforms}.
To prevent aliasing artifacts caused by discontinuities in the waveform (square and sawtooth) or its slope (triangle) it is required to create bandlimited versions of those waveforms.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/basic-waveforms}
\caption{Common (non bandlimited) waveforms supported by most synthesizers: a) sine wave, b) triangle wave, c) ramp/sawtooth, d) square wave.}
\label{fig:waveforms}
\end{figure}

### Fourier Synthesis
\label{subsec:fourier-synthesis}

*Fourier synthesis* is used to create bandlimited versions of those complex waveforms.
It uses the properties of *Fourier series* representation of arbitrary but periodic waveforms \cite[p.~103]{LoyMusimathics2}:

> Any periodic vibration, [\ldots], can be built up from sinusoids whose frequencies are integer multiplies of a fundamental frequency, by choosing the proper amplitudes and phases.

This means that any periodic waveform can be constructed by specifying the power of each its *harmonics*, where a *harmonic* is an integer multiple of the waveforms fundamental frequency.
The Fourier series is defined as (\cref{eq:fourier-series})

\begin{equation}
f(t) = a_0/2 + \sum^\infty_{k=1}a_k \cos(2\pi k t + \phi_k) + \sum^\infty_{k=1}b_k \sin(2\pi k t + \phi_k)
\label{eq:fourier-series}
\end{equation}

where $a_k$ and $b_k$ are coefficients for the strength of the k-th harmonic. The harmonics phase is specified $\phi_k$ and $\omega_k=2\pi{}k$ sets its angular frequency.
By substituting[^euler-ids] eulers equation (\cref{eq:eulers-equation})

\begin{equation}
e^{i\phi} = \cos(\phi)+i\sin(\phi)
\label{eq:eulers-equation}
\end{equation}

into \cref{eq:fourier-series} it can be written in complex form:

\begin{equation}
f(t) = \sum_{k=-\infty}^\infty c_n e^{i\,2\pi\,k\,t+\phi_k}
\label{eq:fourier-series-complex}
\end{equation}.

The summation range has changed for the complex Fourier series \cref{eq:fourier-series-complex} to $-\infty$ and $\infty$, thus there are now negative frequencies.
To retrieve a real-valued signal from the complex Fourier series it is required to take the conjugate transpose of each value, i.e. to specify the coefficients $c_k$ as pairs with a $-c_k$ for each positive coefficient.

The complex coefficients $c_k$ can be converted from the $a_k$, $b_k$'s with the following equation:

\begin{equation}
c_{\pm{}k}=1/2\left(a_k\pm{}i\,b_k\right)
\label{eq:fourier-coeff-conversion}
\end{equation}

<!-- conversion from complex to real
a_n = c_k + c_{-k} = 2\Re(c_k)
b_n = i\,(c_k + c_{-k}) = 2\Im(c_k)
-->

### Bandlimited Waveforms
\label{subsec:bandlimited-waveforms}

Bandlimited sawtooth, triangle and square waveforms can be created by means of Fourier synthesis.
Sawtooth, in contrast to square and triangle waves, contains odd and even harmonics which makes them a great source signal because of their rich harmonic content.
Square and triangle waves consist solely of odd harmonic partials.
The complex Fourier series for sawtooth, triangle and square waves is shown in \cref{eq:sawtooth-series}, \cref{eq:squarewave-series} and \cref{eq:triangle-series} where the sum is zero for $n=0$.
The number of harmonics contained in the waveform is determined by summation limits and \cref{fig:bandlimited-waveforms} illustrates evaluated Fourier series for sawtooth (a) and square wave (b) at increasing numbers of harmonics partials.

\begin{equation}
x_\text{saw}(t) = \sum^{\infty}_{n=-\infty,\;n\neq{}0} -1^n \frac{e^{-i\,2\pi\,n\,t}}{n\pi}
\label{eq:sawtooth-series}
\end{equation}

\begin{equation}
x_\text{square}(t) = 2 \sum^{\infty}_{n=-\infty,\;n\neq{}0}\frac{e^{-i\,2\pi\,(2n - 1)\,t}}{n\pi}
\label{eq:squarewave-series}
\end{equation}

\begin{equation}
x_\text{triangle}(t) = 4\sum^{\infty}_{n=-\infty,\;n\neq{}0} \frac{e^{-i\,2\pi\,(2n-1)\,t}}{(n\pi)^2}
\label{eq:triangle-series}
\end{equation}

\begin{figure}
\includegraphics[width=\textwidth]{imgs/fourier-series-saw.pdf}
\includegraphics[width=\textwidth]{imgs/fourier-series-sqr.pdf}
\caption{One cycle of bandlimited a) sawtooth and b) square waveforms with increasing number of harmonics.}
\label{fig:bandlimited-waveforms}
\end{figure}

## Non-linearity of hearing
\label{sec:hearing}

The intensity of a sound is perceived logarithmically by human hearing \cite[p.~27]{West1986}.
Therefore, the *ratio* between two sound intensities is important and *not* the difference as in the case of linear perceived phenomenons.
The ratio of two physical quantities, e.g. signal amplitudes, is measured in \si{\decibel} (decibel), a dimensionless logarithmic unit.
Distinction should be made between the ratio of signal energy which is expressed by

\begin{equation}
10\log_{10}{}\left(\frac{a}{b}\right)
\label{eq:db-energy}
\end{equation}

and the ratio of signal power \cref{eq:db-power}.

\begin{equation}
20\log_{10}{}\left(\frac{a}{b}\right)
\label{eq:db-power}
\end{equation}

with $a,b\in{}\mathbb{R}$.
The difference in the scaling factors is based on the definition of signal energy where the square of the signals amplitude is taken, see \cref{eq:signal-energy}.
\begin{equation}
\int^{\infty}_{-\infty} \left|x(t)\right|^2 dt
\label{eq:signal-energy}
\end{equation}

\begin{figure}
\includegraphics[width=\textwidth]{imgs/db.pdf}
\caption{Relationship between \si{\decibel} and corresponding amplitude ratios on a logarithmically scaled y-axis.}
\label{fig:db}
\end{figure}
\Cref{fig:db} illustrates the relationship between \si{\decibel} levels (x-axis) and corresponding amplitude ratios on a logarithmically scaled y-axis.
The pressure levels audible by human ears range from 0.00002 \si{\newton\per\meter\squared} to 200 \si{\newton\per\meter\squared} \cite[p.~26]{West1986}, which is seven orders of magnitude larger than the audibility threshold and clearly shows that logarithmic scale is better suited than a linear one to represent sound level ratios.

## Envelope Generators
\label{sec:envelope-generators}

The sound produced by an musical instrument is usually not static and changes in amplitude or spectral content over time.
To simulate these *time-varying* waveforms a function of time, the envelope generator, is used to controls parameters of an oscillator or other parts of a synthesizer's sound engine, e.g. the cutoff of a frequency filter.
In \cref{fig:piano-c4} the time-varying behavior of sounds produced by musical instruments is illustrated by an example of waveform plot of a C\textsubscript{4} note played on piano.

\begin{sidewaysfigure}
\includegraphics[width=\textwidth]{imgs/piano_c4_small.png}
\caption{Waveform plot of sampled C\textsubscript{4} note played on a piano \cite{freesound:piano}.}
\label{fig:piano-c4}
\end{sidewaysfigure}

There are various types of envelope generators that range from simple two stage models, for fading the sound in and out, to ones which have an arbitrary number of stages and envelope shapes.
A commonly used model with a reasonable amount of controllable parameters is the so called ADSR envelope, which stands for the four different stages of the envelope which are *Attack*, *Decay*, *Sustain* and *Release*.
Because of the non-linearity of human hearing, as discussed in \cref{sec:hearing}, it is not sufficient to linearly ramp values between those four stages because this would not yield a smooth change in perceived loudness.

\Citeauthor{Puckette2006a} proposes three different amplitude envelope transfer functions \cite[p.~94]{Puckette2006a} (see \cref{fig:envelope-transfer-functions}) where $10^{2(x-1)}$ converts from \si{\decibel} to linear and the quartic curve $x^4$ approximates the exponential \si{\decibel} curve while being computationally less expensive and reaching true zero at $x=0$.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/adsr-transfer-functions}
\caption{Three amplitude envelope transfer functions for input values in the range of [0, 1] as proposed by \cite[p.~94]{Puckette2006a}.}
\label{fig:envelope-transfer-functions}
\end{figure}

An ADSR generator's output is fully determined by five parameters, that are the output *level* and *duration* of the attack stage, decay duration, sustain level and duration of release.
\Cref{fig:adsr} shows the output and stages for an envelope generator with exponential transfer function.
The generator will start the output on an event like a key press and will reside in the sustain stage as long as the key is still pressed.
If the key is released the generator will switch to the release stage independent of its state at the time of the event.

\begin{sidewaysfigure}
\includegraphics[width=\textheight]{imgs/adsr}
\caption{An ADSR envelope with equal duration of 0.5\si{\second} for each stage and attack and sustain levels of 1 and 0.5.}
\label{fig:adsr}
\end{sidewaysfigure}


# Synthesis Techniques
\label{ch:synthesis-techniques}

Since the invention of the first electrical organ in 1894 \cite[p.~83]{Roads:CMT}, the *Telharmonium* built by Thaddeus Cahill, a lot of different synthesizing techniques have been developed.
Those techniques can be roughly divided into two broad categories, techniques for *mimicking the sound of traditional instruments*, like

- Karplus-Strong synthesis \cite{KarplusStrong}, a simple technique for simulating plucked-string or drum sounds
- physical modeling synthesis which uses a mathematical model of an instrument to generate sounds

or techniques for generating arbitrary sounds, possibly not reproducible by a physical instrument, like *Additive*, *Subtractive* and *FM* synthesis.
Developing a physical simulation of a traditional instrument is not the aim of this thesis, therefore the latter techniques will be described in this section.

## Additive Synthesis
\label{sec:additive-synthesis}

Additive synthesis is one of the oldest sound synthesizing techniques.
It uses separate sinusoidal oscillators to generate a complex sound from its partials.
As the name suggests, the output of each oscillator is added up to obtain the resulting output signal.
The basic structure of an additive synthesizer is shown in \cref{fig:additive-synthesizer}, where $∿_i$ denotes a sinusoidal oscillator with frequency input $f_i$ and amplitude input $a_i$.

An advantage of additive synthesis is its great versatility, because virtually any sound can be synthesized, given a sufficient amount of oscillators.
This comes with two major downsides:

1. this method is computationally expensive.
2. it is hard to control because there are at least as twice as many parameters as there are oscillators.

Additionally, to be able to simulate real or time-varying artificial sounds there must be functions that control those parameters over time, e.g. to reduce the amplitude of higher frequency partials when the sound decays.
The coefficients obtained by Fourier analysis of a real sound (e.g. a sample of played key on a piano) can be used as parameters to reconstruct this sound through additive synthesis, this process is sometimes called *Fourier recomposition* \cite[p.~88]{West1986}.

\begin{figure}
	\centering
	\begin{tikzpicture}
[
    auto,
    >=latex,
    node distance = 1cm
]
\node[block](osc1){$∿_1$};
\node[input, above=of osc1, xshift=-4mm](f1){$f_1$};
\node[input, above=of osc1, xshift=4mm](a1){$a_1$};
\draw[->](f1)--([xshift=-4mm]osc1.north);
\draw[->](a1)--([xshift=4mm]osc1.north);

\node[block, right=of osc1](osc2){$∿_2$};
\node[input, above=of osc2, xshift=-4mm](f2){$f_2$};
\node[input, above=of osc2, xshift=4mm](a2){$a_2$};
\draw[->](f2)--([xshift=-4mm]osc2.north);
\draw[->](a2)--([xshift=4mm]osc2.north);

\node[right=of osc2](oscX){\ldots};

\node[block, right=of oscX](oscN1){$∿_{n-1}$};
\node[input, above=of oscN1, xshift=-4mm](fN1){$f_{n-1}$};
\node[input, above=of oscN1, xshift=4mm](aN1){$a_{n-1}$};
\draw[->](fN1)--([xshift=-4mm]oscN1.north);
\draw[->](aN1)--([xshift=4mm]oscN1.north);

\node[block, right=of oscN1](oscN){$∿_n$};
\node[input, above=of oscN, xshift=-4mm](fN){$f_n$};
\node[input, above=of oscN, xshift=4mm](aN){$a_n$};
\draw[->](fN)--([xshift=-4mm]oscN.north);
\draw[->](aN)--([xshift=4mm]oscN.north);

\node[sum, below=of oscX](sum){$+$};
\draw[->](osc1.south)--(sum);
\draw[->](osc2.south)--(sum);
\draw[->](oscN1.south)--(sum);
\draw[->](oscN.south)--(sum);
\node[output, below=of sum](out){$y_n$};
\draw[->](sum)--(out);
	\end{tikzpicture}
	\caption{Basic structure of an additive synthesizer.}
	\label{fig:additive-synthesizer}
\end{figure}

## Frequency Modulation (FM) Synthesis
\label{sec:fm-synthesis}

Frequency Modulation (FM) was originally used in telecommunications to encode information on a carrier wave by modulating the waves instantaneous frequency, e.g. for radio broadcast.
In \citeyear{Chowning:FM}, \citeauthor{Chowning:FM} presented a new application of this well-known process to control spectral components of an audio signal with great simplicity \cite[p.~1]{Chowning:FM}.
Contrary to its well-understood use for radio transmission, both the *carrier* and the *modulating frequency* are inside the audio band.
The audio spectrum is formed by the carrier wave and side frequencies which are introduced through frequency modulation.
Modulation of the carrier wave is determined by two factors:

- the *frequency of the modulating wave* $m_f$ sets rate at which the instantaneous frequency of the carrier varies.
- the *amount of modulation* $m_a$ which is equal to the modulating waves amplitude.

If both the carrier as well as the modulator, are sinusoids then the instantaneous frequency maybe be calculated as follows \cite[p.~2]{Chowning:FM}:

\begin{equation}
y(t) = A \sin\left(c_f t + I \sin(m_f t)\right)
\label{eq:fm}
\end{equation}

where $A$ is the *peak amplitude*, $c_f$ is the carrier wave's frequency and $I = m_a/m_f$ is the ratio of modulation amount to modulation frequency also called *modulation index*. 
A table of waveforms generated by different values of $m_f$ and $m_a$ is shown in \cref{fig:fm-table}.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/fm-grid}
\caption{One cycle of a FM modulated sine wave for different modulation intensities $a_m$ and modulator frequencies $f_m$.}
\label{fig:fm-table}
\end{figure}

For $I=0$ there is no modulation, but non-zero values will result in frequencies occurring below and above the carrier frequency at intervals of the modulating frequency.
\citeauthor{Chowning:FM} describes the relation of modulation index and occurrence of side frequencies like this \cite[p.~2]{Chowning:FM}:

> The number of side frequencies which occur is related to the modulation index in such a way that as $I$ increases from zero, energy is \enquote{stolen} from the carrier and distributed among an increasing number of side frequencies.

This behavior is shown in \cref{fig:fm-bandwidth} for different modulation indices by constant modulation and carrier frequency. Negative amplitudes for frequency components indicate *phase inversion*[^phase-inversion].

\begin{figure}
\centering
\includegraphics[width=\textwidth]{imgs/fm-bandwidth}
\caption{Magnitude spectra for frequency modulated carrier frequency $c_f$ at different modulation indices $I$ and constant modulation frequency $m_f$. Bandwidth increases symmetrically around $c_f$ with $I$.}
\label{fig:fm-bandwidth}
\end{figure}

Specific carrier and modulation frequency ratios and modulation index values will produce sideband frequencies that fall into the negative spectrum.
Those negative frequency components will be reflected (aliased) around $\SI{0}{\hertz}$.
Reflected sideband components will either increase or---if they are phase inverted---decrease the energy in the spectrum.

Harmonic spectra[^overtones] will be generated if the ratio of carrier and modulation frequency is a rational number.
Ratios that are irrational numbers, e.g. $1/\sqrt{2}$, will result in inharmonic spectra because the reflected sideband frequencies will fall inbetween the positive frequency components.

Carrier and sideband component amplitudes can be determined analytically by evaluating n-th order Bessel functions $J_i$ of the first kind with the modulation index as argument.
A quick estimation for the resulting bandwidth of different modulation indices is shown in \cref{fig:fm-bessel} by evaluating Bessel functions $J_0$ through $J_{15}$ for those indices.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/fm-bessel}
\caption{Bandwidth estimation for modulation indices $I$ ranging from 0 through 20 by evaluating Bessel functions $J_0$ through $J_{15}$ and showing resulting sideband frequency $s_f$ amplitudes $s_A$ \cite[p.~5]{Chowning:FM}.}
\label{fig:fm-bessel}
\end{figure}

The most basic FM synthesizer *algorithm* consists of two *operators* (which are just oscillators in FM terminology), a modulator and a carrier, where the modulators output is summed with the carriers fundamental frequency.
An FM algorithm is described by how its operators are connected among each other.
\Cref{fig:fm-operator} shows the structure of the most basic FM algorithm, a simple pair of operators.

\begin{figure}
	\centering
	\begin{tikzpicture}
[
    auto,
    >=latex,
    node distance = 5mm
]
\node[block](modulator){$∿$};
\node[input, above=of modulator, xshift=-4mm](fm){$f_m$};
\node[input, above=of modulator, xshift=4mm](am){$a_m$};
\draw[->](fm)--([xshift=-4mm]modulator.north);
\draw[->](am)--([xshift=4mm]modulator.north);
\node[sum, below=of modulator](sum){$+$};
\node[input, right=of sum](fc){$f_c$};
\draw[->](modulator)--(sum);
\draw[->](fc)->(sum);
\node[block, below left=of sum](carrier){$∿$};
\draw[->](sum)-|([xshift=4mm]carrier.north);
\node[input, above=of carrier, xshift=-4mm](An){$A(n)$};
\draw[->](An)--([xshift=-4mm]carrier.north);
\node[output, below=of carrier](out){$y_n$};
\draw[->](carrier)--(out);
	\end{tikzpicture}
	\caption{Most basic FM algorithm, a pair of operators with one modulator and carrier.}
	\label{fig:fm-operator}
\end{figure}

FM's advantages lie in the simplicity of control, the small computational effort that is required and the great amount of flexibility, resulting from arranging operators in different algorithms.
On the other hand, FM synthesis is likely to introduce undesirable aliasing of higher frequencies which should be taken into account when implementing the algorithm.

## Subtractive Synthesis
\label{sec:subtractive-synthesis}

Subtractive Synthesis is---like the name implies---the opposite of *Additive Synthesis* and creates musical tones by *removing* parts of the frequency spectrum of a source signal.
A *filter* thereby amplifies or attenuates regions of the spectrum while the source signal passes through.
Spectrally rich signals like noise, sawtooth or square waves are well suited for the use as sound sources. The following section introduces filters by means of audio signal processing, describes commonly used filter types in musical synthesizers and discusses FIR and IIR filters.

# Digital Filters
\label{ch:digital-filters}

Filter is a broad term that has many different and often very general definitions.
Such a general definition is given by \citeauthor{Smith:FilterTheory} in \cite{Smith:FilterTheory}:

> Any medium through which the music signal passes, whatever its form, can be regarded as a filter.

Surprisingly, even \citetitle{Rabiner1972} \cite{Rabiner1972} uses the term without prior specification.
In this thesis a more specific definition will be used \cite[p.~326]{Proakis:DSP}:

> The linear time-invariant system, through its frequency response function, attenuates some frequency components of the input signal and amplifies other frequency components. Thus the system acts as a filter to the input signal.

This is analog to the description given in \cref{sec:subtractive-synthesis}, despite the terms *linear time-invariant* (LTI) system and *frequency response function* not having been explained.

## Linear Time-Invariant Systems
\label{sec:lti-systems}

LTI systems are used as filters because \enquote{no new spectral components are introduced} \cite{Smith:Filters} by them.
The time-invariance property is not overly restrictive because it also holds for filters that change slowly over time.
This is a very convenient property because, if musicians were not allowed to change parameters of a subtractive synthesizer's filter while playing, the result would be static and uninteresting sounds.

### Linearity and Time-Invariance
\label{subsec:lti}

A system is *linear* if the superposition principle \cref{eq:superposition} holds.

\begin{equation}
F[a_1 x_1(n) + a_2x_2(n)] = a_1F[x_1(n)] + a_2F[x_2(n)]
\label{eq:superposition}
\end{equation}

In other words, the response of system $F$ applied to two (or more) stimuli $x_{1,2}$ is equal to the sum of responses of the system applied to each stimulus individually, for any real valued scalars $a_{1,2}$ and points in time $n$.
This also shows the scaling the property of linear systems, i.e. scaling of a systems input results in an identical scaling of the response.
A system is *time invariant*, if

\begin{equation}
F[x(n-k)]=y(n-k),
\label{eq:time-invariance}
\end{equation}

for any time shift $k$.
Thus, the response of the system applied to a stimulus delayed by $k$ units of time is equal to the systems response delayed for the same amount.
Hence, if a system obeys both properties, linearity and time-invariance, then it is called an LTI system.
Such a system is characterized completely by its *impulse* or *frequency response*.

### LTI filters
\label{subsec:LTI-filters}

Linear time-invariant digital filters (systems), in the following simply called *filters*, may be written as *difference equation* 

\begin{equation}
y[n] = \sum^{M}_{i=0}b_i x[n-i]-\sum^{N}_{j=1}a_j y[n-j]
\label{eq:difference-equation}
\end{equation}

where $x$ denotes the input signal[^signal-sequence], $y$ the output signal, and the filter's *coefficients* are the constants $a_j$ and $b_i$.
A signal therefore is a *sequence* of real numbers denoted as a function of integer index $x[n]$ where $n$ denotes the *n*-th sample.
Coefficients $a_j, b_i$ must be in $\mathbb{R}$ to obtain a *real valued filter* that has a real valued output for any given real valued input signal.
Another requirement of the filter is to be *causal*, i.e. it does not depend on future values and only uses past input and output values to calculate its current output value, otherwise the filter can not be realized.
A filters *order* is the maximum sample delay used ($\max(M,N)$ in \cref{eq:difference-equation}) and in general the higher a filters order the steeper its transition slope.

Another way of representing a digital filter is by its rational system- or *transfer function* $H(z)$ in the *z*-domain as shown for a *causal* filter in \cref{eq:transfer-function} where $z = Ae^{i\phi}$ is some complex exponential with amplitude $A$ and phase $\phi$ that acts as time-shift of $j$ samples.
This *z*-domain representation will be required when designing a digital IIR filters based on an analog prototype.

\begin{equation}
H(z) = \frac{Y(z)}{X(z)} = \frac{\sum^{M}_{k=0}b_k z^{-k}}{1+\sum^{N}_{k=1}a_k z^{-k}}
\label{eq:transfer-function}
\end{equation}

The z-domain representation of a discrete-time signal $x[n]$ is defined as the bilateral transform:

\begin{equation}
X(z) = \mathcal{Z}\left\{x[n]\right\} = \sum^{\infty}_{n=-\infty}x[n]z^{-n}.
\label{eq:bilateral-transform}
\end{equation}

LTI filters can be divided into two types, first feedforward or *finite impulse response* (FIR) filters which only use previous input values ($a_j$s are zero) and second feedback or *infinite impulse response* (IIR) filters that also use previous output values to calculate the present filter output $y[n]$.

## Impulse Response
\label{sec:impulse-response}

Another way of representing a LTI system in the time domain is its response to a signal impulse, called the systems *impulse response*.

>   [A one-sample impulse] contains energy at all frequencies that can be represented at the given sampling frequency.
    Hence, a general way of characterizing a filter is to view its response to a one-sample pulse[\ldots] \cite[p.~400]{Roads:CMT}.

The hereby used input signal is the Kronecker delta function $\delta(n)$ \cref{eq:kronecker-delta} which is one if $n = 0$ and zero otherwise.

\begin{equation}
\delta(n) = \begin{cases}
1,\quad n=0,\\
0,\quad otherwise
\end{cases}
\label{eq:kronecker-delta}
\end{equation}

Applying the LTI system on the impulse signal $\delta(n)$ yields the systems impulse response denoted as $h[n]$.
If the impulse response does not reach zero over time the system is said to be *unstable*.
Any LTI system is fully described by its impulse response.
*Convolving* an input signal $x[n]$ with the impulse response $h[n]$ yields the systems time-domain output for that signal \cite[p.~73]{Proakis:DSP}:

\begin{equation}
y[n] = x\ast h = \sum^\infty_{k=-\infty}x[k]h[n-k] = \sum^\infty_{k=-\infty}x[n-k]h[n]
\label{eq:discrete-convolution}
\end{equation}

Clearly, convolution $\ast$ is a commutative operation.
This property can be used to optimize possible implementations \cite{Karas:Convolution}.

## Frequency Response
\label{sec:frequency-response}

Evaluating the systems transfer function \cref{eq:transfer-function} on the unit circle, i.e. set $z$ to $e^{i\omega T}$ where $T$ is the sampling interval, yields the LTI systems frequency response which is the frequency spectrum of the output divided by the frequency spectrum of the input.

It is easy to show that evaluating the (bilateral) z-transform on the unit-circle will find the spectrum because setting $z=e^{i\omega T}$ in \cref{eq:bilateral-transform} results in the definition of the bilateral discrete-time Fourier transform:

\begin{equation}
X\left(e^{i\omega T}\right) = \sum^{\infty}_{n=-\infty} x[n]e^{-i\omega n T}
\label{eq:DTFT}
\end{equation}.

In the following only *causal* sequences are of interest, thus the *unilateral* versions of z-transform and discrete-time Fourier transform are used in which the summation index starts at $n=0$.
Also, the sampling index $T$ is set to $1$ for simplicity.
Another way of obtaining an LTI systems frequency response is by applying the Fourier transform on the systems impulse response $h(z)$ \cite[p.~301]{Proakis:DSP}:

\begin{equation}
H(\omega) = \sum^\infty_{n=-\infty} h[n]e^{-i\omega n}
\end{equation}

It is sufficient to evaluate the frequency response function only for $\omega \in [-\pi,\pi)$ because all frequencies are mapped into a single cycle of the unit circle and the spectrum would repeat for additional cycles anyway[^see-aliasing].

### Magnitude- and Phase Response
\label{subsec:magnitude-phase-response}

The complex valued result of the frequency response function can be decomposed into two real valued functions \cite{Smith:Filters} the systems *magnitude response*[^magnitude-response] $|H(\omega)|$ and its *phase response* $\angle H(\omega)$, where $\angle$ denotes the complex argument.

The magnitude response of a filter shows how frequencies are attenuated or amplified and the phase response specifies the phase-shift experienced by each frequency.
In general the frequency response is of more interest because it is better suited for characterizing a filter but the phase response should not be completely ignored.

\citeauthor{Chamberlin:85} states that \enquote{Poor phase response in
a filter also means poor transient response} and this effect will become worse with increasing filter order \cite[p.~392]{Chamberlin:85}, i.e. sharp changes in a waveform (transients) will be smoothed by a filter with poor phase response.
However, even a poor filter phase response is quite good compared to the phase error introduced by the audio speaker while transforming the signal from an electrical to an acoustical one \cite[p.~392]{Chamberlin:85}.

## Filter Classification
\label{sec:filter-classification}

Filters of a musical synthesizer are classified by the magnitude curve of their frequency response function $|H(\omega)|$ which is the filter's characteristic *frequency response* curve.
Exemplary frequency response curves for lowpass, highpass, bandpass and notch (sometimes called bandreject or bandstop) filters are shown in \cref{fig:filter-classification}.

Low- and highpass filters cut all frequencies below, respectively above of the *cutoff frequency* $f_c$, while bandpass and notch filters let frequencies in a certain range (*frequency band*) pass through or rejecting them.
The width of the pass- or stopband is an additional property of those last two filter types and the difference between their high and low cutoff frequencies is called *bandwidth*.
Correspondingly, the center of the pass- or stopband---the point of maximum or minimum amplitude in this band---is the fiter's *center frequency*.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/filter-classification}
\caption{Log-log plots for exemplary frequency response curves of four elementary filter types with \SI{12}{\decibel\per\text{octave}} roll-off where $f_c$ denotes the cutoff frequency at the half-power point ($\SI{-3}{\decibel}$) shown as a dotted line.}
\label{fig:filter-classification}
\end{figure}

A filters cutoff frequency is commonly specified for the half-power point \cite[p.~8]{Rabiner1972} where the filter reduces the signals energy to $1/\sqrt{2}\approx 0.707$ or in terms of signal power $(1/\sqrt{2})^2 = 1/2$ or $\SI{-3}{\decibel}$ (see \cref{eq:db-power}).

An ideal filter would have a sharp cut between the pass- and stopband that looks like a rectangle in the frequency response but such a filter is not realizable because it would be of infinite order.
Hence, there is a transition between the pass- and stopband called *transition band*.
\Cref{fig:filter-bands} shows those bands and their respective bandwidths $b_{pass}$, $b_{trans}$, $b_{stop}$ as well as other constraints like pass- and stopband $a_{pass}$ and $a_{stop}$ that must be specified and taken into account when designing a filter.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/filter-bands.pdf}
\caption{Terminology for describing the frequency response of a low-pass filter.}
\label{fig:filter-bands}
\end{figure}

The steepness of the frequency response curve in the transition band is measured in $\si{\decibel\per\text{octave}}$ where a larger value implies an increased steepness of the curve,  e.g. a *roll-off* of $\SI{12}{\decibel\per\text{octave}}$ for a lowpass filter means that the amplitude is reduced by $\SI{12}{\decibel}$ for each doubling in frequency above $f_c$.

Depending on the curve of the phase reponse a filter is said to have *zero phase* if the phase-shift is constant over all frequencies, *linear phase* if there is a linear relationship between phase-shift and frequency and *non-linear phase* otherwise.
The phase response for the lowpass filter used in \cref{fig:filter-classification} is shown in \cref{fig:filter-phase}

\begin{figure}
\includegraphics[width=\textwidth]{imgs/filter-phase}
\caption{Non-linear phase response ($\angle H(\omega)$) of a second-order Butterworth lowpass filter.}
\label{fig:filter-phase}
\end{figure}

## FIR Filters
\label{sec:fir-filters}

A FIR filter's response to an impulse will die away after a finite period of time \cite[p.~406]{Roads:CMT}, hence the name *finite* impulse response filter.
The filter's structure is simply the sum of delayed and weighted samples where the weights for each delay are the coefficients $a_j$.
As described in \cref{subsec:LTI-filters}, the filters order is equal to the order of its transfer function polynomial, i.e. the total number of unit-sample delays it uses.
There are various methods for FIR filter design, e.g. window design method, frequency sampling or equiripple method \cite[p.664-690]{Proakis:DSP} and constraint-based linear programming algorithms like METEOR \cite{Steiglitz:METEOR}.

The general equation for a finite impulse response filter is equal to \cref{eq:difference-equation} when all recursive coefficients $a_k$ are zero:

\begin{equation}
y[n] = \sum^M_{k=0}b_k x[n-i] .
\label{eq:fir-general}
\end{equation}

Thus, the denominator of a FIR filter's transfer function is one, hence its polynomial

\begin{equation}
H(z) = \sum^{M}_{k=0}b_k z^{-k}
\label{eq:fir-transfer-function}
\end{equation}

has only zeroes but no poles.
This implies that FIR filters are *always stable*, i.e. a *bounded* (finite) input always results in a bounded output.
The transfer function \cref{eq:fir-transfer-function} may be written in factored form where each complex zero $q$ of the polynomial can be directly seen:

\begin{equation}
H(z) = (1-q_1~z^{-1})(1-q_2~z^{-1})\cdot\ldots\cdot(1-q_M~z^{-1})
\label{eq:fir-transfer-function-factorized}
\end{equation}

There can be less than $M$ factors if some of them cancel out.

If $z$ has the value of one of these factors $q$ then the transfer function evaluates to zero.
The positions of the zeroes in the complex $z$-plane of a 12-th order lowpass FIR filter are shown in \cref{fig:filter-zero-fir}.

By the complex zero's angle from the $z$-plane's origin is determined which frequencies are effected from it and its distance to the unit circle, on which the frequency response is evaluated, determines how large the attenuation is.
Frequencies are mapped on the unit circle counterclockwise starting from 0 at $(1, 0)$ and going to $\pi$, which translates to a frequency limit like the Nyquist frequency, at $(-1, 0)$ where positive frequencies are carried out in the upper half of the unit circle and negative frequencies in the lower half.
It can also be seen that complex zeroes $q=a+i\,b$ come in pairs; if they are not laying on the some point; where one of them is conjugate transposed $q^*=a-i\,b$, so that the imaginary parts cancel each other out to obtain a real valued filter response.

\begin{figure}
\centering
\includegraphics[width=.7\textwidth]{imgs/filter-zero-fir}
\caption{Zeroes of 12-th order lowpass FIR filter with cutoff frequency $f_c=\pi/4$.}
\label{fig:filter-zero-fir}
\end{figure}

## IIR Filters
\label{sec:iir-filters}

Infinite Impulse Response filters additionally use previous output values to calculate the recent result.
Therefore, an IIR's rational transfer function contains feedback coefficients $a_k$ \cref{eq:transfer-function} and can be viewed as $H(z) = H_1(z)H_2(z)$ \cite[p.~583]{Proakis:DSP}, where $H_1(z)$ consists of the zeroes of $H(z)$ and

\begin{equation}
H_2(z) = \frac{1}{1+\sum^N_{k=0}a_k z^{-k}}
\label{eq:iir-poles}
\end{equation}

consists of the poles of $H(z)$ \cite[p.~583]{Proakis:DSP}.
In contrast to FIR filters an impulse fed into an IIR filter will cause an infinite response---numerical quantization error ignored---because the transfer function will never truly reach zero due to the use feedback values, therefore the name *Infinite Impulse Response* filter.

IIR filters can become *unstable* because they can have poles outside of the complex plane's origin, e.g. if there is a pole on the unit circle at frequency $\omega_p$ then $H(z)$ would become $\infty$ when evaluated on the unit circle ($z=e^{i\omega}$) for $\omega_p$.

An advantage of the use of feedback values is that IIR filters can achieve much steeper transition band slopes than FIR filters of the same order, thus they are more efficient in the number of arithmetic operations and memory required.

The effect of transfer function poles in the $z$-plane is the opposite of zeros, i.e. the distance to the unit circle determines how much a frequency is amplified by the pole.
Musical synthesizers often allow the user to specify a *resonance* parameter for the filter cutoff frequency, where a low resonance results in smooth transition transition from the pass- to the transition band whereas a high value creates a peak at the cutoff frequency as shown for different resonance values in the magnitude response of a IIR second-order lowpass filter in \cref{fig:filter-resonance}.
Such a resonance parameter is easy to realize for IIR filters.

\begin{sidewaysfigure}
\includegraphics[width=\textwidth]{imgs/filter-resonance}
\caption{Magnitude response of a recursive second-order lowpass filter for different resonance values.}
\label{fig:filter-resonance}
\end{sidewaysfigure}

IIR filters are very sensitive to numerical rounding error, where the sensitivity depends on their order and implementation structure \cref{sec:iir-structure}.
Therefore, second-order systems, so called *biquads*, are used as building blocks for higher-order filters (order $>= 4$) because the filters sensitiviy to quantization increases with its order \cite[p.~132,589]{Proakis:DSP}.
Filters of odd-order a constructed as separate biquad and a single-order system whereas even-order filters are constructed solely from biquads.

Analog IIR filter design has a long history and is therefore a well researched and understood topic.
There are a number of commonly used analog filters with different characteristics \cite[p.~717pp]{Proakis:DSP}:

- *Butterworth* all-pole filters with monotonic frequency magnitude response in both pass- and stopband.
- *Chebyshev Type I* all-pole filters with equiripple behavior in the passband and monotonic characteristic in the stopband.
- *Chebyshev Type II* filters are like *Type I* except that they have monotonic passband characteristic and equiripple stopband behavior.
- *Elliptic Filters* with equiripple behavior in both pass- and stopband.

The filter design technique used in this thesis is to take an analog prototype filter and transform it into a discrete-time filter by using the *Bilinear transform*.
Other design techniques like *approximation of derivates* or *design by impulse variance* have the limitation that they are only valid for a limited set of filter classes \cite[p.~712]{Proakis:DSP}.

### Bilinear Transform
\label{subsec:bilinear-transform}

The Bilinear transform, defined by \cref{eq:bilinear-transform} for sampling interval $T$, is used to convert a transfer function of a continious-time LTI filter transfer function $H_a(s)$ into a transfer function of a discrete-time LTI filter $H(z)$, where $H_a(s)$ defined in the $s$-domain with $s=\sigma + i\Omega$.

\begin{equation}
s = \frac{2}{T}\left(\frac{1-z^{-1}}{1+z^{-1}}\right)
\label{eq:bilinear-transform}
\end{equation}

A continuous time filter defined in the $s$-plane is stable if all of its poles are located in the left-semi plane which is mapped by the bilinear transform into the unit circle, hence the transformed discrete-time filter is stable if all of its poles are located inside the unit circle of the z-domain.

The frequency relationship of the bilinear transform is non-linear, thus continuous time frequencies $\Omega \in [-\infty, \infty]$ are mapped from the $i\Omega$ axis of the $s$-plane into digital frequencies $\omega \in [-\pi, \pi)$ on the unit-circle by the following transformation (also called *frequency warping*)

\begin{equation}
\omega = \frac{2}{T} \arctan\left(\frac{\Omega T}{2}\right),
\label{eq:frequency-warping}
\end{equation}

and the inverse transformation is given by

\begin{equation}
\Omega = \frac{2}{T} \tan\left(\omega\frac{T}{2}\right).
\label{eq:frequency-warping-inverse}
\end{equation}

### Bilinear Transform Example
\label{subsec:bilinear-transform-example}

The following steps illustrate the general procedure of designing a filter using bilinear transform at the example of a second-order Butterworth lowpass filter with cutoff frequency $f_c=\SI{4000}{\hertz}$ for a sampling rate of $f_s = \SI{48}{\kilo\hertz}$, hence sampling interval $T = 1/f_s$:
 
- Pre-warp the critical frequencies, in this case the filter's cutoff frequency $\omega_c = 2\pi f_c \si{\radian\per\second}$:

\begin{equation*}
\Omega_c = \frac{2}{T}\tan\left(\omega_c\frac{T}{2}\right) \approx \frac{2}{T} 0.268 \si{\radian\per\second}
\end{equation*}

- Set the critical frequency $\omega_c$ and apply the bilinear transformation \cref{eq:bilinear-transform} to obtain $H(z)$ from the analog transfer function $H_a(s)$:

\begin{align}
H_a(s) &= \dfrac{\Omega_c^2}{s^2 + s\sqrt{2}\Omega_c+\Omega_c^2}\\
H(z)  &= \dfrac{\left(\dfrac{2}{T}0.268\right)^2}{
            \left(\dfrac{2}{T}\dfrac{1-z^{-1}}{1+z^{-1}}\right)^2
            + \dfrac{2}{T}\dfrac{1-z^{-1}}{1+z^{-1}}\sqrt{2}\dfrac{2}{T}0.268
            + \left(\dfrac{2}{T}0.268\right)^2}\\
      &= \dfrac{\left(\dfrac{2}{T}\right)^2 0.268^2}{
            \left(\dfrac{2}{T}\right)^2 \left(\dfrac{1-z^{-1}}{1+z^{-1}}\right)^2
            + \left(\dfrac{2}{T}\right)^2 \dfrac{1-z^{-1}}{1+z^{-1}}\sqrt{2}\cdot 0.268
            + \left(\dfrac{2}{T}\right)^2 0.268^2}\\
      &= \dfrac{0.268^2}{
            \left(\dfrac{1-z^{-1}}{1+z^{-1}}\right)^2 + \dfrac{1-z^{-1}}{1+z^{-1}} \cdot 0.379 + 0.268^2}\\
      &= \dfrac{0.0495 (1 + z)^2}{0.4775 - 1.2795 z + z^2}\cdot\dfrac{z^{-2}}{z^{-2}}\\
      &= 0.0495\cdot\dfrac{1 + 2z^{-1} + z^{-2}}{
          1 - 1.2795z^{-1} + 0.4775 z^{-2}}
\label{eq:iir-example-transfer-function}
\end{align}

- $H(z)$ is evaluated on the unit circle to check the magnitude frequency response \cref{fig:freq-response-iir-example} which shows the that the cutoff frequency lays exactly on the half-power point

\begin{figure}
\includegraphics[width=\textwidth]{imgs/filter-freq-resp-iir-example}
\caption{Magnitude frequency response of $H(z)$ \cref{eq:iir-example-transfer-function} showing cutoff frequency $f_c$ at half-power point (dotted line).}
\label{fig:freq-response-iir-example}
\end{figure}

- The pole-zero diagram \cref{fig:pole-zero-iir-example} of $H(z)$ shows that the pair of poles is located inside the unit circle, hence the filter is real-valued and stable. Also, a pair of zeros is located at the maximum frequency point which gives the lowpass characteristic.
	
\begin{figure}
\centering
\includegraphics[width=.7\textwidth]{imgs/filter-pole-zero-iir-example}
\caption{Pole-zero diagram of $H(z)$ \cref{eq:iir-example-transfer-function} with pole and zero positions marked as $\times$, respectively \bullet.}
\label{fig:pole-zero-iir-example}
\end{figure}

- Lastly, the filters difference equation \cref{eq:difference-equation} can be derived directly from $H(z)$: $y[n] = 0.0495 x[n] + 0.099 x[n-1] + 0.0495 x[n-2] + 1.2795 y[n-1] - 0.4775 y[n-2]$.

## Implementation Structures for IIR Filters
\label{sec:iir-structure}

An recursive LTI filter given as difference equation may be implemented as one of four *direct-form* (DF) filter implementations.
The direct-form is another way of representing a filter, besides impulse response and difference equation, with the benefit of directly representing its implementation structure.

The four direct-form structures are DF-I and DF-II, shown in \cref{fig:direct-form} where ${z^{-1}}$ denotes a unit-sample delay, and their *transposed* counterparts.
Transposed forms can be obtained from their DF-I or DF-II forms by reversing the signal path directions, replacing sums with branch-points and vice versa.
This operations does not effect the filters transfer function.

The technical properties of all four forms are different, despite that they represent the same transfer function.
A DF-I implementation saves addition operations at the cost of requiring twice as many delays (memory) as necessary while a DF-II implementation saves memory by sharing delays at the cost of possible fixed-point arithmetic overflow \cite{Smith:Filters}.
Transposed forms, TDF-I and TDF-II, have enhanced numerical robustness while obeying the same advantages and disadvantages of their fundamental structure.
An additional advantage of TDF-II structures is that they perform well in applications where the filter parameters change in audio rate, in other words as implementation for time-varying filters \cite{Wishnick:TVF}.

\begin{figure}
\includegraphics[width=.48\textwidth]{imgs/Biquad_filter_DF-I}
\hfill
\includegraphics[width=.48\textwidth]{imgs/Biquad_filter_DF-II}
\caption{Direct-Form I and II implementation of a second-order filter with the normalized (divided by $a_0$) difference equation $y[n]=b_{0}x[n]+b_{1}x[n-1]+b_{2}x[n-2]-a_{1}y[n-1]-a_{2}y[n-2]$ \cite{Wiki:digital-filter}.}
\label{fig:direct-form}
\end{figure}

## Comparison FIR against IIR
\label{sec:fir-vs-iir}

\begin{table}[]
    \centering
    \caption{Comparison of FIR against IIR filters.}
    \label{tab:fir-iir-comparison}
    \begin{tabular}{@{}rp{5cm}p{5cm}@{}}
    \toprule
\toprule
                 & FIR                        & IIR                                         \\ \midrule
Impulse Response & finite                     & infinite                                    \\
Magnitude response & arbitrary responses are easy to design, e.g. \emph{frequency sampling technique} \cite[p.~671]{Proakis:DSP} & often based on analog prototypes, arbitrary responses are hard to achieve   \\
Stability        & always stable              & feedback coeffecients can cause instability \\
Efficiency       & more memory and operations & less memory and operations                  \\
Linear phase     & always possible            & no technique available                      \\ \bottomrule
    \end{tabular}
\end{table}

A comparison between FIR and IIR filter design techniques is given by \cref{tab:fir-iir-comparison}.
For the synthesizer's filter an IIR design was chosen because of two requirements, first high computational efficiency to achieve short latencies, and an easy to implement parameter for controlling filter resonance.
Additionally, a Butterworth characteristic was picked because the phase response of Elliptic filters is more nonlinear and the equiripple behavior in pass- and/or stopband of Elliptic and Chebyshev characteristics is unfavorable.
It is not of great disadvantage for a musical synthesizers filter that a Butterworth filter rolls of more slowly at the cutoff frequency, because very sharp transition band steepness is not required.

# Oscillators and Waveform Synthesis
\label{ch:oscillators}

An oscillator is one of a synthesizer's fundamental building blocks because it is the source signal from which the desired sound is modeled from.
Typically, one or more oscillators are used as a signal source.
Subtractive synthesizers require spectrally rich source signals.
Therefore, oscillators must be able to generate a variety of waveforms other than pure sine, but at least the trivial ones listed in \cref{sec:waveforms}.
Requirements for an oscillator's waveform synthesis algorithm are to generate periodic bandlimited signals, to avoid aliasing and to be computationally efficient.
The latter requirement originates from the number of times the oscillator is called, this is at least once for each sample instant.
Depending on the amount of polyphony, i.e. the maximum number of parallel voices playable, they can even be called multiple times for each sample.

\citeauthor{Valimaki2006} (and others \cite[p.~28]{Pekonen2007}, \cite{Otalvar2015}) divide digital oscillator algorithms into three classes in regard to the amount aliasing left\cite{Valimaki2006}:

1. Ideal-bandlimited methods without harmonics above the Nyquist frequency, e.g. additive or wavetable synthesis
1. Quasi-bandlimited methods with low aliasing, e.g. *BLIT* and *BLEP* methods \cite{Brandt2001}
1. Alias-supressing methods, e.g. oversampling and filtering trivial waveforms

A fourth class, so called ad-hoc methods, that uses non-linear processing techniques is mentioned in more recent publications (\cite{Otalvar2015}, \cite{Pekonen2007}).
They are not of interest for this research because this class of methods is developed for very specific applications \cite[p.~26]{Otalvar2015}.

The aim of this chapter is to introduce the generic structure of a digital oscillator and to evaluate quasi-bandlimited methods against (fully) bandlimited wavetable synthesis.

## Generic Oscillator Structure
\label{sec:oscillator-structure}

A common structure that is used for a wide variety of oscillators is shown in \cref{fig:oscillator-structure}.
It consists of a *phase accumulator* which adds a *phase increment* $\varphi$ to itself each time a clock signal $t$ arrives.
The phase increment is given by the *fundamental frequency* $f_0$ as $\phi = 2\pi f_0/f_s$. <!-- TODO RENAME phase increment variable to something like phi_i
-->
Subsequently, an initial *phase offset* $\phi_0 \in [0, 2\pi]$ is added to the accumulators output.
The *phasor signal*

\begin{align}
\begin{split}
\phi[t]	&= \phi t \mod 2\pi\\
		&= \left(\phi[t - 1] + \phi\right) \mod 2\pi
\end{split}
\end{align}

for a discrete time variable $t$ wraps around on a full cycle. The second form shows the recurrence relation for the phasor signal where $\phi[t] = 0, \forall t \le 0$.
It is convenient to normalize $\phi[t]$ to a fraction of the waveform's period

\begin{equation}
\varphi[t] = \frac{\phi}{2\pi} t
\end{equation}

with $\varphi \in [0,1]$, e.g. to map the phasor to a wavetable lookup index.
The *wave* function maps the *phasor signal* to the desired wave shape and multiplies the output signal with a given amplitude *A*.

\begin{figure}
    \centering
    \begin{tikzpicture}
[
    auto,
    >=latex,
    node distance = 10mm
]
\node[input](clock){t};
\node[block, right=of clock](phasor){phase accu.};
\node[input, below=of phasor](freq){$f_0$};
\node[sum, right=of phasor](sum1){$+$};
\node[input, below=of sum1](phase){$\phi_0$};
\node[block, right=of sum1](wrap){wrap};
\node[block, right=of wrap](f){wave};
\node[input, below=of f](A){$A$};
\node[output, right=of f](y){y};
\draw[->](clock) -- (phasor);
\draw[->](freq) -- (phasor);
\draw[->](phasor) -- (sum1);
\draw[->](phase) -- (sum1);
\draw[->](sum1) -- (wrap);
\draw[->](wrap) -- node{$\phi[t]$}(f);
\draw[->](A) -- (f);
\draw[->](f) -- (y);
    \end{tikzpicture}
\caption{Block diagram of a generic oscillator.}
\label{fig:oscillator-structure}
\end{figure}

## Trivial Waveform Generation
\label{sec:trivial-waveform-generation}

The trivial way for generating geometric waveforms is to sample them without bandlimiting.
A sawtooth wave can be expressed by a *bipolar modular counter*

\begin{equation}
saw(t) = 2\varphi(t) - 1
\end{equation}

where

\begin{equation}
\varphi(t) = f_0 t \mod 1
\end{equation}

is a phasor signal (modular counter) for a continuous time variable $t$ in seconds \cite[p.~5]{Pekonen2013}.
An inverted sawtooth wave with a ramp that decreases from 1 to -1 can be obtained 
by

\begin{equation}
saw_{\text{invert}}(t) = 1 - saw(t).
\end{equation}

Both remaining waveforms, square and triangle, can be derived from sawtooth waves.
Rectangular waveforms can be produced by subtracting two sawtooth waveforms with a proper phase shift \cite[p.~22]{Valimaki2006}

\begin{equation}
rect(t)=saw(t) - saw(t - \frac{p}{f_0})
\end{equation}

where $p \in (0, 1)$ is the duty cycle.
Square waves are simply the symmetric case of rectangular pulses with 50% pulse width.
Another trivial way of generating a rectangular pulse is by comparing the output $x$ of bipolar modular counter with the pulse width $p$ as in the following closed form expression:

\begin{equation}
rect(x)=\begin{cases}
	1	& x < p\\
	0	& x = p\\
	1	& x > p
\end{cases}
\end{equation}

By taking the absolute value of a sawtooth wave one gets a inverted triangle in the range of zero to one, doubling and subtracting from one results in bipolar triangle wave as shown in the first form of \cref{eq:triangle-wave}.
The second form shows that integrating a square wave over time $t$ also results in a triangle wave which then needs to be scaled to a normalized range from -1 to 1 \cite[p.~7]{Pekonen2013}, hence a (scaled) square wave is the time derivate of a triangle waveform.

\begin{align}
\begin{split}
tri(t) 	&= 1 - 2|saw(t)|\\
		&= 4 f_0 \int^t_{-\infty}sqr(\tau)d\tau
		\label{eq:triangle-wave}
\end{split}
\end{align}

A straightforward digital implementation of those trivial waveforms is constructed by replacing the continuous-time phasor time signal with its discrete-time counterpart \cite[p.~8]{Pekonen2013}.
Unfortunately, those naive digital implementations suffer from severe aliasing distortion because the continous-time source signal of those geometric waveforms is not bandlimited, hence it contains an infinte number of harmonics as can be seen in their fourier-series representation (see \cref{eq:sawtooth-series}, \cref{eq:squarewave-series} and \cref{eq:triangle-series}).
The spectral tilt, i.e. the attenuation of harmonic partials with increasing frequency, is about $\SI{6}{\decibel}$ per octave for pulse and sawtooth waveforms and $\SI{12}{\decibel}$ per octave for triangle waveforms \cite[p.~11]{Pekonen2013}.
The steeper spectral tilt for triangle waveforms can be explained by their construction from pulse waves via integration which corresponds to the application of a first-order lowpass filter.
Thus, a trivial triangle oscillator implementation can be sufficient if implemented with two or more times oversampling depending on the amount of tolerable aliasing, especially for devices with very limited processing resources.

## Quasi-Bandlimited Waveform Synthesis
\label{sec:quasi-bandlimited-synthesis}

Quasi-bandlimited oscillator algorithms allow a certain degree of aliasing to be produced while making use of *psychoacoustic* effects like *masking*.
Auditory masking means how sensitivity for one sound is affected by the presence of another sound which is largely dependend on the intensity and spectrum of the sound that causes the masking \cite[p.~187]{Gelfand2010}, i.e. the human ear cannot differentiate between two sounds with roughly the same frequency spectrum if the intensity difference is large enough.
Accordingly, the harmonics of a waveform can mask the aliasing components in their spectral vicinity. 
The intensity of aliasing must be particularly reduced in the range of $\SI{1}{\kilo\hertz}$ to $\SI{5}{\kilo\hertz}$, because this is where human ears are most sensitive \cite[fig.~11.1]{Gelfand2010}.

### BLITs
\label{subsec:blit}

In \citeyear{Stilson1996a} \citeauthor{Stilson1996a} presented in their paper \citetitle{Stilson1996a} \cite{Stilson1996a} a method for synthesizing alias free geometric waveforms by integrating a *bandlimited impulse train* (BLIT).
Sawtooth, pulse and, of course, triangle waveforms can be derived from a pulse train by integration which is inherently a bandlimited operation.
Hence, it is sufficient to show how bandlimited impulse trains can be constructed by this method.
The naive way of discretizing an impulse train is by approximating each impulse with a unit-sample pulse.
The impulse trains period $p = f_s/f$ is rarely an integer, thus the locations of the unit-sample pulses must be approximated to the nearest sample instant.
\Cref{fig:impulse-train-approximation} (b) clearly shows the irregular intervals between unit-sample pulses the *pitch-period jitter* which adds noise to the signal \cite[p.~2]{Stilson1996a}.

\begin{sidewaysfigure}
\includegraphics[width=\textheight]{imgs/box-train-plot.pdf}
\caption{(a) Impulse train $\bullet$ with frequency $f=\SI{8.3}{\hertz}$ and sample positions $\triangle$. (b) The approximated unit-sample pulse train with sample positions $p=f_s/f$ rounded to the nearest integer.}
\label{fig:impulse-train-approximation}
\end{sidewaysfigure}

The naive discretization approach suffers from aliasing just as the trivial waveform generation method (see \cref{sec:trivial-waveform-generation}) because it is also not-bandlimited.
Hence, a more sophisticated method is needed.
<!-- TODO: two times the -->
The idea is to apply an ideal anti-aliasing filter before sampling the impulse train.
\Cref{fig:ideal-filter-response} shows the frequency response of an ideal anti-aliasing filter is a rectangle function in the frequency interval ($-f_s/2, f_s/2$) and a continuous-time impulse response that is a $\sinc$ function:

\begin{equation}
h(t) = sinc(f_s t) = \frac{\sin(\pi f_s t)}{\pi f_s t}
\label{eq:sinc}
\end{equation}

\begin{figure}
\includegraphics[width=\textwidth]{imgs/ideal-filter-plot.pdf}
\caption{(a) Impulse response and (b) frequency response of an ideal anti-aliasing (lowpass) filter where $f_{Ny}=f_s/2$ is the frequency of the Nyquist limit.}
\label{fig:ideal-filter-response}
\end{figure}

Applying the ideal filter $h(t)$ to the unit-amplitude impulse train of period $T_1$

\begin{equation}
x(t) = \sum^\infty_{k=-\infty} \delta(t=k T_1)
\label{eq:unit-impulse-train}
\end{equation}

by means of convolution gives a bandlimited signal $x_f(t) = (x \ast h)(t)$.
Thus, $x_f$ can now be sampled without aliasing which gives

\begin{equation}
y[n] = x_f(n T_s) = \sum^\infty_{k=-\infty} \sinc(n + k p)
\label{eq:blit}
\end{equation}

where $T_s = 1 / f_s$ is the sample period and $p=f_s/f$ as defined above.
The bandlimited discrete-time signal $y[n]$ can be interpreted as time-aliased $\sinc$ functions \cite[p.~5]{Stilson1996a}, i.e. every impulse is replaced with $\sinc$ response of the ideal filter.
Furthermore, \citeauthor{Stilson1996a} provided a closed-form expression for the *sampled bandlimited impulse train*:

\begin{equation}
y[n] = \frac{M}{P} \sinc_M\left(\frac{M}{P}n\right)
\label{eq:blit-closed-form}
\end{equation}

where

\begin{equation}
\sinc_M(x) = \frac{\sin(\pi x)}{M\sin\left(\pi x/M\right)}
\end{equation}

and $M$ is the number harmonics.
It is convenient to relate the number of harmonics $M$ to the period in samples $p$ as

\begin{equation}
M = 2 \lfloor P/2 \rfloor + 1
\label{eq:blit-harmonics-to-period}
\end{equation}

that is the largest odd integer smaller than the period \cite[p.~6]{Stilson1996a}.
However, the BLIT method cannot be implemented as is because the $\sinc$ function is infinitely long.

### BLIT-SWS
\label{subsec:blit-sws}

A realizable approximation that was proposed in the original paper by \citeauthor{Stilson1996a} is called *Sum of Windowed Sincs* (SWS) and will be discussed in more detail.
Since then, a lot more methods were developed but it is not in the scope of this thesis to give an overview about all of them.
For a very thorough analysis of alternative methods refer to \cite{Otalvar2015} or \cite{Pekonen2013}.

The difference between this realizable approach and the theoretical BLIT method in the previous section is that a window is applied to the ideal filter's impulse response to make it finite.
Hence, \cref{eq:blit} becomes

\begin{equation}
y_w[n] = \sum^\infty_{k=-\infty} w(n)\sinc(n + k p)
\label{eq:blit-sws}
\end{equation}

where $w(n)$ is a window function.
The choice of the window function determines the attenuation of harmonics in the spectrum.
This is advantageous for frequency sweeps in contrast to exactly bandlimited methods where harmonics pop in and out which can cause unwanted transients.
Aliasing is reduced by increasing the window length, in fact, a doubling in length approximately halves the transition band where most of the aliasing occurs.
\citeauthor{Stilson1996a} used a blackman window that spanned 32 zero crossings[^zero-crossing] of the $\sinc$ function \cite[fig.~10]{Stilson1996a} which attenuated aliasing to about $\SI{-90}{\decibel}$ for 80% of the spectrum.
The paper also proposed to use some oversampling to get a guard band in which the transition band can be moved by using a appropriate window length, e.g. for a sampling rate of $\SI{48}{\kilo\hertz}$ and a window length of 64 $\sinc$ zero crossings the transition band would span 10% of the spectrum which gives a nearly alias-free frequency range up to $0.9 \cdot\SI{24}{\kilo\hertz} = \SI{21.6}{\kilo\hertz}$.

A disadvantage of BLIT-SWS method is that the CPU consumption is proportional to the frequency because for each period a impulse must be inserted and replaced by the impulse response of the windowed filter ($\sinc$).
Furthermore, the $\sinc$ is centered on the impulse and must be mixed in several samples before the actual impulse arrives, thus lookahead is required \cite[p.~20]{Huovilainen2010}.
By controlling the window length a tradeoff between CPU usage and quality can be achieved.
Another feasible optimization is to tabulate the windowed $\sinc$ function and to retrieve its values by the use of an interpolated table-lookup.

### BLEPs
\label{subsec:blep}

\citeauthor{Brandt2001} proposed in \citeyear{Brandt2001} a method to synthesize hard synced oscillators without aliasing called *bandlimited step* (BLEP) \cite{Brandt2001}, an extension of BLIT.
Hard sync is the phase synchronisation of two oscillators with frequencies $f_1, f_2$ where a slave oscillator's phase is reset with each period of $f_1$.
Clearly, this adds discontinuities to the synthesized waveform which cause aliasing if the waveform is sampled without bandlimiting.
The method is not limited to hard-sync instead it can synthesize bandlimited versions of arbitray waveforms with discontinuities, like the geometric waveforms, if the derivates of the waveform are continuous across the point of discontinuity \cite[p.~3]{Brandt2001}.

The method improves BLIT in two ways, first it almost removes the lookahead to the center of the impulse by using a minimum-phase impulse, and second, it removes the integration at run-time by pre-integrating the bandlimited step.
The minimum-phase impulse is considered as a minimum phase FIR filter, i.e. all zeroes are located inside ($|z| < 1$) the unit 
circle, with the impulse response of a windowed sinc.
Integrating the minimum-phase impulse results in a minimum-phase bandlimited step (MinBLEP).

Waveforms are synthesized by producing their naive non-bandlimited shape and mixing in a MinBLEP each time a discontinuity in the waveform occurs.

Advantages of MinBLEP in contrast to BLIT-SWS are that former method removes numerical error and computational costs by avoiding the numerical integration of the impulse train at run-time and the lookahead is reduced to several samples of the bandlimited step.
Also, the pre-integration step reduces aliasing by another 6dB over BLIT-SWS.
However, the CPU usage is still proportional to the oscillators frequency.

## Ideal Bandlimited Waveform Synthesis
\label{sec:ideal-bandlimited-synthesis}

Ideal bandlimited methods generate waveforms with a finite number of harmonics.
The number of harmonics is limited by the highest harmonic frequency less than the Nyquist limit, hence, the waveforms are completly alias-free.
A multitude of ideal bandlimited methods were developed, e.g. additive synthesis (\cref{sec:additive-synthesis}), feedback delay loops (FDL) and discrete summation formulae (DSF).
DSF's are solely using properties of trigonometric functions to synthesize the waveform and are not considered for the synthesizer of this thesis because evaluating trigonometric functions is quite CPU intensive.
Feedback Delay Loop's are a relatively new method (first published in \citeyear{Nam2009}) and, unfortunately, are not taken into account since I have discovered this method at a very late time of editing and the implementation effort is unclear[^oscillator-ref].
Pure additive synthesis is the most CPU intensive of those three because it requires to sum a sinusoidal oscillator for every harmonic to be synthesized, therefore a wavetable based approach is evaluated which uses lookup-tables containing bandlimited cycles of the waveform.

### Wavetables
\label{subsec:wavetables}

Wavetable oscillators can generate *arbitrary* static harmonic spectra.
Nonetheless, dynamic spectra can be generated by crossfading the output of two detuned or different wavetable oscillators \cite[p.~41]{Frei2010} or by applying a time-varying filter to the output \cite[p.~37]{Huovilainen2010}.
The basic idea behind wavetable oscillator's is simple, a single cycle of an arbitrary waveform is sampled and stored into an array of memory locations and looped at different speeds to simulate playback at a different pitch.
The fundamental frequency $f_0$ of a wavetable oscillator's output signal is given by

\begin{equation}
f_0 = \frac{\varphi~f_s}{N}
\label{eq:wavetable-freq}
\end{equation}

where $f_s$ is the sample rate, $N$ the table length and $\varphi$ the phase or table increment, with a special case for $\varphi=1$ called *natural fundamental* or $f_{nat}$ \cite[p.~41]{Frei2010}.
\Cref{fig:wavetable-single-cycle} shows the wavetable of a sine wave sampled at 24 equidistand points with a natural fundamental $f_{nat} = \SI{48}{\kilo\hertz} / 24 = \SI{2}{\kilo\hertz}$ for a sample rate $f_s = \SI{48}{\kilo\hertz}$.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/table-lookup.pdf}
\caption{Wavetable for a single sine wave cycle sampled at 24 equidistant points. The bottom x-axis shows the index of the sample in the wavetable and the top shows angle in radians.}
\label{fig:wavetable-single-cycle}
\end{figure}

The oscillators output signal loses clarity if the wavetable is played back with a frequency that is lower than $f_{nat}$, thus the wavetable should not be too short.
The oscillators table increment

\begin{equation}
\varphi = \frac{N~f_0}{N}
\label{eq:table-increment}
\end{equation}

is usually not an integer, thus the lookup index

\begin{equation}
n = t~\varphi\mod{}N
\label{eq:lookup-index}
\end{equation}

also has a fractional part.
Therefore, some kind of interpolation is needed to map the lookup index $n$ onto a integer valued table index.
The accuracy of the table lookup depends on two factors, the interpolation scheme that is used and the stored waveform cycle's length.
Interpolation schemes may be evaluated by comparing the error of the interpolated output that is the difference between the *ideal* waveform function's value at some point and the interpolated value at the same point.
It is common practice to use a sinusoid as the ideal waveform because other waveforms can be interpreted as superpositions of those.
The error for zero- and first-degree interpolation of a sine wavetable with length $N = 64$ is shown in \cref{fig:interpolation-error}.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/interpolation-error.pdf}
\caption{Error of zero-degree (nearest neighbour) and first-degree (linear) interpolation for a wavetable of 64 samples.}
\label{fig:interpolation-error}
\end{figure}

Polynominal interpolation schemes, which are the only ones considered here, are determined by their order $n$, i.e. a n-th order polynomial passes through the $n+1$ nearest points in the wavetable.
A zero-degree (or nearest-neighbour) interpolation simply rounds the index to the next integer.
A first-order (or linear) interpolation uses a linear function to approximate the table output

\begin{equation}
y_{lin}(n) = y[n_0]+(y[n_0 +1]−y[n_0])(n−n_0)
\end{equation}

where $n_0 = \lfloor n \rfloor$.
Higher order interpolation schemes can be considered for systems where memory is scarce, otherwise linear interpolation performs well for $N\geq 64$ samples.
In fact, the root-mean square (RMS) lookup error decreases by $\SI{12}{\decibel}$ each time the table length doubles \cite[p.~44]{Puckette2006a}, hence the RMS error of a linear interpolated wavetable with 64 samples length is $\approx\SI{-64}{\decibel}$ (\cref{fig:interpolation-error} shows the non-RMS error which is $\approx\SI{-58.3}{\decibel}$).

For sine, where the sole harmonic is also the fundamental, a single table is enough but for the geometric waveforms more tables are required.
Each of those tables spans only frequencies in the vicinity of $f_{nat}$ because pitch shifting the table by a large factor ($>2 f_{nat}$) can introduce severe aliasing whereas shifting it to a low frequency $<0.75f_{nat}$ may result in a dull sound because too many harmonics are missing.
Hence, a table for each octave of the desired frequency range should be used which is then selected a run time depending on the note to be played.
A table switch causes a sudden drop in spectral energy, as can be seen in \cref{fig:wavetable-sweep}, which can be perceived for slow frequency sweeps.
This effect can be alleviated by increasing the number of tables at the cost of memory.

It is convenient to design the waveforms spectra directly in the frequency domain and convert it back into the time domain by using the inverse discrete-time fourier transform that is \cref{eq:IDFT} from the transform pair given below where $\omega = 2\pi f$ is the angular frequency as usual.

\begin{align}
X[f] &= \sum_{n=0}^{N-1}x[n]e^{-i\omega n}
\label{eq:DFT}\\
x[n] &= \frac{1}{N}\sum_{f=0}^{N-1}X[f]e^{i\omega n}
\label{eq:IDFT}
\end{align}

To get a real valued signal it must be ensured that $X[k] = X^*[N-k]$, i.e. the spectrum is conjugate symmetric (mirrored), where $N$ is the table size.
This process is illustrated by \cref{fig:saw-table} and \cref{fig:sqr-table} for wavetables of length $N=128$ where each iteration doubles the number of harmonics.

\begin{figure}
\includegraphics[width=.9\textwidth]{imgs/saw-table.pdf}
\caption{Harmonic spectra and time domain representation for a sawtooth waveform.}
\label{fig:saw-table}
\end{figure}

\begin{figure}
\includegraphics[width=.9\textwidth]{imgs/sqr-table.pdf}
\caption{Harmonic spectra and time domain representation for a square waveform.}
\label{fig:sqr-table}
\end{figure}

## Conclusion
\label{sec:oscillator-conclusion}

A wavetable based approach was selected for the synthesizer's oscillators because of its simple implementation, high audio quality and great flexibility that allows to easily support any arbitrary waveform.
Nonetheless, a mixed approach that uses BLEP based oscillators for geometric waveforms and wavetables for arbitray or user defined spectras would be a desirable optimization.

# Implementation Details and Evaluation
\label{ch:implementation-evaluation}

The following chapter explains implementation details of the software synthesizer that was developed in this thesis and evaluates the implementation with respect to latency in \cref{subsec:system-latency}, aliasing behaviour of the wavetable oscillators in \cref{sec:wavetable-oscillator-implementation} and the time-varying behaviour of the multi-mode filter in \cref{sec:filter-implementation}.
In the first section is explained why Rust was chosen as the implementation language and the important terms *real-time* and *latency* are defined in \Cref{sec:real-time} and \cref{sec:latency}, followed by an followed by an overview of the synthesizer's structure in \cref{sec:synthesizer-structure}.
Thereafter, all components of the structure are described in \cref{sec:input-implementation} to \cref{sec:audio-output-implementation}.

The full source code of the synthesizer and its libraries is published as open source software available in \cite{github:ytterbium}, \cite{github:rb}, \cite{github:rsoundio} and \cite{github:rosc}.

## Why Rust?
\label{sec:why-rust}

Real time audio applications must finish their signal processing in tight time bounds to ensure that the sound card can output a continuous audio stream.
Missing such a bound will result in unpleasant sound glitches, and---in the worst case---renders a whole song recording useless.
Additionally, the time bounds must be as tight as possible to reduce the latency (see \cref{sec:latency}) between user input (playing a note) and output of the calculated signal from the speaker.
Therefore, (memory-)managed languages like [Java](https://en.wikipedia.org/wiki/Java_(programming_language)), [Go](https://golang.org) or [C#](https://en.wikipedia.org/wiki/C_Sharp_(programming_language)) that use a garbage collector, which can produce non deterministic program stops while examining the state of variable references, will not receive any further consideration.
Interpreted languages like [Python](https://www.python.org/) or [Ruby](https://www.ruby-lang.org/en/) face the same problems as compiled managed languages by also having increased runtime costs.
Audio application development in those languages is still possible but commonly requires to write the signal processing as C modules and interface with them through the languages foreign function interface (FFI) (e.g. pyo \cite{pyo} uses this approach). 
Unmanaged languages like [C](https://en.wikipedia.org/wiki/C_(programming_language)) and [C++](https://en.wikipedia.org/wiki/C%2B%2B) offer the control over memory that is needed to make reliable claims about the runtime behavior while avoiding the overhead of an additional runtime environment.
This comes with a downside in memory safety and introduces a whole new class of possible bugs compared to managed languages.
Those bugs are very likely to cause *undefined behavior* or to crash the program. Unfortunately, they are also very hard to debug.

One of the major selling points of [Rust](https://rust-lang.org) is guaranteed memory safety *without* garbage collection and *data race freedom* while providing the same level of control over memory as C/C++.
These goals are achieved through a variety of concepts, like ownership, lifetimes and borrowing, to know at compile when memory can be safely freed, and to enforce that there is only a single mutable access to any variable at any given time in the run of the program.
Explaining these concepts is outside the scope of this thesis and the official [Rust Book](https://doc.rust-lang.org/book/ownership.html) \cite{RustBook} does a great job doing this in detail, hence, this is left as an exercise for the interested reader.

The [Max Planck Institute for Software Systems](http://plv.mpi-sws.org/) has started the \cite{RustBelt} RustBelt research project in 2015 to develop formal foundations for the Rust programming language.
One of the main goals of the research group is to formally investigate if the claims made about data-race freedom, memory and type safety actually hold.
As of the time of writing there is no evidence that the claims are untrue, therefore it's assumed that they actually hold.

## Real-Time
\label{sec:real-time}

There are different forms of real-time computing in computer science but common to all of them is that computations have to be finished in a predefined amount of time and the shorter this time span is the faster those computations have to be.
The definition of real-time used in the context of this thesis is of \citeauthor{Rabiner1972} \cite[p.2]{Rabiner1972}:

> A real-time process is one for which, on the average, the computing associated with each sampling interval can be completed in a time less than or equal to the sampling interval.

Based on this definition, the time to compute a single sample should not exceed $1/f_s = \SI{20.833}{\micro\second}$ for a common sampling rate of $f_s = \SI{48}{\kilo\hertz}$.
Nonetheless, sample calculation time should be much shorter than this limit to compensate for delays introduced by the operating system, e.g. context switches caused by the process scheduler.
Multimedia applications can greatly benefit from a custom process scheduler like \citeauthor{Kolivas:MuQSS}'s *Brainfuck Scheduler* or the more recent implementation called *Multiple Queue Skiplist Schedule* \cite{Kolivas:MuQSS} because those schedulers are optimized for process responsiveness on symmetric multiprocessing platforms like desktops or laptops unlike the default *Completely Fair Scheduler* \cite{Molinar:CFS} which aims to maximizes for overall CPU utilization and must fit for a wide variety of use cases.

## Latency
\label{sec:latency}

The *responsiveness* of an electronic musical instrument is mainly determined by its latency, i.e. the time delay between two causally connected events.

An instrument is the more responsive, the less latency between an input event and the corresponding sound output it has.
The latency is imperceptible for the user if the delay between the input event and audio output stimuli is less than $\SI{24}{\milli\second}$ \cite{NasaLatency}.
Furthermore, there is an even stronger latency limit of $\approx\SI{2}{\milli\second}$, that is the *temporal resolution* of the human hearing, as shown by \cite[p.~294]{FastlZwicker} using psycho acoustic measurements.
However, it is not realistic to use this as an upper limit for the synthesizer's system latency, considering that the sound propagation delay from a speaker to a listener at a speed of sound $v = \SI{343.2}{\meter\per\second}$ at normal temperature[^stdTemp] and a common listening distance of $d = \SI{2}{\meter}$ is about 3 times larger than the temporal resolution $d/v = \SI{2}{\meter}/\SI{343.2}{\meter\per\second} = \SI{5.82}{\milli\second}$.
Therefore, achieving a system latency of less than $\SI{24}{\milli\second}$ is favorable.

The sum of two delayed audio signals can act like a comb filter \cite[p.~1]{AES:Latency}, such that even very short latencies can cause spectral artifacts in applications that monitor input signals.
Fortunately, there is no input audio signal for this synthesizer, thus only temporal issues must be taken into account.


### System Latency
\label{subsec:system-latency}

The overall latency of the synthesizer is the sum of different latency portions made up a variety of components in the audio output chain.
At first there is the control input latency that is either that of the MIDI or OSC connection, where the OSC latency greatly depends on the network connection's physical layer, e.g. Bluetooth, Wi-Fi or USB.
Usually a Wi-Fi connection is used to connect an OSC client, like a tablet running [liine's lemur](https://liine.net/de/products/lemur/) or some other OSC capable controller application.
A USB connection is recommended to get reliable network latencies between an OSC client and the synthesizer but an ad-hoc Wi-Fi connection that is exlusively used for transporting OSC messages can also work well.
Another latency portion is introduced through the synthesizers and the audio backends output buffer.
The backend's audio buffer size can be configured in most cases except for some backends like PulseAudio which is not recommended for real-time use anyway.
Lastly, there is a propagation delay which can be neglected for usual listening distances or is near zero if headphones are used.

Measuring the system latency is hard because an experimental setup is needed that triggers the input device and starts a sound recording at the same time.
Such a setup is error prone because the time for triggering a piano key of the MIDI keyboard and the input buffer of the sound card must be compensated.
Summing the input latency and delays introduced by the internal audio buffers gives a good approximation without the errors of an experimental setup.
The overall latency $l$ can be approximate by evaluating

\begin{equation}
l = l_{in}+\frac{b_{syn}+b_{out}}{f_s}
\label{eq:delay-approximation}
\end{equation}

where $l_{in}$ is the input latency, $f_s$ the sample rate and $b_{syn}, b_{out}$ are the synthesizer's and sound card's buffer sizes in samples.
Given a sample rate of $\SI{48}{\kilo\hertz}$, an input latency of $\SI{1.3}{\milli\second}$ and buffer sizes of $b_{syn}=512, b_{out}=256$ inserted in \cref{eq:delay-approximation} results in a system latency
$l 	= \SI{1.3}{\milli\second}+(512 + 256)\cdot(\SI{48}{\kilo\hertz})^{-1}
	= \SI{17.3}{\milli\second}$
that is about \textthreequarters{} of the $\SI{24}{\milli\second}$ limit and thus sufficiently small to be unperceivable.
Both buffer sizes were used without buffer underruns on the development machine, a HP elitebook 8460p equipped with an intel\textregistered{} i5-2520M running Arch Linux kernel 4.8.15-2-ck-sandybridge and JACK2 \cite{Jack2} as audio backend.
The input latency was measured by taking the average round-trip times of 100 ICMP echo requests (ping) between an OSC client, an android tablet running liine's Lemur OSC app, connected through an ad-hoc Wi-Fi connection and the computer running the synthesizer application.

## Structure
\label{sec:synthesizer-structure}

The synthesizer's structure consists of mainly three parts.
In the first part, incoming user input, as MIDI or OSC messages, is processed and transformed into user events which are then send to the appropriate component addressed by the input message.
Signal generation takes place in the second part which also handles user events, e.g. by triggering voices on a note input or modifying parameters of a component like changing the filter's cutoff frequency.
Then the generated signal is fed into a ring buffer (see \cref{sec:ring-buffer}).
Lastly, in the third part, the ring buffer is read-out by a callback from an audio backend that writes the data into the sound card's audio buffer.
The overall structure of the synthesizer is depicted in \cref{fig:basic-synth-structure}.

\begin{sidewaysfigure}
	\centering
    \resizebox{\textheight}{!}{
	\begin{tikzpicture}
[
    auto,
    >=latex,
    node distance = 1cm
]
\node[align=center](control){Control\\Input};
\node[matrix, row sep=5mm, right=of control](voices){
    \node[draw](voice1){
        $\text{voice}_1$
        \begin{tikzpicture}
            \node[block](osc){$∿$};
            \node[left=of osc, yshift=4mm](freq){freq};
            \node[draw, left=of osc, yshift=-4mm](adsr){envelope};
            \draw[->](freq)--([yshift=4mm]osc.west);
            \draw[->](adsr)--node[above, midway]{A}([yshift=-4mm]osc.west);
        \end{tikzpicture}
    };\\
    \node[draw](voice2){
        $\text{voice}_2$
        \begin{tikzpicture}
            \node[block](osc){$∿$};
            \node[left=of osc, yshift=4mm](freq){freq};
            \node[draw, left=of osc, yshift=-4mm](adsr){envelope};
            \draw[->](freq)--([yshift=4mm]osc.west);
            \draw[->](adsr)--node[above, midway]{A}([yshift=-4mm]osc.west);
        \end{tikzpicture}
    };\\
    \node[](voiceX){\vdots};\\
    \node[draw](voiceN1){
        $\text{voice}_{n-1}$
        \begin{tikzpicture}
            \node[block](osc){$∿$};
            \node[left=of osc, yshift=4mm](freq){freq};
            \node[draw, left=of osc, yshift=-4mm](adsr){envelope};
            \draw[->](freq)--([yshift=4mm]osc.west);
            \draw[->](adsr)--node[above, midway]{A}([yshift=-4mm]osc.west);
        \end{tikzpicture}
    };\\
    \node[draw](voiceN){
        $\text{voice}_n$
        \begin{tikzpicture}
            \node[block](osc){$∿$};
            \node[left=of osc, yshift=4mm](freq){freq};
            \node[draw, left=of osc, yshift=-4mm](adsr){envelope};
            \draw[->](freq)--([yshift=4mm]osc.west);
            \draw[->](adsr)--node[above, midway]{A}([yshift=-4mm]osc.west);
        \end{tikzpicture}
    };\\
};
\draw[->](control.north)|-(voice1.west);
\draw[->](control.north)|-(voice2.west);
\draw[->](control.south)|-(voiceN1.west);
\draw[->](control.south)|-(voiceN.west);
\node[sum, right=of voices](sum){$+$};
\draw[->](voice1.east)-|(sum.north);
\draw[->](voice2.east)-|(sum.north);
\draw[->](voiceN1.east)-|(sum.south);
\draw[->](voiceN.east)-|(sum.south);
\node[block, right=of sum, align=center](filter){Multi mode\\Filter};
\draw[->](sum)--(filter);
\node[block, right=of filter,align=center](ringbuffer){Ring\\Buffer};
\draw[->](filter)--(ringbuffer);
\node[right=of ringbuffer,align=center](out){Output\\Device};
\draw[->](ringbuffer)--(out);
	\end{tikzpicture}}
	\caption{Structure of a basic polyphonic subtractive synthesizer.}
	\label{fig:basic-synth-structure}
\end{sidewaysfigure}

## Control Input
\label{sec:input-implementation}

MIDI and OSC (see \cref{sec:midi} and \cref{sec:osc}) protocol support is provided by two libraries, \citetitle{github:portmidi-rs} \cite{github:portmidi-rs} and \citetitle{github:rosc} \cite{github:rosc}.
The first library, \citetitle{github:portmidi-rs}, is a safe Rust wrapper around the cross-platform real-time MIDI C library \citetitle{portmidi} \cite{portmidi}.
The word *safe* in this context means that memory-safety is guaranteed by the fact that there are no unsafe operations for the use of \citetitle{github:portmidi-rs}' API necessary, like e.g. pointer dereferencing.
Furthermore, \citetitle{github:portmidi-rs} was completly rewritten \cite{github:portmidi-rs:rewrite} in order to follow Rust's language idioms more strictly, i.e. a library user does not have to drop unused objects explicitly, instead this is done implicitly when the object goes out of scope.
<!--TODO: completely playable but no parameter change implemented for MIDI controller -->

At the time of writing, there was no Rust OSC library that fully supported the OSC 1.0 specification and was also compatible with Rust 1.0 and later versions, thus a new library had to be implemented, named \citetitle{github:rosc}.
The library achieves full compatibility with the OSC 1.0 specification and features encoding and decoding of OSC messages while being transport and platform indepent.
OSC messages are decoded from their byte array representation, e.g. received as payload of an UDP packet, by pattern matching \cite[`src/decoder.rs`, lines 129-161]{github:rosc} against the corresponding variant of the algebraic Rust data type shown in \cref{lst:osc-type}.

\begin{listing}
\begin{minted}{rust}
pub enum OscType {
    Int(i32),
    Float(f32),
    String(String),
    Blob(Vec<u8>),
    Time(u32, u32),
    Long(i64),
    Double(f64),
    Char(char),
    Color(OscColor),
    Midi(OscMidiMessage),
    Bool(bool),
    Nil,
    Inf,
}
\end{minted}
\caption{Definition of OSC data types in \cite[`src/types.rs`]{github:rosc}.}
\label{lst:osc-type}
\end{listing}

### Lemur
\label{subsec:lemur}

A multitouch capable graphical user interface (GUI) for the synthesizer is provided by the implemenation of a custom controller patch for liine's Lemur application \cite{LiineLemur}.
Lemur resembles the control surface of Jazzmutant's Lemur, a multi touch capable OSC control surface hardware device, but runs on Android or iOS devices instead.
Control information is send as OSC Bundles which contain one or more OSC Messages containing the actual control signal.
The custom control interface shown in \cref{fig:lemur-piano} to \cref{fig:lemur-filter} was implemented using Lemur's control surface editor and makes use of the integrated scripting language.

\Cref{fig:lemur-piano} shows the piano grid of the control surface with 8 key rows of 12 keys each.
A row represents a single octave of a musical keyboard with black and white keys where the lowest key is colored blue.
The root key of the grid can be shifted in a range from -3 to +3 octaves with the slider shown on the right.
In contrast to classical MIDI keyboards it is not possible to play notes with different velocities because, Android and iOS devices lack pressure sensitivity sensors.
Multi touch support allows to play the synthesizer polyphonically by pressing multiple keys at once.

![View of the piano panel\label{fig:lemur-piano}](imgs/lemur-piano.png){ width=100% }

There are four identical oscillator control sections with separate controls for each oscillator's envelope, phase, detune and transpose parameters and a list of waveforms to choose from.
Each oscillator can be transposed in a \textpm{} 3 octave range and detuned in a by \textpm{} one seminote at a resolution of 1 cent \cite[`src/dsp/wavetable.rs`, lines 357-372]{github:ytterbium} which is a 100th of an equal tempered semitone.
Also, the phase offset can be controlled in a range of \SI{0}{\degree} to \SI{180}{\degree}.

![View of the oscillator control panel showing the first of four control panels\label{fig:lemur-oscillator}](imgs/lemur-oscillator.png){ width=100% }

\Cref{fig:lemur-fm} shows the FM control section where each oscillator section contains a group of sliders to control the amount of frequency modulation applied to its fundamental frequency, including feedback modulation, i.e. an oscillator is modulating its own frequency.
In the screenshot the frequency of the first oscillator is modulated by the second which is itself modulated by the third oscillator.

![View of the FM control panel\label{fig:lemur-fm}](imgs/lemur-fm.png){ width=100% }

The oscillator mixing panel shown in \cref{fig:lemur-mixer} contains a slider for amplitude control and a bipolar (zero at center position) slider to set the stereo panning for each oscillator.

![View of the mixer panel\label{fig:lemur-mixer}](imgs/lemur-mix.png){ width=100% }

Parameters of the multi-mode filter can be set in the filter control panel.
It contains a drop-down list for selecting the filter mode and a two dimensional control field to set the filter's cutoff frequency on its x-axis and the resonance control on its y-axis.

![View of the mixer panel\label{fig:lemur-filter}](imgs/lemur-filter.png){ width=100% }

## Wavetable Oscillator
\label{sec:wavetable-oscillator-implementation}

The wavetable oscillator's implementation follows closely the description that was given in \cref{subsec:wavetables}, i.e. there are separate wavetables for each octave of the desired frequency range, i.e. ongoing from a lowest frequency $f_L$ there is a new wavetable that covers frequencies in

$$\left[n\,f_L, (n+1)\,f_L\right],\quad{}n \in\mathbb N \wedge n\,f_L < \lfloor f_{s}/2 \rfloor - 1$$

where $n$ is the octave.
A tables length and the number of harmonics contained in its waveform gets halved each time $n$ increases until the waveform consists solely of its fundamental in the highest wavetable.

The waveforms are generated by means of Fourier synthesis, i.e. an inverse Fourier transform (IFT) is applied to the spectrum that was predefined for the desired waveform.
\Cref{lst:sawtooth-spectrum} shows how the spectrum of a inverse sawtooth wave is defined where line 8 asserts that the spectrum is mirrored in order to obtain a real valued signal from the IFT.
By using a length $l=2^n,\;n\in\mathbb{N}$ for the initial spectrum table assures that each following table also has a length which is a power of two because halving the table's length corresponds to decrementing $n$ by 1.
Those table lenghts are optimal for the inverse mixed-radix fast Fourier transform (FFT) \cite{github:rustfft} used in the implementation because it can make use of all symmetries in the FFT algorithm.
All wavetables are serialized to disk so they can be read from a file \cite[`src/dsp/wavetable.rs`, lines 89-100]{github:ytterbium} in subsequent application starts to reduce its initialization time.

\begin{listing}
\begin{minted}[linenos]{rust}
Waveform::Saw => {
    for i in 1..harmonics {
        let magnitude = (i as Float).recip();
        spectrum[i] = Complex {
            re: 1.0,
            im: -1.0 * magnitude,
        };
        spectrum[table_size - i] = -spectrum[i];
    }
}
\end{minted}
\label{lst:sawtooth-spectrum}
\caption{Calculation of Fourier coefficients for sawtooth waveform definition in the frequency domain \cite[`src/dsp/waveform.rs`, lines 163-172]{github:ytterbium}.}
\end{listing}

Both spectrograms shown in \cref{fig:wavetable-sweep} are calculated from frequency sweeps to which a Hann window was applied and that were generated from the wavetable oscillator as part of the unit test set.
It can be seen that the aliasing amount does not exceed $\SI{-80}{\decibel}$ which makes it inaudible.
Additionally, the frequency sweep visualizes the use of separate tables and their decreasing number of harmonic content.
The loss of spectral energy at each table switch can be perceived in the frequency sweep but it is not of great concern because the oscillators frequency is rather constant when the synthesizer is played.
However, this should be considered for effects with pulsating pitch changes like a vibrato.

\begin{sidewaysfigure}
\includegraphics[width=\textheight]{imgs/ytterbium-spectrum-legend.png}
\includegraphics[width=\textheight]{imgs/ytterbium-0.1.0-Saw-sweep.png}
\includegraphics[width=\textheight]{imgs/ytterbium-0.1.0-Square-sweep.png}
\caption{Spectrogram of a frequency sweep from 20Hz to 20kHz in the sawtooth (upper) and square (lower) wavetable. The x-axis denotes time in seconds whereas the y-axis represents frequencies in a range from 0 to $\SI{24}{\kilo\hertz}$.}
\label{fig:wavetable-sweep}
\end{sidewaysfigure}

A spectrogram of a frequency sweep for a sinusoidal carrier and modulator with increasing modulation amount is shown in \cref{fig:sweep-fm}.
The increasing amount of harmonic content is clearly visible as well as the distortion that starts approximately in the last quarter of the spectrogram and which is almost unavoidable because it is hard to give a constraint for the modulation amount so that no distortion will occur.

\begin{sidewaysfigure}
\includegraphics[width=\textheight]{imgs/ytterbium-spectrum-legend.png}
\includegraphics[width=\textheight]{imgs/ytterbium-0.1.0-fm.png}
\label{fig:sweep-fm}
\caption{Spectrogram of a frequency sweep for sine modulator and carrier with increasing modulation amount.}
\end{sidewaysfigure}

## Filter
\label{sec:filter-implementation}

The multi-mode biquad filter \cite[`src/dsp/filter.rs`]{github:ytterbium} is a direct implementation of the filter design from \citeauthor{Bristow:Cookbook}'s EQ Cookbook \cite{Bristow:Cookbook}.
\citeauthor{Bristow:Cookbook}'s filter is derived from an analog prototype using bilinear transform (see \cref{sec:iir-filters} and \cref{subsec:bilinear-transform}) and is a stable and popular design which is also used in the biquad implementation of the WebAudio API \cite{github:webaudio-api} which got adopted by chromium \cite{webaudio:biquad:chromium} and firefox \cite{webaudio:biquad:firefox} browsers.

Calculation of coefficients for the biquad filter is shown in \cref{lst:filter-coeff} where `w` is the cutoff frequency expressed as angular frequency[^angular-frequency], `q` is the resonance parameter and `filter_type` is an `enum` variant that is matched against the supported filter types (line 11 in \cref{lst:filter-coeff}) to select the its zero coefficients corresponding to the desired mode.

\begin{listing}
\begin{minted}[linenos]{rust}
fn coeffs(w: Float, q: Float, filter_type: FilterType)
    -> ([Float; 2], [Float; 3]) {
    let (sinw, cosw) = (Float::sin(w), Float::cos(w));
    let (mut As, mut Bs) = ([0.; 2], [0.; 3]);
    let alpha = sinw / (2.0 * q);

    let a0 = 1. + alpha;
    As[0] = -2. * cosw;
    As[1] = 1. - alpha;

    match filter_type {
        FilterType::LP => {
            Bs[0] = (1. - cosw) / 2.;
            Bs[1] = 1. - cosw;
            Bs[2] = (1. - cosw) / 2.;
        }
        FilterType::HP => {
            Bs[0] = (1. + cosw) / 2.;
            Bs[1] = -1. - cosw;
            Bs[2] = (1. + cosw) / 2.;
        }
        FilterType::BP => {
            Bs[0] = alpha;
            Bs[1] = 0.;
            Bs[2] = -alpha;
        }
        FilterType::Notch => {
            Bs[0] = 1.;
            Bs[1] = -2. * cosw;
            Bs[2] = 1.;
        }
    }
    // normalize by dividing through a0
    for x in Bs.iter_mut().chain(As.iter_mut()) {
        *x /= a0;
    }
    (As, Bs)
}
\end{minted}
\caption{Calculation of the biquad filter's coefficients \cite[`src/dsp/filter.rs`, lines 47-78]{github:ytterbium}.}
\label{lst:filter-coeff}
\end{listing}

A spectrogram of a white noise filtered by the biquad implementation in lowpass and bandpass mode with increasing filter cutoff and constant neutral filter resonance is shown in \cref{fig:filter-sweep}.
The parameter sweep of the spectrograms is generated by the filter's unit test to check the stability for time-varying parameters by sweeping the cutoff parameter at audio rate with the result that there are no sound artifacts visible and perceivable in the rendered audio, hence the implementation is suitable as a time-varying filter.

\begin{sidewaysfigure}
\includegraphics[width=\textheight]{imgs/ytterbium-spectrum-legend.png}
\includegraphics[width=\textheight]{imgs/ytterbium-0.1.0-LP-filter.png}
\includegraphics[width=\textheight]{imgs/ytterbium-0.1.0-BP-filter.png}
\caption{Spectrogram of a white noise filtered by a lowpass (upper) and bandpass (lower) biquad with increasing filter cutoff and constant neutral filter resonance.}
\label{fig:filter-sweep}
\end{sidewaysfigure}

## Ring Buffer
\label{sec:ring-buffer}

The ring buffer is used as a synchronization element between the signal generated from the DSP section (the synthesizer's oscillators and filter) and the audio backend.
A ring buffer is a fixed size FIFO queue with a write and read index.
In contrast to a normal queue the index wraps around when it reaches the end of the queue as if both ends of the queue were connected.
Implementing such a buffer in Rust proved to be more complicated than in traditional system languages like C because Rust's compiler assures exclusive write or read access to any data structure at any time \cite[4.9 References and Borrowing]{RustBook} in order to achieve its memory safety guarantees.
Mutual exclusive access to the buffer by either a consumer (reader) or producer (writer) view was ensured by wrapping the buffers underlying array in a mutex \cite[`src/lib.rs`, line 148]{github:rb}.

The ring buffer's interface provides blocking and non-blocking read and write access and, in contrast to common ring buffer implementations, protects for over- and underflow by assuring that none of the read or write indexes is overtaking the other.
A buffer underflow means that the read pointer overtakes the write pointer, e.g. if the producer is to slow, and an overflow is the exact opposite, i.e. the reader cannot consume the data fast enough.
The under- and overflow protection is achieved by keeping track of the number free and filled slots (line 4 and 5 in \cref{lst:ring-buffer}).
The synthesizer implementation uses the blocking interface which allows the DSP thread to sleep if the whole buffer is filled.
The implementation of the blocking interface does not use a naive busy-wait loop, instead, it uses conditional variables (`Condvar` in \cref{lst:ring-buffer}) to signal either a consumer or producer view when there are slots free or data is available to be read.

\begin{listing}[H]
\begin{minted}[linenos]{rust}
pub struct SpscRb<T> {
    buf: Arc<Mutex<Vec<T>>>,
    inspector: Arc<Inspector>,
    slots_free: Arc<Condvar>,
    data_available: Arc<Condvar>,
}
\end{minted}
\label{lst:ring-buffer}
\caption{\cite[`src/lib.rs`, lines 147-152]{github:rb}}
\end{listing}

It showed, by benchmarking the throughput of the ring buffer implementation, that the use of synchronization primitives did not introduce a significant amount of runtime overhead.
In the benchmark $2~880~000$ samples were pushed through the buffer \cite[`benches/bench.rs`]{github:rb}, which equates to one minute of audio data at $\SI{48}{\kilo\hertz}$ sampling rate, in $\SI{13915402}{\nano\second} \approx \SI{14}{\milli\second}$[^benchmark-machine] which is by several orders of magnitude faster than the required throughput-rate of the synthesizer that is equal to its sample rate.
Hence, the ring buffer implementation is suitable for use in real-time audio applications.

<!--NOTE
throughput-rate the total rate at which digital information is processed by a discrete-time system, measured in samples per second \cite{Rabiner1972}
-->

## Audio Output
\label{sec:audio-output-implementation}

There is a variety of different audio backend APIs, e.g. CoreAudio for Apple's macOS, WASAPI is one of the audio APIs for Microsoft Windows and Linux has JACK, ALSA, PulseAudio and others.
Implementing a separate binding for each of those C APIs is error prone and makes the portability of the code more difficult.
Therefore, it is convenient to use a wrapper that provides a cross-platform abstraction for those audio backend APIs.

A popular choise which provides Windows, Mac and Linux support is \citetitle{portaudio} \cite{portaudio} and, with rust-portaudio, a Rust binding for this library was also available at the time of writing.
Nonetheless, with \citetitle{github:rsoundio} \cite{github:rsoundio}, a Rust binding for \citetitle{github:libsoundio} \cite{github:libsoundio}, another cross-platform library, was implemented.
The decision for using \citetitle{github:libsoundio} was made because it provides support for PulseAudio, has better error handling than \citetitle{portaudio} (a more thorough comparison is provided by \cite{libsoundio-vs-portaudio}).

Sound data is send to the sound card by a callback function which had to be registered in libsoundio's API.
Libsoundio's API required to register a callback function before a sound stream could be opened, i.e. the callback was called each time the sound card requested more data.
The same rules that apply for signal handlers in systems software apply to the callback function, the code should avoid memory allocations, complex operations and every form of blocking IO to prevent underruns of the sound card buffer.
In general, the callback function should only copy data from the synthesizer's internal ring buffer into the sound card's buffer while casting each samples into another float or integer type if necessary.

Registering a callback funtion in \citetitle{github:rsoundio} translates to storing a pointer to a Rust function into a C struct of \citetitle{github:libsoundio} \cite[`src/stream.rs`, lines 173-185]{github:rsoundio}.
Unfortunately, the execution of the callback function left the scope of the output stream struct \cite[`src/stream.rs`]{github:rsoundio} that stored a pointer to the callback and also caused the structs memory to be freed automatically.
This in turn caused a segmentation fault when the callback function returned.
Those types of segmentation faults at runtime are hard to debug and preventing the output stream struct to be dropped at all will likely cause a memory leak.
A simple solution was to set a boolean marker that prevented the struct from being freed in a callback context \cite[commit \#1fcdde5]{github:rsoundio} but otherwise allowed the struct to be dropped.

Implementing a safe Rust wrapper for \citetitle{github:libsoundio} has shown to be more complicated than expected but the final implementation could be used successfully as an audio backend for the synthesizer.

# Summary
\label{ch:summary}

This work presented an overview of synthesis techniques and algorithms suitable for the implementation of a polyphonic real-time audio synthesizer.
The evaluation of the synthesizer prototype showed that the chosen techniques achieved the required amount of audio quality and are efficient enough so that the instrument can be played with very low latency through MIDI and OSC controller hard- or software.
Also, Rust proved to be an excellent choice for the implementation of real-time audio software, even though the language's ecosystem still lacks mature signal processing libraries.

## Optimizations
\label{sec:optimizations}

It should be possible to compile the source code on platforms other than Linux, but those were not tested, hence a cross platform build setup could be developed.
Moreover, commercial synthesizers usually provide a vast amount of modulation options, e.g. nearly every parameter of the instrument can be controlled from low frequency oscillators, envelope generators or integrated step sequencers, the possibilities are nearly endless.
Adding an envelope generator to control the filter's cutoff frequency would greatly expand the range of sounds that can be created with the synthesizer.
A combination of different oscillator algorithms for specific waveforms could improve the performance of the application, especially the sine wave could be generated without the use of wavetables.
Also, of great value would be the addition of a variety of sound effects to the synthesizer's audio chain, e.g. a reverb effect, a delay or one of its special forms like phaser and flanger, bit reduction or a distortion effect.
The set of possible extensions and improvements is as large as the variety of sonic themes that can be produced, so the given ideas can be seen as a starting point.

## Conclusion
\label{sec:conclusion}

I underestimated the amount of work that comes with such a project, especially for one with no previous experience in the development of signal processing software.

The closing sentence is a quote from my presentation of the master's thesis:

> Implementing an audio synthesizer is serious work, if done right.

<!-- FOOTNOTES -->

[^foldover-ref]: \citeauthor{Dashow1978} presented a method for generating non-harmonic spectra using foldover frequencies \cite[p.~82]{Dashow1978}.

[^midi-audio]:
	It is possible to transmit audio data over MIDI by using *System Exclusive* (SysEx) messages, but this can not be done in real-time and is often used to replace or update samples or wavetables in hardware synthesizers.

[^sysex]:
	System Exclusive (SysEx) messages can be made up of more than two data-bytes, in fact they are build by a sequence of data bytes followed by an *End of Exclusive* (EOX) message to mark the end of the stream. This type of message does not contain any musical control data, in general it is used to upload binary data, like firmware updates or samples, to a MIDI device.

[^comb-filter]:
	A comb filter adds a delayed copy of the signal to itself causing addition or subtraction in the signal. The filters frequency response shows regularly spaced notches, might resemble the shape of a comb.

[^osc-naming]:
	The term *OSC Receiver* and *OSC Server* is interchangeable.
	This also applies to *OSC Sender* and *OSC Client*.
	OSC applications often act as server and receiver, hence no clear distinction between those roles can be made.

[^euler-ids]: Eulers identities: $\cos = (e^{i\phi} + e^{-i\phi})/2$ and $\sin = (e^{i\phi} + e^{-i\phi})/(2i)$.

[^phase-inversion]: $-\sin(\phi)=\sin(-\phi)$

[^overtones]: Overtones are an integer multiple of waves the fundamental frequency.

[^signal-sequence]: The term sequence and signal will be used interchangeably.

[^magnitude-response]: Sometimes improperly called *amplitude* response because amplitudes can be negative.

[^see-aliasing]: see aliasing \cref{subsec:aliasing}

[^zero-crossing]: The $\sinc$ function has zeros at $n*\pi, n \in \mathbb{Z}$

[^oscillator-ref]: The interested reader can refer to \cite{Moorer1976} for description of DSF and \cite{Nam2009} for FDL.

[^stdTemp]: The *normal temperature* is defined by the National Institute of Standards and Technology (NIST) as $20°C$ at 1\thinspace{}atm absolute pressure.

[^benchmark-machine]: The same machine was used for the benchmark as for the latency test in \cref{subsec:system-latency}.

[^angular-frequency]: As usual, $w = (2\pi f_c)/f_s$ where $f_c$ is the cutoff frequency and $f_s$ is the sample rate in Hertz.