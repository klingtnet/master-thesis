# Introduction

## Scope of this Thesis

It's not required for the reader to have any prior knowledge of soft- and hardware tools used in music production environments.
However, a basic knowledge of signal processing and some familiarity with computer programming is beneficial to grasp the presented concepts.

## Objectives

<!-- TODO
Be more specific
-->

The objective of this thesis is to show how to build a real time audio software application at the example of an audio synthesizer with frequency modulated oscillators.

The amount of problems that arise when building such an application is quite large.
This includes timing and synchronization problems that occur because the computations made are required to be finished in short time windows.
Computations must be as fast as possible to ensure a responsive feedback for the player of the instrument, and are needed to avoid sound distortion when the generated signals could not be delivered fast enough to the sound card buffer.

Not every programming language is equally suitable for the development of real time applications, thus it is explained why Rust was chosen as the implementation language.
Libraries for the MIDI and Open Sound Control protocol were implemented and released as open-source software.
Both protocols are supported by the application and are used to play polyphonic\footnote{More than one at a time.} notes or remote control all parameters of the synthesizer.
Common synthesizer architectures and signal generation methods are discussed and evaluated.

Furthermore, sound shaping techniques and effects are presented in detail.
The precautions taken to reduce the amount of signal distortion are shown as well.
Lastly, an outlook on possible optimizations and enhancements, as well as a summary of the achieved results is given.

## Overview

<!-- TODO: provide a quick summary of each chapter. -->

Chapter 2 gives an overview about common synthesis concepts and evaluates why Rust was chosen as the implementation language.

## Why Rust?

Real time audio applications must finish their signal processing in tight time bounds to ensure that the sound card can output a continuous audio stream.
Missing such a bound will result in unpleasant sound glitches, and---in the worst case---renders a whole song recording useless.
Additionally, the time bounds must be as tight as possible to reduce the latency between the user input (playing a note) and the output of the calculated signal in the speaker.
Therefore, (memory-)managed languages like [Java](https://en.wikipedia.org/wiki/Java_(programming_language)), [Go](https://golang.org) or [C#](https://en.wikipedia.org/wiki/C_Sharp_(programming_language)) that use a garbage collector, which can produce non deterministic program stops while examining the state of variable references, will not receive any further consideration.
Interpreted languages like [Python](https://www.python.org/) or [Ruby](https://www.ruby-lang.org/en/) face the same problems as compiled managed languages by also having increased runtime costs.
Audio application development in those languages is still possible but commonly requires to write the signal processing as C modules and interface with them through the languages foreign function interface (FFI) (e.g. pyo \cite{pyo} uses this approach). 
Unmanaged languages like [C](https://en.wikipedia.org/wiki/C_(programming_language)) and [C++](https://en.wikipedia.org/wiki/C%2B%2B) offer the control over memory that is needed to make reliable claims about the runtime behavior while avoiding the overhead of an additional runtime environment.
This comes with a downside in memory safety and introduces a whole new class of possible bugs compared to managed languages.
Those bugs are very likely to cause *undefined behavior* or to crash the program. Unfortunately, they are also very hard to debug.

One of the major selling points of [Rust](https://rust-lang.org) is guaranteed memory safety *without* garbage collection and *data race freedom* while providing the same level of control over memory as C/C++.
These goals are achieved through a variety of concepts like ownership, lifetimes and borrowing to know at compile when memory can be safely freed, and to enforce that there is only a single mutable access to any variable at any given time in the run of the program.
Explaining these concepts is outside the scope of this thesis and the official [Rust Book](https://doc.rust-lang.org/book/ownership.html) \cite{RustBook} does a great job doing this in detail, hence, this is left as an exercise for the interested reader.
The [Max Planck Institute for Software Systems](http://plv.mpi-sws.org/) has started the \cite{RustBelt} RustBelt research project in 2015 to develop formal foundations for the Rust programming language.
One of the main goals of the research group is to formally investigate if the claims made about data-race freedom, memory and type safety actually hold.
<!-- Furthermore, they want to provide formal tools for verifying safe encapsulation of `unsafe` code blocks. -->
As of the time of writing there is no evidence that the claims are untrue, therefore it's assumed that they're actually hold.

In the following I will describe a number of language features that were crucial in making the decision to use Rust for the implementation of the synthesizer:  

Generics (parametric polymorphism)
:	Generics allow to write a single implementation of a function or method so that it can be used with a number of different types.
	A *generic* type in the declaration is replaced by an *type parameter*.
	Such a parameter can also be constrained so it can only be replaced with types that implement certain traits.

	[Example](https://is.gd/Ob3eqr):
	```rust
	fn sum<T>(a: T, b: T) -> T::Output {
		a + b
	}

	println!(sum(3+4));
	```

Cross compilation
:	Cross compilation is the ability to compile statically linked binaries for different target platforms on a single machine, e.g. one can compile a windows executable on a Linux machine.

Abstractions without overhead
:	The Rust compiler will create a separate implementation for each type that a generic function is used with in the code.
	This is called *monomorphization* and allows to statically dispatch calls to generic functions but comes with the cost of slightly increased binary sizes.
	Iterators and other high-level features will be compiled down to simpler constructs like *for* loops.

FFI
:	A foreign function interface allows the calling of functions written in other languages.
	Rust's FFI allows to easily call functions from a variety of languages but, as a systems language, it focuses on C as the default target.
	This is of great value because most audio APIs and I/O libraries, e.g. [portaudio](http://www.portaudio.com/) or [libsoundio](http://libsound.io/), are C libraries.
	The opposite directions is possible as well, because a Rust function can be defined to use the C ABI (application binary interface) so it can be called from C or other languages that provide a C FFI.

Algebraic data types and pattern matching
:	An algebraic data type is formed by combination of other types.
	The term *algebraic* refers to the operations that are used to form the type, which is either *sum*, i.e., the type is $A$ or $B$, or *product*, i.e., it is a combination of $A$ and $B$, where $A$ and $B$ are different types.
	Rust's `enum`s are formed as *sum* types whereas `tuple`s are formed by a *product* of types.
	A classic example for the former kind is the `Option` type, which can either be `None` or have `Some` value of type $T$:

	```rust
	enum Option<T> {
	    None,
		Some(T),
	}
	```

	Destructuring the values of such types is done through *pattern matching* in Rust.
	In contrast to C-like enums, the compiler knows how much variants such an algebraic data type has and won't compile if they aren't matched exhaustively, i.e. there is at least one unmatched variant of the type.

Iterators
:	Rust provides conventional `for` loops but additionally has *iterators*.
	An iterator is something that provides a sequence of things where you can loop over.
	The great benefit of using iterators over index based access in traditional `for` loops is, that it avoids the, often unintended, access of elements that are outside the iterators range.
	In addition, iterators provide a vast amount of methods to combine them, filter elements, perform map-reduce like operations and in many cases even parallelize the computation \cite{Rayon}.

	Example: $\sum_{x\,\in\,v_1, v_2\;\wedge\;x > 3}$

	```rust
	let v1 = [1, 2, 3];
	let v2 = [3, 4, 5];
	let sum = v1.iter().chain(v2.iter())
				.filter(|val| **val > 3)
				.fold(|acc, val| acc + val, 0);
	assert_eq!(sum, 9)
	```

Toolchain
:	Rust has a standard package manager and build tool called *Cargo*, that is typically used to manage the dependencies of a project but also to run tests and benchmarks.

# User Interface

Interaction with the software synthesizer is done through its *user interface*.
The user interface serves three senses, which are sight, touch and hearing.
This section concentrates on the first two, the *visual* and the *haptic* component.
However, a synthesizer can be played solely through haptic controls and audio feedback.
Visual indicators for the synthesizer's parameters, e.g. through a display, are convenient, but not necessary for the playing musician.

The application supports the two most common musical control signal protocols, *MIDI* and *Open Sound Control* (OSC).
Adding MIDI support is highly beneficial, because it enables the synthesizer to be played with any---of the vast amount of available---MIDI hardware controllers (\cref{midi:edirol} shows an example of such a device).
On the other hand, Open Sound Control software like [liine's Lemur](https://liine.net/de/products/lemur/) \cite{LiineLemur} provides an editor to create or customize a software defined controller for a multi-touch device like a smartphone or tablet.

![Edirol PCR-300 MIDI controller keyboard\label{midi:edirol}](imgs/pcr_300_angle.jpg){ width=100% }

<!-- TODO
- OSC: custom controllers, more modern approach
- User interface must work in real time (latency)

MIDI support is a basic requirement for nearly all types of music hard- and software because it allows to use controller hardware
User input, if it is a played note or a parameter change, must be processed in real-time to give the player direct feedback without perceived *latency* (see \nameref{latency}).
-->

## MIDI

The Musical Instrument Digital Interface (MIDI) specification stipulates a hardware interconnection scheme and a method for data communication \cite[p.~972]{Roads:CMT}, but only the protocol specification is of interest for this work.
Most modern MIDI hardware is connected via USB anyway.
The *MIDI 1.0 Specification* \cite{MIDI10} provides a high level description of the MIDI protocol:

> The Musical Instrument Digital Interface (MIDI) protocol provides a standardized and efficient means of conveying musical performance information as electronic data.
> MIDI information is transmitted in \enquote{MIDI messages}, which can be thought of as instructions which tell a music synthesizer how to play a piece of music. \cite[p.~1]{MIDI10}

Transmitting *control data* is the purpose of the MIDI protocol, and not, like it is sometimes confused, to transmit audio data[^midi-audio].
Control data can be thought of as the press of a key, turning a knob, or an instruction to change the clock speed of a song.

[^midi-audio]: It is possible to transmit audio data over MIDI by using *System Exclusive* (SysEx) messages, but this can not be done in real-time and is often used to replace or update samples or wavetables in hardware synthesizers.

The work on the MIDI specification began in 1981 by a consortium of Japanese and American synthesizer manufacturers, the MIDI Manufacturers Association (MMA).
In August 1983, the version 1.0 was published \cite[p.~974]{Roads:CMT}.
This year, 2016, the MMA established The MIDI Association (TMA).
The TMA should support the global community of MIDI users and establish [midi.org](https://www.midi.org/) \cite{MidiOrg} as a central source for information about MIDI.
MIDI is used in nearly every music electronic device, like synthesizers, samplers, digital audio effects, and music software, due to its simple protocol structure and long time of existence.

### MIDI Protocol

The MIDI protocol specifies a standard transmission rate of 31.250 baud.
This may seem like an unusual choice for the transmission rate, but it was derived by dividing the common clock frequency of 1MHz by 32 \cite[p.~976]{Roads:CMT}.
It uses an 8b/10b encoding, i.e. 8 bits of data are transmitted as a 10 bit word.
A data byte is enclosed by a start- and stop bit which in turn results in the 10 bit encoding.
Asynchronous serial communication is used to transfer *MIDI messages*, thus the start and stop bit.

A MIDI message is composed of a *status byte* which is followed by up to two[^sysex] *data byte*s.
Both types are differentiated by their most significant bit (MSB), `1` for status- and `0` for data bytes.
Consequently, the usable payload size is reduced to 7 bit, in other words, values can range from 0 to 127.

[^sysex]: System Exclusive (SysEx) messages can be made up of more than two data-bytes, in fact they are build by a sequence of data bytes followed by an *End of Exclusive* (EOX) message to mark the end of the stream. This type of message does not contain any musical control data, in general it is used to upload binary data, like firmware updates or samples, to a MIDI device.

\Cref{fig:midi-status} shows the structure of a status byte.
The message type is denoted by three bits (`T`) and the remaining four bits are used to denote the channel number (`C`), hence there are sixteen different channels.
MIDI channels allow to route different logical streams over one physical MIDI connection, e.g. to reach a different, daisy-chained MIDI device or to control different timbres of a multitimbral synthesizer.

\begin{figure}
    \centering
	$0\quad\underbrace{T\quad{}T\quad{}T}_{\text{message type}}\quad\overbrace{C\quad{}C\quad{}C\quad{}C}^{\text{channel number}}$
	\caption{Structure of a MIDI status byte.}
	\label{fig:midi-status}
\end{figure}

MIDI messages are divided in two categories, *channel* and *system* messages.
Only the latter contain musical control information and therefore are of interest for this thesis.
\Cref{fig:midi-classification} illustrates the classification, status byte values are shown as edge labels where \texttt{x} illustrates *don't care*.
*Channel Mode Messages* define the instrument's response to Voice Messages \cite[p.~36]{MIDI10}, i.e. listen on all channels (omni mode), or switch between mono- and polyphonic mode (multiple simultaneous voices).

\begin{figure}
	\centering
	\begin{tikzpicture}
[
	grow = right,
	every node/.style = {font=\footnotesize},
	sibling distance = 1cm,
	level distance = 1cm,
	level 1/.style = {sibling distance = 5cm, level distance = 3cm},
	level 2/.style = {sibling distance = 2cm, level distance = 5cm},
	sloped
]
\node[msg]{MIDI Message}
	child { node [msg]{System Message}
		child { node [msg]{System Exclusive\\Message}
			edge from parent node [above] {\texttt{F0}}
		}
		child { node [msg]{System Common\\Message}
			edge from parent node [above] {\texttt{F1-F7}}
		}
		child { node [msg]{System Real-Time\\Message}
			edge from parent node [above] {\texttt{F8-FF}}
		}
		edge from parent node [above] {\texttt{F0-FF}}
	}
	child { node [msg]{Channel Message}
		child { node [msg]{Channel Voice\\Message}
			edge from parent node [above] {\texttt{8x-Ex}}
		}
		child { node [msg]{Channel Mode\\Message}
			edge from parent node [above] {\texttt{Bx}}
			edge from parent node [below] {\texttt{Data1: 79-7F}}
		}
		edge from parent node [above] {\texttt{8x-Ex}}
	}
	;
	\end{tikzpicture}
	\caption{Classification of MIDI messages.}
	\label{fig:midi-classification}
\end{figure}

### MIDI Pitch

\begin{table}[]
    \centering
    \caption{Types of MIDI Voice Messages.}
    \label{tab:voice-messages}
    \begin{tabular}{@{}llllp{3.5cm}@{}}
    \toprule
    Type                    & Status & Data1      & Data2    & Description                                                \\ \midrule
    Note-Off                & 8x     & Key \#     & Velocity & Key released.                                              \\
    Note-On                 & 9x     & Key \#     & Velocity & Key press from a triggering device.                        \\
    Polyphonic Key Pressure & Ax     & Key \#     & Pressure & Aftertouch event.                                          \\
    Control Change          & Bx     & Ctrl. \#	  & Value    & Move of a controller other than a key (e.g. Knob, Slider). \\
    Program Change          & Cx     & Program \# & ---      & Instruction to load specified preset.                      \\
    Channel Pressure        & Dx     & Pressure   & ---      & Aftertouch event.                                          \\
    Pitch Bend              & Ex     & MSB        & LSB      & Altering pitch (14-bit resolution).                        \\ \bottomrule
    \end{tabular}
\end{table}

\Cref{tab:voice-messages} gives an overview of the types on voice messages.
Corresponding Note-On and Off messages do not necessarily follow one after another, therefore, to relate associated messages, pitch information is contained in the Note-Off as well.
Pitch is encoded as a 7-bit value in note messages, hence there is a range of 128 pitches or about 10 octaves.
MIDI's pitch representation was designed with an *chromatic western music scale* in mind.
A *chromatic scale* has 12 pitches per octave with one semitone difference between each pitch, that is a ratio of $2^{1/12}$ between successive notes.
An interval of one octave is equivalent to a doubling or halving (in the negative case) in frequency.
Instruments in *western music* are usually *equal-tempered*, i.e. all semitones have the same size.
MIDI pitches are considered to be equal-tempered and range from C0 (*c* in the lowest octave) to a G10 (*g* in the 10th octave).
Middle C, pitch number 60 (C5), is used as reference.

\begin{align}
\begin{split} % creates only a single equation number
	f &= f_\text{tune}\cdot 2^{\displaystyle\left(p-p_\text{ref}\right)/12}\\
    p &= p_\text{ref}+12\cdot\log_2(f/f_\text{tune})
\end{split}
\label{eq:midi-pitch}
\end{align}

\Cref{eq:midi-pitch} shows how to calculate the frequency $f$ for a given MIDI pitch $p$, and vice versa, where $f_\text{tune}$ is the tuning frequency and $p_\text{ref}$ is the reference pitch number.
Musical instruments are commonly tuned to the *Concert A*, the note A above middle C or MIDI pitch 69.
The default tuning of Concert A is 440 Hz \cite{ISO16:1975}.
The following example shows how to calculate the frequency for middle C by using the *Concert A* tuned to 440 Hz as reference pitch in \cref{eq:midi-pitch}:

$$
\begin{aligned}
f	&= 440\,\text{Hz}\cdot 2^{(60-69)/12}\\
	&= 440\,\text{Hz}\cdot 2^{-9/12}\\
	&\approx 261.626\,\text{Hz}
\end{aligned}
$$

### Timing Problems

Playing two or more notes a the same time, i.e. playing a chord, can lead to timing problems because of MIDI's low bandwidth.

$$
\begin{aligned}
t_\text{Note-On}	&= 3\cdot\left(31250\,\frac{\text{bit}}{\text{s}}/10\,{\text{bit}}\right)^{-1}\\
					&= 0.0096\,\text{s} = 0.96\,\text{ms}
\end{aligned}
$$

The time to transmit a single note-on event $t_\text{Note-On}$ takes $\approx 1\,\text{ms}$, this means that the last transmitted note of an $n$-key chord arrives with $n \cdot 0.96\,\text{ms}$ delay, e.g. the last note of a pentachord (5 keys) will be received $5 \cdot 0.96\,\text{ms} = 4.8\,\text{ms}$ later than the first one.
This may result in a *comb filter*[^comb-filter] like distortion of the synthesized chord sound.

[^comb-filter]: A comb filter adds a delayed copy of the signal to itself causing addition or subtraction in the signal. The filters frequency response shows regularly spaced notches, might resemble the shape of a comb.

## Open Sound Control

The *UC Berkeley Center for New Music and Audio Technology* (CNMAT) originally developed, and continues to research, Open Sound Control.
In 2002, OSC's 1.0 specification was released.
It provides the following definition \cite{OSC:10}:

> Open Sound Control (OSC) is an open, transport-independent, message-based protocol developed for communication among computers, sound synthesizers, and other multimedia devices.

The protocol is not limited to being used with audio or multimedia devices, however, it is often used as a high-speed network replacement for MIDI.
Referring to OSC as a *message format* is more accurate, since error-handling, synchronization or negotiation methods are not specified.
Therefore, OSC can be compared to formats like JSON or XML.
A draft of the OSC 1.1 specification was published in a 2009 paper \cite{OSC:11} only adding minor, backward compatible changes.
UDP is often used as the transport layer to avoid the time required to establish a connection by TCP's three-way handshake.
A connection less transport is sufficient because OSC sender and receiver are almost always in physical proximity and connected through the same LAN.

### OSC Data Types

\begin{table}[]
    \centering
    \caption{Overview of OSC 1.0 and 1.1 data types.}
    \label{tab:osc-data-types}
    \begin{tabular}{@{}cp{7.5cm}cc@{}}
    \toprule
    Tag     & Description                                                      & 1.0 Required & 1.1 Required \\ \midrule
    i       & 32-bit two's complement integer                                  & ✔            & ✔            \\
    f       & IEEE 754 single precision (32-bit)                               & ✔            & ✔            \\
    s       & null-terminated sequence of ASCII characters                     & ✔            & ✔            \\
    b       & binary blob with size information                                & ✔            & ✔            \\
    h       & 64-bit big-endian two's complement integer                       &              &              \\
    t       & OSC-timetag in NTP format                                        &              & ✔            \\
    d       & IEEE 754 double precision (64-bit)                               &              &              \\
    S       & alternate string type                                            &              &              \\
    c       & ASCII character                                                  &              &              \\
    r       & RGBA color (8-bit per channel)                                   &              &              \\
    m       & 4 byte MIDI message. From MSB to LSB: port, status, data1, data2 &              &              \\
    T/F     & true, false boolean values                                       &              & ✔            \\
    N       & Nil                                                              &              & ✔            \\
    I       & Infinitum (1.0)/Impulse(1.1) used as event trigger               &              & ✔            \\
    {[},{]} & Array delimiters                                                 &              &              \\ \bottomrule
    \end{tabular}
\end{table}

An overview of the predefined data types for both, OSC 1.0 and 1.1, is shown in \cref{tab:osc-data-types}.
The byte order of OSC's integer, float and timetags is big-endian.
OSC's unit of transmission is called *OSC Packet*.
The EBNF grammar for OSC packets is described by \cref{osc:grammar}.
Fields of an OSC packet have to be aligned to multiples of 4-byte and are zero-padded, thus the size of such a packet is also a multiple of four.
The packets contents can either be an *OSC Message* or *OSC Bundle*.
An OSC message starts with an *address pattern*  followed by zero or more *arguments* to be applied to the *OSC Method* matched by the pattern.
Address pattern can contain basic regular expression with single-/multi-character `?/*` wildcards, range `[A-Z]` and list matches `{foo, bar}`, hence multiple OSC Methods can be triggered with a single OSC Message.
An OSC Receiver's[^osc-naming] address space forms a tree structure with branch nodes called *OSC Containers* and leaves are named *OSC Methods*.
Methods are *`italicized`* in the tree structure example of \cref{fig:osc-address-space-example}.
The address of an OSC method starts with a `/`, followed by any container name along the path in order from the root of the tree, joined by forward slashes `/` and the method's name, e.g. `/oscillator/1/phase`.

[^osc-naming]:
	The term *OSC Receiver* and *OSC Server* is interchangeable.
	This also applies to *OSC Sender* and *OSC Client*.
	OSC applications often act as server and receiver, hence no clear distinction between those roles can be made.

\begin{figure}
	\centering
	\begin{tikzpicture}
[
	every node/.style = {font=\footnotesize\ttfamily},
	level 1/.style = {sibling distance = 5cm},
	level 2/.style = {sibling distance = 2cm},
	sloped
]
\node{/}
	child { node {oscillator/}
		child { node {1/}
			child { node [osc-method]{freq} }
			child { node [osc-method]{phase} }
		}
		child [sibling distance=20mm]{ node {2/}
			child { node {\ldots} }
		}
	}
	child { node {filter/}
		child { node [osc-method]{mode} }
		child { node [osc-method]{cutoff} }
		child { node [osc-method]{resonance} }
	}
	;
	\end{tikzpicture}
	\caption{OSC Address Space example.}
	\label{fig:osc-address-space-example}
\end{figure}

\begin{figure}
\caption{Grammar of an OSC packet described as EBNF (ISO14977 syntax \cite[p.~14]{ISO14977})}
\label{osc:grammar}
\begin{verbatim}
packet			= size, content ;
size			= (* 4-byte aligned packet content field length *) ;
content			= message | bundle ;
message			= address, ",", { type-tag }, { argument } ;
address			= "/", osc-string - ( "'" | "#" | "*" | "," | "/" |
									  "?" | "[" | "]" | "{" | "}" ) ;
osc-string		= { ASCII }, "0" ;
type-tag		= "i" | "f" | "s" | "b" | "h" | "t" | "d" | "S" |
				  "c" | "r" | "m" | "T" | "F" | "N" | "I" | 
				  "{", {type-tag}, "}" ;
argument		= (* binary representation of the argument *) ;
bundle			= "#bundle", OSC-timetag , { bundle-element } ;
bundle-element	= size, content ;
\end{verbatim}
\end{figure}

### Comparison to MIDI

Both protocols provide a number of benefits and limitations in comparison to each other.
The following list shows them for OSC compared to MIDI:

\begin{enumerate}
	\item[$+$] OSC's data-types allow a much higher resolution for control values.
	They also provide symbolic types like booleans or \emph{Nil} to represent an empty value.
	\item[$+$] The definition of \emph{custom data-types} is allowed, therefore OSC applications must be made robust against unknown ones.
	\item[$+$] The \emph{bandwidth} is orders of magnitudes larger than MIDI's, but it depends on the type of network used.
	A common choice are ad-hoc WiFi connections between OSC receiver and sender because the player (sender) and the instrument (receiver) are in local proximity to each other.
	This, in turn, results in an acceptable network \emph{latency} in the single digit millisecond range.
	\item[$+$] Control events can be send simultaneously as an OSC bundle, e.g. note events of a chord.
	\item[$+$] Events can be timed with an resolution of $\approx 200$ picoseconds \cite{OSC:10}.
	\item[$+$] OSC can be used to tunnel MIDI messages over a network connection.
	\item[$-$] There is no standard for discovering OSC devices in a network, thus addresses must be configured manually which is cumbersome.
	\item[$-$] Unlike MIDI, there is no standard namespace for interfacing with an OSC device, although, a proposal for a standard exists \cite{synoscopy}.
	\item[$-$] The number of applications that support OSC is very limited.
\end{enumerate}

# Synthesizer Fundamentals 

This chapter outlines the fundamental elements of a synthesizer and briefly describes the fundamental methods of sound generation.

## Oscillator

Oscillators are the fundamental building blocks of a synthesizer's sound generation engine.
They serve the purpose of emitting a periodic waveform.
An oscillator is controlled through its *frequency* and *amplitude* parameters.
In the context of a synthesizer there are additional controls for starting *phase*, the point at which the waveforms begins, and *type of waveform* to emit.

The amplitude parameter sets the *peak amplitude* for the signal, i.e. the absolute value of the waveforms highest amplitude.
Frequency is usually specified as number of waveform cycles per second (Hz) but in the software implementation stored as *phase increment* (angular frequency) for each sample step.
The software oscillators output is a sequence of samples at equidistant intervals $T$.
Let $f_s=1/T$ be the sample rate and $f$ the frequency in Hz ($s^{-1}$), then the phase increment $\omega$ is calculated like this:

\begin{equation}
\omega = \frac{2\pi\,f}{f_s}
\label{eq:phase-incr}
\end{equation}

The oscillators highest frequency is limited to $f_s/2$ or $\omega = \pi$, which is called *Nyquist frequency*.
In general, the (Nyquist-Shannon) *sampling-theorem* states that a signal can be exactly reconstructed from its digitization if its entire frequency spectrum lies below the Nyquist frequency \cite[p.~244]{Benson2008}.
In other words, it ensures that there are at least two sample points for any frequency contained in the sampled signal.

### Aliasing
\label{sec:aliasing}

\begin{figure}
\resizebox{\textwidth}{!}{
\includegraphics[width=\textwidth]{imgs/aliasing.pdf}}
\caption{Two sinusoids with angular frequencies $\omega_1 = \pi/2,\,\omega_2 = 3/2\,\pi$ sampled in intervals of $T=\pi/2$. Both sinusoids produce the same sampled signal due to aliasing.}
\label{fig:aliasing}
\end{figure}

\Cref{fig:aliasing} shows two sinusoids with frequencies $\omega_1=\pi/2$ and $\omega_2=3/2\,\pi$ that are sampled at sample rate $\omega_s=\pi/2$.
Clearly, $\omega_2$ is above the Nyquist frequency $\omega_{\text{Ny}}=\pi$, thus $\omega_2$ is *foldover* at $\omega_{\text{Ny}}$ which results in a frequency of $\omega_2\mod\omega_{\text{Ny}} = \pi/2$ that is equal to $\omega_1$, therefore $\omega_2$ is an *alias* of $\omega_1$, so both signals are indistinguishable after the sampling process.
This effect is called *aliasing* or *foldover* and is an inevitable result of sampling or sample rate conversion, hence signal components with frequencies above Nyquist must be removed or reduced before fed into the sampling process.
The effect of foldover in the frequency spectrum is shown in \cref{fig:foldover}.

The alias $f_a$ for a frequency $f$ and a sampling frequency $f_s$ can be calculated as shown in \cref{eq:aliasing-frequency}\footnote{\citeauthor{Dashow1978} presented a method for generating non-harmonic spectra using foldover frequencies \cite[p.~82]{Dashow1978}.}.

\begin{equation}
f_a =
\begin{cases}
|N\,f_s - f|,& \text{if }N\text{ is even}\\
|(N+1)f_s - f|,& \text{otherwise}
\end{cases}
\text{, where }N = \lfloor{}f/f_s\rfloor{}
\label{eq:aliasing-frequency}
\end{equation}

\begin{figure}
\includegraphics[width=\textwidth]{imgs/foldover}
\caption{Spectra for waveforms sampled at a) $f_{\text{Ny}}=30$ and b) $f_{\text{Ny}}=10$ which is $1/3$ of the highest frequency contained, hence foldover (aliasing) occurs.}
\label{fig:foldover}
\end{figure}

### Waveforms
\label{sec:waveforms}

Synthesis techniques like subtractive synthesis require a source signal with rich harmonic content, hence providing only sine wave oscillators is not sufficient.
Oscillator waveforms typically used in subtractive synthesis are *triangle*, *sawtooth* and *square wave* the so called *trivial* or *geometric waveforms* \cite[p.~5]{Pekonen2013} because of their well-defined shape consisting of piece-wise linear or constant segments.
Non band limited versions of those waveforms are shown in \cref{fig:waveforms}.
To prevent aliasing artifacts caused by discontinuities in the waveform (square and sawtooth) or its slope (triangle) it is required to create band limited versions of those waveforms.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/basic-waveforms}
\caption{Common (non band-limited) waveforms supported by most synthesizers: a) sine wave, b) triangle wave, c) ramp/sawtooth, d) square wave}
\label{fig:waveforms}
\end{figure}

#### Fourier Synthesis
\label{sec:fourier-synthesis}

*Fourier synthesis* is used to create band limited versions of those complex waveforms.
It uses the properties of *Fourier series* representation of arbitrary but periodic waveforms \cite[p.~103]{LoyMusimathics2}:

> Any periodic vibration, [\ldots], can be built up from sinusoids whose frequencies are integer multiplies of a fundamental frequency, by choosing the proper amplitudes and phases.

This means that any periodic waveform can be constructed by specifying the power of each its *harmonics*, where a *harmonic* is an integer multiple of the waveforms fundamental frequency.
The Fourier series is defined as (\cref{eq:fourier-series})

\begin{equation}
f(t) = a_0/2 + \sum^\infty_{k=1}a_k \cos(2\pi k t + \phi_k) + \sum^\infty_{k=1}b_k \sin(2\pi k t + \phi_k)
\label{eq:fourier-series}
\end{equation}

where $a_k$ and $b_k$ are coefficients for the strength of the k-th harmonic. The harmonics phase is specified $\phi_k$ and $\omega_k=2\pi{}k$ sets its angular frequency.
By substituting\footnote{Eulers identities: $\cos = (e^{i\phi} + e^{-i\phi})/2$ and $\sin = (e\,i^{i\phi} + e\,i^{-i\phi})/2$} eulers equation (\cref{eq:eulers-equation})

\begin{equation}
e^{i\phi} = \cos(\phi)+i\sin(\phi)
\label{eq:eulers-equation}
\end{equation}

into \cref{eq:fourier-series} it can be written in complex form:

\begin{equation}
f(t) = \sum_{k=-\infty}^\infty c_n e^{i\,2\pi\,k\,t+\phi_k}.
\label{eq:fourier-series-complex}
\end{equation}

The summation range has changed for the complex Fourier series \cref{eq:fourier-series-complex} to $-\infty$ and $\infty$, thus there are now negative frequencies.
To retrieve a real-valued signal from the complex Fourier series it is required to take the conjugate transpose of each value, i.e. to specify the coefficients $c_k$ as pairs with a $-c_k$ for each positive coefficient.

The complex coefficients $c_k$ can be converted from the $a_k$, $b_k$'s with the following equation:

\begin{equation}
c_{\pm{}k}=1/2\left(a_k\pm{}i\,b_k\right)
\label{eq:fourier-coeff-conversion}
\end{equation}

<!-- conversion from complex to real
a_n = c_k + c_{-k} = 2\Re(c_k)
b_n = i\,(c_k + c_{-k}) = 2\Im(c_k)
-->

#### Bandlimited Waveforms

Bandlimited sawtooth, triangle and square waveforms can be created by means of Fourier synthesis.
<!-- Implementation detail: For each waveform only a single cycle is required because the oscillator will loop the cycle in the given frequency.-->
Sawtooth, in contrast to square and triangle waves, contains odd and even harmonics which makes them a great source signal because of their rich harmonic content.
Square and triangle waves consist solely of odd harmonic partials.
The complex Fourier series for sawtooth, triangle and square waves is shown in \cref{eq:sawtooth-series}, \cref{eq:squarewave-series} and \cref{eq:triangle-series} where the sum is zero for $n=0$.
The number of harmonics contained in the waveform is determined by summation limits and \cref{fig:waveforms} illustrates evaluated Fourier series for sawtooth (a) and square wave (b) at increasing numbers of harmonics partials.

\begin{equation}
x_\text{saw}(t) = \sum^{\infty}_{n=-\infty,\;n\neq{}0} -1^n \frac{e^{-i\,2\pi\,n\,t}}{n\pi}
\label{eq:sawtooth-series}
\end{equation}

\begin{equation}
x_\text{square}(t) = 2 \sum^{\infty}_{n=-\infty,\;n\neq{}0}\frac{e^{-i\,2\pi\,(2n - 1)\,t}}{n\pi}
\label{eq:squarewave-series}
\end{equation}

\begin{equation}
x_\text{triangle}(t) = 4\sum^{\infty}_{n=-\infty,\;n\neq{}0} \frac{e^{-i\,2\pi\,(2n-1)\,t}}{(n\pi)^2}
\label{eq:triangle-series}
\end{equation}

\begin{figure}
\includegraphics[width=\textwidth]{imgs/fourier-series-saw.pdf}
\includegraphics[width=\textwidth]{imgs/fourier-series-sqr.pdf}
\caption{One cycle of band limited a) sawtooth and b) square waveforms with increasing number of harmonics.}
\label{fig:waveforms}
\end{figure}

## Non-linearity of hearing
\label{sec:hearing}

The intensity of a sound is perceived logarithmically by human hearing \cite[p.~27]{West1986}.
Therefore, the *ratio* between two sound intensities is important and *not* the difference as in the case of linear perceived phenomenons.
The ratio of two physical quantities, e.g. signal amplitudes, is measured in \si{\decibel} (decibel), a dimensionless logarithmic unit.
Distinction should be made between the ratio of signal energy which is expressed by

\begin{equation}
10\log_{10}{}\left(\frac{a}{b}\right)
\label{eq:db-energy}
\end{equation}

and the ratio of signal power \cref{eq:db-power}.

\begin{equation}
20\log_{10}{}\left(\frac{a}{b}\right)
\label{eq:db-power}
\end{equation}
The difference in the scaling factors is based on the definition of signal energy where the square of the signals amplitude is taken, see \cref{eq:signal-energy}.
\begin{equation}
\int^{\infty}_{-\infty} \left|x(t)\right|^2 dt
\label{eq:signal-energy}
\end{equation}

\begin{figure}
\includegraphics[width=\textwidth]{imgs/db.pdf}
\caption{Relationship between \si{\decibel} and corresponding amplitude ratios on a logarithmically scaled y-axis.}
\label{fig:db}
\end{figure}
\Cref{fig:db} illustrates the relationship between \si{\decibel} levels (x-axis) and corresponding amplitude ratios on a logarithmically scaled y-axis.
The pressure levels audible by human ears range from 0.00002 \si{\newton\per\meter\squared} to 200 \si{\newton\per\meter\squared} \cite[p.~26]{West1986}, which is seven orders of magnitude larger than the audibility threshold and clearly shows that logarithmic scale is better suited than a linear one to represent sound level ratios.

## Envelope Generators

The sound produced by an musical instrument is usually not static and changes in amplitude or spectral content over time.
To simulate these *time-varying* waveforms a function of time, the envelope generator, is used to controls parameters of an oscillator or other parts of a synthesizers sound engine, e.g. the cutoff of a frequency filter.
In \cref{fig:piano-c4} the time-varying behavior of sounds produced by musical instruments is illustrated by an example of waveform plot of a C\textsubscript{4} note played on piano.

<!-- Replace this by the high resolution image -->
\begin{figure}
\includegraphics[width=\textwidth]{imgs/piano_c4_small.png}
\caption{Waveform plot of sampled C\textsubscript{4} note played on a piano \cite{freesound:piano}.}
\label{fig:piano-c4}
\end{figure}

There are various types of envelope generators that range from simple two stage models, for fading the sound in and out, to ones which have an arbitrary number of stages and envelope shapes.
A commonly used model with a reasonable amount of controllable parameters is the so called ADSR envelope, which stands for the four different stages of the envelope which are *Attack*, *Decay*, *Sustain* and *Release*.
Because of the non-linearity of human hearing, as discussed in \cref{sec:hearing}, it is not sufficient to linearly ramp values between those four stages because this would not yield a smooth change in perceived loudness.

\Citeauthor{Puckette2006a} proposes three different amplitude envelope transfer functions \cite[p.~94]{Puckette2006a} where $10^{2(x-1)}$ converts from \si{\decibel} to linear and the quartic curve $x^4$ approximates the exponential \si{\decibel} curve while being computationally less expensive and reaching true zero at $x=0$.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/adsr-transfer-functions}
\caption{Three amplitude envelope transfer functions for input values in the range of [0, 1] as proposed by \cite[p.~94]{Puckette2006a}.}
\label{fig:transfer-functions}
\end{figure}

An ADSR generator's output is fully determined by five parameters, that are the output *level* and *duration* of the attack stage, decay duration, sustain level and duration of release.
\Cref{fig:adsr} shows the output and stages for an envelope generator with exponential transfer function.
The generator will start the output on an event like a key press and will reside in the sustain stage as long as the key is still pressed.
If the key is released the generator will switch to the release stage independent of its state at the time of the event.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/adsr}
\caption{An ADSR envelope with equal duration of 0.5\si{\second} for each stage and attack and sustain levels of 1 and 0.5.}
\label{fig:adsr}
\end{figure}


# Synthesis Techniques
\label{sec:synthesis-techniques}

Since the invention of the first electrical organ in 1894 \cite[p.~83]{Roads:CMT}, the *Telharmonium* built by Thaddeus Cahill, a lot of different synthesizing techniques have been developed.
Those techniques can be roughly divided into two broad categories, techniques for *mimicking the sound of traditional instruments*, like

- Karplus-Strong synthesis \cite{KarplusStrong}, a simple technique for simulating plucked-string or drum sounds
- physical modeling synthesis which uses a mathematical model of an instrument to generate sounds

or techniques for generating arbitrary sounds, possibly not reproducible by a physical instrument, like *Additive*, *Subtractive* and *FM* synthesis.
Developing a physical simulation of a traditional instrument is not the aim of this thesis, therefore the latter techniques will be described in this section.

## Additive Synthesis

Additive synthesis is one of the oldest sound synthesizing techniques.
It uses separate sinusoidal oscillators to generate a complex sound from its partials.
As the name suggests, the output of each oscillator is added up to obtain the resulting output signal.
The basic structure of an additive synthesizer is shown in \cref{fig:additive-synthesizer}, where $∿_i$ denotes a sinusoidal oscillator with frequency input $f_i$ and amplitude input $a_i$.

An advantage of additive synthesis is its great versatility, because virtually any sound can be synthesized, given a sufficient amount of oscillators.
This comes with two major downsides:

1. this method is computationally expensive.
2. it is hard to control because there are at least as twice as many parameters as there are oscillators.

Additionally, to be able to simulate real or time-varying artificial sounds there must be functions that control those parameters over time, e.g. to reduce the amplitude of higher frequency partials when the sound decays.
The coefficients obtained by Fourier analysis of a real sound (e.g. a sample of played key on a piano) can be used as parameters to reconstruct this sound through additive synthesis, this process is sometimes called *Fourier recomposition* \cite[p.~88]{West1986}.

\begin{figure}
	\centering
	\begin{tikzpicture}
[
    auto,
    >=latex,
    node distance = 1cm
]
\node[block](osc1){$∿_1$};
\node[input, above=of osc1, xshift=-4mm](f1){$f_1$};
\node[input, above=of osc1, xshift=4mm](a1){$a_1$};
\draw[->](f1)--([xshift=-4mm]osc1.north);
\draw[->](a1)--([xshift=4mm]osc1.north);

\node[block, right=of osc1](osc2){$∿_2$};
\node[input, above=of osc2, xshift=-4mm](f2){$f_2$};
\node[input, above=of osc2, xshift=4mm](a2){$a_2$};
\draw[->](f2)--([xshift=-4mm]osc2.north);
\draw[->](a2)--([xshift=4mm]osc2.north);

\node[right=of osc2](oscX){\ldots};

\node[block, right=of oscX](oscN1){$∿_{n-1}$};
\node[input, above=of oscN1, xshift=-4mm](fN1){$f_{n-1}$};
\node[input, above=of oscN1, xshift=4mm](aN1){$a_{n-1}$};
\draw[->](fN1)--([xshift=-4mm]oscN1.north);
\draw[->](aN1)--([xshift=4mm]oscN1.north);

\node[block, right=of oscN1](oscN){$∿_n$};
\node[input, above=of oscN, xshift=-4mm](fN){$f_n$};
\node[input, above=of oscN, xshift=4mm](aN){$a_n$};
\draw[->](fN)--([xshift=-4mm]oscN.north);
\draw[->](aN)--([xshift=4mm]oscN.north);

\node[sum, below=of oscX](sum){$+$};
\draw[->](osc1.south)--(sum);
\draw[->](osc2.south)--(sum);
\draw[->](oscN1.south)--(sum);
\draw[->](oscN.south)--(sum);
\node[output, below=of sum](out){$y_n$};
\draw[->](sum)--(out);
	\end{tikzpicture}
	\caption{Basic structure of an additive synthesizer.}
	\label{fig:additive-synthesizer}
\end{figure}

## Frequency Modulation (FM) Synthesis

Frequency Modulation (FM) was originally used in telecommunications to encode information on a carrier wave by modulating the waves instantaneous frequency, e.g. for radio broadcast.
In \citeyear{Chowning:FM}, \citeauthor{Chowning:FM} presented a new application of this well-known process to control spectral components of an audio signal with great simplicity \cite[p.~1]{Chowning:FM}.
Contrary to its well-understood use for radio transmission, both the *carrier* and the *modulating frequency* are inside the audio band.
The audio spectrum is formed by the carrier wave and side frequencies which are introduced through frequency modulation.
Modulation of the carrier wave is determined by two factors:

- the *frequency of the modulating wave* $m_f$ sets rate at which the instantaneous frequency of the carrier varies.
- the *amount of modulation* $m_a$ which is equal to the modulating waves amplitude.

If both the carrier as well as the modulator, are sinusoids then the instantaneous frequency maybe be calculated as follows \cite[p.~2]{Chowning:FM}:

\begin{equation}
y(t) = A \sin\left(c_f t + I \sin(m_f t)\right)
\label{eq:fm}
\end{equation}

where $A$ is the *peak amplitude*, $c_f$ is the carrier wave's frequency and $I = m_a/m_f$ is the ratio of modulation amount to modulation frequency also called *modulation index*. 
A table of waveforms generated by different values of $m_f$ and $m_a$ is shown in \cref{fig:fm-table}.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/fm-grid}
\caption{One cycle of a FM modulated sine wave for different modulation intensities $a_m$ and modulator frequencies $f_m$}
\label{fig:fm-table}
\end{figure}

For $I=0$ there is no modulation, but non-zero values will result in frequencies occurring below and above the carrier frequency at intervals of the modulating frequency.
\citeauthor{Chowning:FM} describes the relation of modulation index and occurrence of side frequencies like this \cite[p.~2]{Chowning:FM}:

> The number of side frequencies which occur is related to the modulation index in such a way that as $I$ increases from zero, energy is \enquote{stolen} from the carrier and distributed among an increasing number of side frequencies.

This behavior is shown in \cref{fig:fm-bandwidth} for different modulation indices by constant modulation and carrier frequency. Negative amplitudes for frequency components indicate *phase inversion*\footnote{$-\sin(\phi)=\sin(-\phi)$}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{imgs/fm-bandwidth}
\caption{Magnitude spectra for frequency modulated carrier frequency $c_f$ at different modulation indices $I$ and constant modulation frequency $m_f$. Bandwidth increases symmetrically around $c_f$ with $I$.}
\label{fig:fm-bandwidth}
\end{figure}

Specific carrier and modulation frequency ratios and modulation index values will produce sideband frequencies that fall into the negative spectrum.
Those negative frequency components will be reflected (aliased) around $\SI{0}{\hertz}$.
Reflected sideband components will either increase or---if they are phase inverted---decrease the energy in the spectrum.

Harmonic spectra\footnote{Overtones are an integer multiple of waves the fundamental frequency.} will be generated if the ratio of carrier and modulation frequency is a rational number.
Ratios that are irrational numbers, e.g. $1/\sqrt{2}$, will result in inharmonic spectra because the reflected sideband frequencies will fall inbetween the positive frequency components.

Carrier and sideband component amplitudes can be determined analytically by evaluating n-th order Bessel functions $J_i$ of the first kind with the modulation index as argument.
A quick estimation for the resulting bandwidth of different modulation indices is shown in \cref{fig:fm-bessel} by evaluating Bessel functions $J_0$ through $J_{15}$ for those indices.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/fm-bessel}
\caption{Bandwidth estimation for modulation indices $I$ ranging from 0 through 20 by evaluating Bessel functions $J_0$ through $J_{15}$ and showing resulting sideband frequency $s_f$ amplitudes $s_A$ \cite[p.~5]{Chowning:FM}.}
\label{fig:fm-bessel}
\end{figure}

The most basic FM synthesizer *algorithm* consists of two *operators* (which are just oscillators in FM terminology), a modulator and a carrier, where the modulators output is summed with the carriers fundamental frequency.
<!-- TODO: untereinander verbunden -->
An FM algorithm is described by how its operators are connected among each other.
\Cref{fig:fm-operator} shows the structure of the most basic FM algorithm, a simple pair of operators.

\begin{figure}
	\centering
	\begin{tikzpicture}
[
    auto,
    >=latex,
    node distance = 5mm
]
\node[block](modulator){$∿$};
\node[input, above=of modulator, xshift=-4mm](fm){$f_m$};
\node[input, above=of modulator, xshift=4mm](am){$a_m$};
\draw[->](fm)--([xshift=-4mm]modulator.north);
\draw[->](am)--([xshift=4mm]modulator.north);
\node[sum, below=of modulator](sum){$+$};
\node[input, right=of sum](fc){$f_c$};
\draw[->](modulator)--(sum);
\draw[->](fc)->(sum);
\node[block, below left=of sum](carrier){$∿$};
\draw[->](sum)-|([xshift=4mm]carrier.north);
\node[input, above=of carrier, xshift=-4mm](An){$A(n)$};
\draw[->](An)--([xshift=-4mm]carrier.north);
\node[output, below=of carrier](out){$y_n$};
\draw[->](carrier)--(out);
	\end{tikzpicture}
	\caption{Most basic FM algorithm, a pair of operators with one modulator and carrier.}
	\label{fig:fm-operator}
\end{figure}

FM's advantages lie in the simplicity of control, the small computational effort that is required and the great amount of flexibility, resulting from arranging operators in different algorithms.
On the other hand, FM synthesis is likely to introduce undesirable aliasing of higher frequencies which should be taken into account when implementing the algorithm.

## Subtractive Synthesis
\label{sec:subtractive-synthesis}

Subtractive Synthesis is---like the name implies---the opposite of *Additive Synthesis* and creates musical tones by *removing* parts of the frequency spectrum of a source signal.
A *filter* thereby amplifies or attenuates regions of the spectrum while the source signal passes through.
Spectrally rich signals like noise, sawtooth or square waves are well suited for the use as sound sources. The following section introduces filters by means of audio signal processing, describes commonly used filter types in musical synthesizers and discusses FIR and IIR filters.

# Digital Filters
\label{sec:digital-filters}

Filter is a broad term that has many different and often very general definitions.
Such a general definition is given by \citeauthor{Smith:FilterTheory} in \cite{Smith:FilterTheory}:

> Any medium through which the music signal passes, whatever its form, can be regarded as a filter.

Surprisingly, even \citetitle{Rabiner1972} (\cite{Rabiner1972}) uses the term without prior specification.
In this thesis a more specific definition will be used \cite[p.~326]{Proakis:DSP}:

> The linear time-invariant system, through its frequency response function, attenuates some frequency components of the input signal and amplifies other frequency components. Thus the system acts as a filter to the input signal.

This is analog to the description given in \cref{sec:subtractive-synthesis}, despite the terms *linear time-invariant* (LTI) system and *frequency response function* not having been explained.

## Linear Time-Invariant Systems

LTI systems are used as filters because \enquote{no new spectral components are introduced} \cite{Smith:Filters} by them.
The time-invariance property is not overly restrictive because it also holds for filters that change slowly over time.
This is a very convenient property because if musicians were not allowed to change a subtractive synthesizers filter while playing then the result would be static and uninteresting sounds.

### Linearity and Time-Invariance

A system is *linear* if the superposition principle \cref{eq:superposition} holds.

\begin{equation}
F[a_1 x_1(n) + a_2x_2(n)] = a_1F[x_1(n)] + a_2F[x_2(n)]
\label{eq:superposition}
\end{equation}

In other words, the response of system $F$ applied to two (or more) stimuli $x_{1,2}$ is equal to the sum of responses of the system applied to each stimulus individually, for any real values of $a_{1,2}$.
This also shows the scaling the property of linear systems, i.e. scaling of a systems input results in an identical scaling of the response.
A system is *time invariant*, if

\begin{equation}
F[x(n-k)]=y(n-k),
\label{eq:time-invariance}
\end{equation}

for any time shift $k$.
Thus the response of the system applied to a stimuli delayed by $k$ units of time is equal to the systems response delayed for the same amount.
Hence, if a system obeys both properties, linearity and time-invariance, then it is called an LTI system.
Such a system is characterized completely by its *impulse* or *frequency response*.

### LTI filters
\label{sec:LTI-filters}

Linear time-invariant digital filters (systems), in the following simply called *filters*, may be written as *difference equation* 

\begin{equation}
y[n] = \sum^{M}_{i=0}b_i x[n-i]-\sum^{N}_{j=1}a_j y[n-j]
\label{eq:difference-equation}
\end{equation}

where $x$ denotes the input signal\footnote{The term sequence and signal will be used interchangeably.}, $y$ the output signal, and the filter's *coefficients* are the constants $a_j$ and $b_i$.
A signal therefore is a *sequence* of real numbers denoted as a function of integer index $x[n]$ where $n$ denotes the *n*-th sample.
Coefficients $a_j, b_i$ must be in $\mathbb{R}$ to obtain a *real valued filter* that has a real valued output for any given real valued input signal.
Another requirement of the filter is to be *causal*, i.e. it does not depend on future values and only uses past input and output values to calculate its current output value, otherwise the filter can not be realized.
A filters *order* is the maximum sample delay used ($\max(M,N)$ in \cref{eq:difference-equation}) and in general the higher a filters order the steeper its transition slope.

Another way of representing a digital filter is by its rational system- or *transfer function* $H(z)$ in the *z*-domain as shown for a *causal* filter in \cref{eq:transfer-function} where $z = Ae^{i\phi}$ is some complex exponential with amplitude $A$ and phase $\phi$ that acts as time-shift of $j$ samples.
This *z*-domain representation will be required when designing a digital IIR filters based on an analog prototype.

\begin{equation}
H(z) = \frac{Y(z)}{X(z)} = \frac{\sum^{M}_{k=0}b_k z^{-k}}{1+\sum^{N}_{k=1}a_k z^{-k}}
\label{eq:transfer-function}
\end{equation}

The z-domain representation of a discrete-time signal $x[n]$ is defined as the bilateral transform:

\begin{equation}
X(z) = \mathcal{Z}\left\{x[n]\right\} = \sum^{\infty}_{n=-\infty}x[n]z^{-n}.
\label{eq:bilateral-transform}
\end{equation}

LTI filters can be divided into two types, first feedforward or *finite impulse response* (FIR) filters which only use previous input values ($a_j$s are zero) and second feedback or *infinite impulse response* (IIR) filters that also use previous output values to calculate the present filter output $y[n]$.

## Impulse Response

Another way of representing a LTI system in the time domain is its response to a signal impulse, called the systems *impulse response*.

>   [A one-sample impulse] contains energy at all frequencies that can be represented at the given sampling frequency.
    Hence, a general way of characterizing a filter is to view its response to a one-sample pulse[\ldots] \cite[p.~400]{Roads:CMT}.

The hereby used input signal is the Kronecker delta function $\delta(n)$ \cref{eq:kronecker-delta} which is one if $n = 0$ and zero otherwise.

\begin{equation}
\delta(n) = \begin{cases}
1,\quad n=0,\\
0,\quad otherwise
\end{cases}
\label{eq:kronecker-delta}
\end{equation}

Applying the LTI system on the impulse signal $\delta(n)$ yields the systems impulse response denoted as $h[n]$.
If the impulse response does not reach zero over time the system is said to be *unstable*.
Any LTI system is fully described by its impulse response.
*Convolving* an input signal $x[n]$ with the impulse response $h[n]$ yields the systems time-domain output for that signal \cite[p.~73]{Proakis:DSP}:

\begin{equation}
y[n] = x\ast h = \sum^\infty_{k=-\infty}x[k]h[n-k] = \sum^\infty_{k=-\infty}x[n-k]h[n]
\label{eq:discrete-convolution}
\end{equation}

Clearly, convolution $\ast$ is a commutative operation.
This property can be used to optimize possible implementations \cite{Karas:Convolution}.

## Frequency Response

Evaluating the systems transfer function \cref{eq:transfer-function} on the unit circle, i.e. set $z$ to $e^{i\omega T}$ where $T$ is the sampling interval, yields the LTI systems frequency response which is the frequency spectrum of the output divided by the frequency spectrum of the input.

It is easy to show that evaluating the (bilateral) z-transform on the unit-circle will find the spectrum because setting $z=e^{i\omega T}$ in \cref{eq:bilateral-transform} results in the definition of the bilateral discrete-time Fourier transform:

\begin{equation}
X\left(e^{i\omega T}\right) = \sum^{\infty}_{n=-\infty} x[n]e^{i\omega T}n .
\label{eq:DTFT}
\end{equation}

In the following only *causal* sequences are of interest, thus the *unilateral* versions of z-transform and discrete-time Fourier transform are used in which the summation index starts at $n=0$.
Also, the sampling index $T$ is set to $1$ for simplicity.
Another way of obtaining an LTI systems frequency response is by applying the Fourier transform on the systems impulse response $h(z)$ \cite[p.~301]{Proakis:DSP}:

\begin{equation}
H(\omega) = \sum^\infty_{n=-\infty} h[n]e^{i\omega n}
\end{equation}

It is sufficient to evaluate the frequency response function only for $\omega \in [-\pi,\pi)$ because all frequencies are mapped into a single cycle of the unit circle and the spectrum would repeat for additional cycles anyway\footnote{see aliasing \cref{sec:aliasing}}.

### Magnitude- and Phase Response

The complex valued result of the frequency response function can be decomposed into two real valued functions \cite{Smith:Filters} the systems *magnitude response*\footnote{Sometimes improperly called *amplitude* response because amplitudes can be negative.} $|H(\omega)|$, and *phase response* $\angle H(\omega)$.

The magnitude response of a filter shows how frequencies are attenuated or amplified and the phase response specifies the phase-shift experienced by each frequency.
In general the frequency response is of more interest because it is better suited for characterizing a filter but the phase response should not be completely ignored.

\citeauthor{Chamberlin:85} states that \enquote{Poor phase response in
a filter also means poor transient response} and this effect will become worse with increasing filter order \cite[p.~392]{Chamberlin:85}, i.e. sharp changes in a waveform (transients) will be smoothed by a filter with poor phase response.
However, even a poor filter phase response is quite good compared to the phase error introduced by the audio speaker while transforming the signal from an electrical to an acoustical one \cite[p.~392]{Chamberlin:85}.

## Filter Classification

Filters of a musical synthesizer are classified by the magnitude curve of their frequency response function $|H(\omega)|$ which is the filter's characteristic *frequency response* curve.
Exemplary frequency response curves for lowpass, highpass, bandpass and notch (sometimes called bandreject or bandstop) filters are shown in \cref{fig:filter-classification}.

Low- and highpass filters cut all frequencies below, respectively above of the *cutoff frequency* $f_c$, while bandpass and notch filters let frequencies in a certain range (*frequency band*) pass through or rejecting them.
The width of the pass- or stopband is an additional property of those last two filter types and the difference between their high and low cutoff frequencies is called *bandwidth*.
Correspondingly, the center of the pass- or stopband---the point of maximum or minimum amplitude in this band---is the fiter's *center frequency*.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/filter-classification}
\caption{Log-log plots for exemplary frequency response curves of four elementary filter types with \SI{12}{\decibel\per\text{octave}} roll-off where $f_c$ denotes the cutoff frequency at the half-power point ($\SI{-3}{\decibel}$) shown as a dotted line.}
\label{fig:filter-classification}
\end{figure}

A filters cutoff frequency is commonly specified for the half-power point \cite[p.~8]{Rabiner1972} where the filter reduces the signals energy to $1/\sqrt{2}\approx 0.707$ or in terms of signal power $(1/\sqrt{2})^2 = 1/2$ or $\SI{-3}{\decibel}$ (see \cref{eq:db-power}).

An ideal filter would have a sharp cut between the pass- and stopband that looks like a rectangle in the frequency response but such a filter is not realizable because it would be of infinite order.
Hence, there is a transition between the pass- and stopband called *transition band*.
\Cref{fig:filter-bands} shows those bands and their bandwidths $b_{pass},b_{trans}, b_{stop}$ as well as other constraints like pass- and stopband $a_{pass}, a_{stop}$ that must be specified and taken into account when designing a filter a filter..

\begin{figure}
\includegraphics[width=\textwidth]{imgs/filter-bands.pdf}
\caption{Terminology for describing the frequency response of a low-pass filter.}
\label{fig:filter-bands}
\end{figure}

The steepness of the frequency response curve in the transition band is measured in $\si{\decibel\per\text{octave}}$ where a larger value implies an increased steepness of the curve,  e.g. a *roll-off* of $\SI{12}{\decibel\per\text{octave}}$ for a lowpass filter means that the amplitude is reduced by $\SI{12}{\decibel}$ for each doubling in frequency above $f_c$.

Depending on the curve of the phase reponse a filter is said to have *zero phase* if the phase-shift is constant over all frequencies, *linear phase* if there is a linear relationship between phase-shift and frequency and *non-linear phase* otherwise.
The phase response for the lowpass filter used in \cref{fig:filter-classification} is shown in \cref{fig:filter-phase}

\begin{figure}
\includegraphics[width=\textwidth]{imgs/filter-phase}
\caption{Non-linear phase response ($\angle H(\omega)$) of a second-order Butterworth lowpass filter.}
\label{fig:filter-phase}
\end{figure}

## FIR Filters

A FIR filter's response to an impulse will die away after a finite period of time \cite[p.~406]{Roads:CMT}, hence the name *finite* impulse response filter.
The filter's structure is simply the sum of delayed and weighted samples where the weights for each delay are the coefficients $a_j$.
As described in \cref{sec:LTI-filters}, the filters order is equal to the order of its transfer function polynomial, i.e. the total number of unit-sample delays it uses.
There are various methods for FIR filter design, e.g. window design method, frequency sampling or equiripple method \cite[p.664-690]{Proakis:DSP} and constraint-based linear programming algorithms like METEOR \cite{Steiglitz:METEOR}.

The general equation for a finite impulse response filter is equal to \cref{eq:difference-equation} when all recursive coefficients $a_k$ are zero:

\begin{equation}
y[n] = \sum^M_{k=0}b_k x[n-i] .
\label{eq:fir-general}
\end{equation}

Thus, the denominator of a FIR filter's transfer function is one, hence its polynomial

\begin{equation}
H(z) = \sum^{M}_{k=0}b_k z^{-k}
\label{eq:fir-transfer-function}
\end{equation}

has only zeroes but no poles.
This implies that FIR filters are *always stable*, i.e. a *bounded* (finite) input always results in a bounded output.
The transfer function \cref{eq:fir-transfer-function} may be written in factored form where each complex zero $q$ of the polynomial can be directly seen:

\begin{equation}
H(z) = (1-q_1~z^{-1})(1-q_2~z^{-1})\cdot\ldots\cdot(1-q_M~z^{-1})
\label{eq:fir-transfer-function-factorized}
\end{equation}

There can be less than $M$ factors if some of them cancel out.

If $z$ has the value of one of these factors $q$ then the transfer function evaluates to zero.
The positions of the zeroes in the complex $z$-plane of a 12-th order lowpass FIR filter are shown in \cref{fig:filter-zero-fir}.

By the complex zero's angle from the $z$-plane's origin is determined which frequencies are effected from it and its distance to the unit circle, on which the frequency response is evaluated, determines how large the attenuation is.
Frequencies are mapped on the unit circle counterclockwise starting from 0 at $(1, 0)$ and going to $\pi$, which translates to a frequency limit like the Nyquist frequency, at $(-1, 0)$ where positive frequencies are carried out in the upper half of the unit circle and negative frequencies in the lower half.
It can also be seen that complex zeroes $q=a+i\,b$ come in pairs; if they are not laying on the some point; where one of them is conjugate transposed $q^*=a-i\,b$, so that the imaginary parts cancel each other out to obtain a real valued filter response.

\begin{figure}
\centering
\includegraphics[width=.7\textwidth]{imgs/filter-zero-fir}
\caption{Zeroes of 12-th order lowpass FIR filter with cutoff frequency $f_c=\pi/4$}
\label{fig:filter-zero-fir}
\end{figure}

## IIR Filters

Infinite Impulse Response filters additionally use previous output values to calculate the recent result.
Therefore, an IIR's rational transfer function contains feedback coefficients $a_k$ \cref{eq:transfer-function} and can be viewed as $H(z) = H_1(z)H_2(z)$ \cite[p.~583]{Proakis:DSP}, where $H_1(z)$ consists of the zeroes of $H(z)$ and

\begin{equation}
H_2(z) = \frac{1}{1+\sum^N_{k=0}a_k z^{-k}}
\label{eq:iir-poles}
\end{equation}

consists of the poles of $H(z)$ \cite[p.~583]{Proakis:DSP}.
In contrast to FIR filters an impulse fed into an IIR filter will cause an infinite response---numerical quantization error ignored---because the transfer function will never truly reach zero due to the use feedback values, therefore the name *Infinite Impulse Response* filter.

IIR filters can become *unstable* because they can have poles outside of the complex plane's origin, e.g. if there is a pole on the unit circle at frequency $\omega_p$ then $H(z)$ would become $\infty$ when evaluated on the unit circle ($z=e^{i\omega}$) for $\omega_p$.

An advantage of the use of feedback values is that IIR filters can achieve much steeper transition band slopes than FIR filters of the same order, thus they are more efficient in the number of arithmetic operations and memory required.

The effect of transfer function poles in the $z$-plane is the opposite of zeros, i.e. the distance to the unit circle determines how much a frequency is amplified by the pole.
Musical synthesizers often allow the user to specify a *resonance* parameter for the filter cutoff frequency, where a low resonance results in smooth transition transition from the pass- to the transition band whereas a high value creates a peak at the cutoff frequency as shown for different resonance values in the magnitude response of a IIR second-order lowpass filter in \cref{fig:filter-resonance}.
Such a resonance parameter is easy to realize for IIR filters.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/filter-resonance}
\caption{Magnitude response of a recursive second-order lowpass filter for different resonance values.}
\label{fig:filter-resonance}
\end{figure}

IIR filters are very sensitive to numerical rounding error, where the sensitivity depends on their order and implementation structure \cref{sec:iir-structure}.
Therefore, second-order systems, so called *biquads*, are used as building blocks for higher-order filters (order $>= 4$) because the filters sensitiviy to quantization increases with its order \cite[p.~132,589]{Proakis:DSP}.
Filters of odd-order a constructed as separate biquad and a single-order system whereas even-order filters are constructed solely from biquads.

Analog IIR filter design has a long history and is therefore a well researched and understood topic.
There are a number of commonly used analog filters with different characteristics \cite[p.~717pp]{Proakis:DSP}:

- *Butterworth* all-pole filters with monotonic frequency magnitude response in both pass- and stopband.
- *Chebyshev Type I* all-pole filters with equiripple behavior in the passband and monotonic characteristic in the stopband.
- *Chebyshev Type II* filters are like *Type I* except that they have monotonic passband characteristic and equiripple stopband behavior.
- *Elliptic Filters* with equiripple behavior in both pass- and stopband.

The filter design technique used in this thesis is to take an analog prototype filter and transform it into a discrete-time filter by using the *Bilinear transform*.
Other design techniques like *approximation of derivates* or *design by impulse variance* have the limitation that they are only valid for a limited set of filter classes \cite[p.~712]{Proakis:DSP}.

### Bilinear Transform

The Bilinear transform, defined by \cref{eq:bilinear-transform} for sampling interval $T$, is used to convert a transfer function of a continious-time LTI filter transfer function $H_a(s)$ into a transfer function of a discrete-time LTI filter $H(z)$, where $H_a(s)$ defined in the $s$-domain with $s=\sigma + i\Omega$.

\begin{equation}
s = \frac{2}{T}\left(\frac{1-z^{-1}}{1+z^{-1}}\right)
\label{eq:bilinear-transform}
\end{equation}

A continuous time filter defined in the $s$-plane is stable if all of its poles are located in the left-semi plane which is mapped by the bilinear transform into the unit circle, hence the transformed discrete-time filter is stable if all of its poles are located inside the unit circle of the z-domain.

The frequency relationship of the bilinear transform is non-linear, thus continuous time frequencies $\Omega \in [-\infty, \infty]$ are mapped from the $i\Omega$ axis of the $s$-plane into digital frequencies $\omega \in [-\pi, \pi)$ on the unit-circle by the following transformation (also called *frequency warping*)

\begin{equation}
\omega = \frac{2}{T} \arctan\left(\frac{\Omega T}{2}\right),
\label{eq:frequency-warping}
\end{equation}

and the inverse transformation is given by

\begin{equation}
\Omega = \frac{2}{T} \tan\left(\omega\frac{T}{2}\right).
\label{eq:frequency-warping-inverse}
\end{equation}

### Bilinear Transform example

The following steps illustrate the general procedure of designing a filter using bilinear transform at the example of a second-order Butterworth lowpass filter with cutoff frequency $f_c=\SI{4000}{\hertz}$ for a sampling rate of $f_s = 48000$, hence sampling interval $T = 1/f_s$:
 
- Pre-warp the critical frequencies, in this case the filter's cutoff frequency $\omega_c = 2\pi f_c \si{\radian\per\second}$:

\begin{equation*}
\Omega_c = \frac{2}{T}\tan\left(\omega_c\frac{T}{2}\right) \approx \frac{2}{T} 0.268 \si{\radian\per\second}
\end{equation*}

- Set the critical frequency $\omega_c$ and apply the bilinear transformation \cref{eq:bilinear-transform} to obtain $H(z)$ from the analog transfer function $H_a(s)$:

\begin{align}
H_a(s) &= \dfrac{\Omega_c^2}{s^2 + s\sqrt{2}\Omega_c+\Omega_c^2}\\
H(z)  &= \dfrac{\left(\dfrac{2}{T}0.268\right)^2}{
            \left(\dfrac{2}{T}\dfrac{1-z^{-1}}{1+z^{-1}}\right)^2
            + \dfrac{2}{T}\dfrac{1-z^{-1}}{1+z^{-1}}\sqrt{2}\dfrac{2}{T}0.268
            + \left(\dfrac{2}{T}0.268\right)^2}\\
      &= \dfrac{\left(\dfrac{2}{T}\right)^2 0.268^2}{
            \left(\dfrac{2}{T}\right)^2 \left(\dfrac{1-z^{-1}}{1+z^{-1}}\right)^2
            + \left(\dfrac{2}{T}\right)^2 \dfrac{1-z^{-1}}{1+z^{-1}}\sqrt{2}\cdot 0.268
            + \left(\dfrac{2}{T}\right)^2 0.268^2}\\
      &= \dfrac{0.268^2}{
            \left(\dfrac{1-z^{-1}}{1+z^{-1}}\right)^2 + \dfrac{1-z^{-1}}{1+z^{-1}} \cdot 0.379 + 0.268^2}\\
      &= \dfrac{0.0495 (1 + z)^2}{0.4775 - 1.2795 z + z^2}\cdot\dfrac{z^{-2}}{z^{-2}}\\
      &= 0.0495\cdot\dfrac{1 + 2z^{-1} + z^{-2}}{
          1 - 1.2795z^{-1} + 0.4775 z^{-2}}
\label{eq:iir-example-transfer-function}
\end{align}

- $H(z)$ is evaluated on the unit circle to check the magnitude frequency response \cref{fig:freq-response-iir-example} which shows the that the cutoff frequency lays exactly on the half-power point

\begin{figure}
\includegraphics[width=\textwidth]{imgs/filter-freq-resp-iir-example}
\caption{Magnitude frequency response of $H(z)$ \cref{eq:iir-example-transfer-function} showing cutoff frequency $f_c$ at half-power point (dotted line).}
\label{fig:freq-response-iir-example}
\end{figure}

- The pole-zero diagram \cref{fig:pole-zero-iir-example} of $H(z)$ shows that the pair of poles is located inside the unit circle, hence the filter is real-valued and stable. Also, a pair of zeros is located at the maximum frequency point which gives the lowpass characteristic.

\begin{figure}
\centering
\includegraphics[width=.7\textwidth]{imgs/filter-pole-zero-iir-example}
\caption{Pole-zero diagram of $H(z)$ \cref{eq:iir-example-transfer-function} with pole and zero positions marked as $\times$, respectively \bullet.}
\label{fig:pole-zero-iir-example}
\end{figure}

- Lastly, the filters difference equation \cref{eq:difference-equation} can be derived directly from $H(z)$: $y[n] = 0.0495 x[n] + 0.099 x[n-1] + 0.0495 x[n-2] + 1.2795 y[n-1] - 0.4775 y[n-2]$.

## Implementation Structures for IIR Filters
\label{sec:iir-structure}

An recursive LTI filter given as difference equation may be implemented as one of four *direct-form* (DF) filter implementations.
The direct-form is another way of representing a filter, besides impulse response and difference equation, with the benefit of directly representing its implementation structure.

The four direct-form structures are DF-I and DF-II, shown in \cref{fig:direct-form} where ${z^{-1}}$ denotes a unit-sample delay, and their *transposed* counterparts.
Transposed forms can be obtained from their DF-I or DF-II forms by reversing the signal path directions, replacing sums with branch-points and vice versa.
This operations does not effect the filters transfer function.

The technical properties of all four forms are different, despite that they represent the same transfer function.
A DF-I implementation saves addition operations at the cost of requiring twice as many delays (memory) as necessary while a DF-II implementation saves memory by sharing delays at the cost of possible fixed-point arithmetic overflow \cite{Smith:Filters}.
Transposed forms, TDF-I and TDF-II, have enhanced numerical robustness while obeying the same advantages and disadvantages of their fundamental structure.
An additional advantage of TDF-II structures is that they perform well in applications where the filter parameters change in audio rate, in other words as implementation for time-varying filters \cite{Wishnick:TVF}.

\begin{figure}
\includegraphics[width=.48\textwidth]{imgs/Biquad_filter_DF-I}
\hfill
\includegraphics[width=.48\textwidth]{imgs/Biquad_filter_DF-II}
\caption{Direct-Form I and II implementation of a second-order filter with the normalized (divided by $a_0$) difference equation $y[n]=b_{0}x[n]+b_{1}x[n-1]+b_{2}x[n-2]-a_{1}y[n-1]-a_{2}y[n-2]$ \cite{Wiki:digital-filter}}
\label{fig:direct-form}
\end{figure}

### Comparison FIR against IIR

\begin{table}[]
    \centering
    \caption{Comparison of FIR against IIR filters.}
    \label{tab:fir-iir-comparison}
    \begin{tabular}{@{}rp{5cm}p{5cm}@{}}
    \toprule
\toprule
                 & FIR                        & IIR                                         \\ \midrule
Impulse Response & finite                     & infinite                                    \\
Magnitude response & arbitrary responses are easy to design, e.g. \emph{frequency sampling technique} \cite[p.~671]{Proakis:DSP} & often based on analog prototypes, arbitrary responses are hard to achieve   \\
Stability        & always stable              & feedback coeffecients can cause instability \\
Efficiency       & more memory and operations & less memory and operations                  \\
Linear phase     & always possible            & no technique available                      \\ \bottomrule
    \end{tabular}
\end{table}

A comparison between FIR and IIR filter design techniques is given by \cref{tab:fir-iir-comparison}.
For the synthesizer's filter an IIR design was chosen because of two requirements, first high computational efficiency to achieve short latencies, and an easy to implement parameter for controlling filter resonance.
Additionally, a Butterworth characteristic was picked because the phase response of Elliptic filters is more nonlinear and the equiripple behavior in pass- and/or stopband of Elliptic and Chebyshev characteristics is unfavorable.
It is not of great disadvantage for a musical synthesizers filter that a Butterworth filter rolls of more slowly at the cutoff frequency, because very sharp transition band steepness is not required.

# Oscillators and Waveform Synthesis

An oscillator is one of a synthesizer's fundamental building blocks because it is the source signal from which the desired sound is modeled from.
Typically, one or more oscillators are used as a signal source.
Subtractive synthesizers require spectrally rich source signals.
Therefore, oscillators must be able to generate a variety of waveforms other than pure sine, but at least the trivial ones listed in \cref{sec:waveforms}.
Requirements for an oscillator's waveform synthesis algorithm are to generate periodic band-limited signals, to avoid aliasing and to be computationally efficient.
The latter requirement originates from the number of times the oscillator is called, this is at least once for each sample instant.
Depending on the amount of polyphony, i.e. the maximum number of parallel voices playable, they can even be called multiple times for each sample.

\citeauthor{Valimaki2006} (and others \cite[p.~28]{Pekonen2007}, \cite{Otalvar2015}) divide digital oscillator algorithms into three classes in regard to the amount aliasing left\cite{Valimaki2006}:

1. Bandlimited methods without harmonics above the Nyquist frequency, e.g. additive or wavetable synthesis
1. Quasi-bandlimited methods with low aliasing, e.g. *BLIT* and *BLEP* methods \cite{Brandt2001}
1. Alias-supressing methods, e.g. oversampling and filtering trivial waveforms

A fourth class, so called ad-hoc methods, that uses non-linear processing techniques is mentioned in more recent publications (\cite{Otalvar2015}, \cite{Pekonen2007}).
They are not of interest for this research because this class of methods is developed for very specific applications \cite[p.~26]{Otalvar2015}.

The aim of this chapter is to introduce the generic structure of a digital oscillator and to evaluate quasi-bandlimited methods against (fully) bandlimited wavetable synthesis.

## Generic Oscillator Structure

A common structure that is used for a wide variety of oscillators is shown in \cref{fig:oscillator-structure}.
It consists of a *phase accumulator* which adds a *phase increment* $\varphi$ to itself each time a clock signal $t$ arrives.
The phase increment is given by the *fundamental frequency* $f_0$ as $\phi = 2\pi f_0/f_s$. <!-- TODO RENAME phase increment variable to something like phi_i
-->
Subsequently, an initial *phase offset* $\phi_0 \in [0, 2\pi]$ is added to the accumulators output.
The *phasor signal*

\begin{align}
\begin{split}
\phi[t]	&= \phi t \mod 2\pi\\
		&= \left(\phi[t - 1] + \phi\right) \mod 2\pi
\end{split}
\end{align}

for a discrete time variable $t$ wraps around on a full cycle. The second form shows the recurrence relation for the phasor signal where $\phi[t] = 0, \forall t \le 0$.
It is convenient to normalize $\phi[t]$ to a fraction of the waveform's period

\begin{equation}
\varphi[t] = \frac{\phi}{2\pi} t
\end{equation}

with $\varphi \in [0,1]$, e.g. to map the phasor to a wavetable lookup index.
The *wave* function maps the *phasor signal* to the desired wave shape and multiplies the output signal with a given amplitude *A*.

\begin{figure}
    \centering
    \begin{tikzpicture}
[
    auto,
    >=latex,
    node distance = 10mm
]
\node[input](clock){t};
\node[block, right=of clock](phasor){phase accu.};
\node[input, below=of phasor](freq){$f_0$};
\node[sum, right=of phasor](sum1){$+$};
\node[input, below=of sum1](phase){$\phi_0$};
\node[block, right=of sum1](wrap){wrap};
\node[block, right=of wrap](f){wave};
\node[input, below=of f](A){$A$};
\node[output, right=of f](y){y};
\draw[->](clock) -- (phasor);
\draw[->](freq) -- (phasor);
\draw[->](phasor) -- (sum1);
\draw[->](phase) -- (sum1);
\draw[->](sum1) -- (wrap);
\draw[->](wrap) -- node{$\phi[t]$}(f);
\draw[->](A) -- (f);
\draw[->](f) -- (y);
    \end{tikzpicture}
\caption{Block diagram of a generic oscillator.}
\label{fig:oscillator-structure}
\end{figure}

## Trivial Waveform Generation
\label{sec:trivial-waveform-generation}

The trivial way for generating geometric waveforms is to sample them without bandlimiting.
A sawtooth wave can be expressed by a *bipolar modular counter*

\begin{equation}
saw(t) = 2\varphi(t) - 1
\end{equation}

where

\begin{equation}
\varphi(t) = f_0 t \mod 1
\end{equation}

is a phasor signal (modular counter) for a continuous time variable $t$ in seconds \cite[p.~5]{Pekonen2013}.
An inverted sawtooth wave with a ramp that decreases from 1 to -1 can be obtained 
by

\begin{equation}
saw_{\text{invert}}(t) = 1 - saw(t).
\end{equation}

Both remaining waveforms, square and triangle, can be derived from sawtooth waves.
Rectangular waveforms can be produced by subtracting two sawtooth waveforms with a proper phase shift \cite[p.~22]{Valimaki2006}

\begin{equation}
rect(t)=saw(t) - saw(t - \frac{p}{f_0})
\end{equation}

where $p \in (0, 1)$ is the duty cycle.
Square waves are simply the symmetric case of rectangular pulses with 50% pulse width.
Another trivial way of generating a rectangular pulse is by comparing the output $x$ of bipolar modular counter with the pulse width $p$ as in the following closed form expression:

\begin{equation}
rect(x)=\begin{cases}
	1	& x < p\\
	0	& x = p\\
	1	& x > p
\end{cases}
\end{equation}

By taking the absolute value of a sawtooth wave one gets a inverted triangle in the range of zero to one, doubling and subtracting from one results in bipolar triangle wave as shown in the first form of \cref{eq:triangle-wave}.
The second form shows that integrating a square wave over time $t$ also results in a triangle wave which then needs to be scaled to a normalized range from -1 to 1 \cite[p.~7]{Pekonen2013}, hence a (scaled) square wave is the time derivate of a triangle waveform.

\begin{align}
\begin{split}
tri(t) 	&= 1 - 2|saw(t)|\\
		&= 4 f_0 \int^t_{-\infty}sqr(\tau)d\tau
		\label{eq:triangle-wave}
\end{split}
\end{align}

A straightforward digital implementation of those trivial waveforms is constructed by replacing the continuous-time phasor time signal with its discrete-time counterpart \cite[p.~8]{Pekonen2013}.
Unfortunately, those naive digital implementations suffer from severe aliasing distortion because the continous-time source signal of those geometric waveforms is not bandlimited, hence it contains an infinte number of harmonics as can be seen in their fourier-series representation (see \cref{eq:sawtooth-series}, \cref{eq:squarewave-series} and \cref{eq:triangle-series}).
The spectral tilt, i.e. the attenuation of harmonic partials with increasing frequency, is about $\SI{6}{\decibel}$ per octave for pulse and sawtooth waveforms and $\SI{12}{\decibel}$ per octave for triangle waveforms \cite[p.~11]{Pekonen2013}.
The steeper spectral tilt for triangle waveforms can be explained by their construction from pulse waves via integration which corresponds to the application of a first-order lowpass filter.
Thus, a trivial triangle oscillator implementation can be sufficient if implemented with two or more times oversampling depending on the amount of tolerable aliasing, especially for devices with very limited processing resources.

## Quasi-Bandlimited Waveform Synthesis

Quasi-bandlimited oscillator algorithms allow a certain degree of aliasing to be produced while making use of *psychoacoustic* effects like *masking*.
Auditory masking means how sensitivity for one sound is affected by the presence of another sound which is largely dependend on the intensity and spectrum of the sound that causes the masking \cite[p.~187]{Gelfand2010}, i.e. the human ear cannot differentiate between two sounds with roughly the same frequency spectrum if the intensity difference is large enough.
Accordingly, the harmonics of a waveform can mask the aliasing components in their spectral vicinity. 
The intensity of aliasing must be particularly reduced in the range of $\SI{1}{\kilo\hertz}$ to $\SI{5}{\kilo\hertz}$, because this is where human ears are most sensitive \cite[fig.~11.1]{Gelfand2010}.

### BLITs

In \citeyear{Stilson1996a} \citeauthor{Stilson1996a} presented in their paper \citetitle{Stilson1996a} \cite{Stilson1996a} a method for synthesizing alias free geometric waveforms by integrating a *bandlimited impulse train* (BLIT).
Sawtooth, pulse and, of course, triangle waveforms can be derived from a pulse train by integration which is inherently a bandlimited operation.
Hence, it is sufficient to show how bandlimited impulse trains can be constructed by this method.
The naive way of discretizing an impulse train is by approximating each impulse with a unit-sample pulse.
The impulse trains period $p = f_s/f$ is rarely an integer, thus the locations of the unit-sample pulses must be approximated to the nearest sample instant.
\Cref{fig:impulse-train-approximation} (b) clearly shows the irregular intervals between unit-sample pulses the *pitch-period jitter* which adds noise to the signal \cite[p.~2]{Stilson1996a}.

\begin{figure}
\includegraphics[width=\textwidth]{imgs/box-train-plot.pdf}
\caption{(a) Impulse train $\bullet$ with frequency $f=\SI{8.3}{\hertz}$ and sample positions $\triangle$. (b) The approximated unit-sample pulse train with sample positions $p=f_s/f$ rounded to the nearest integer.}
\label{fig:impulse-train-approximation}
\end{figure}

The naive discretization approach suffers from aliasing just as the trivial waveform generation method (see \cref{sec:trivial-waveform-generation}) because it is also not-bandlimited.
Hence, a more sophisticated method is needed.
<!-- TODO: two times the -->
The idea is to apply an ideal anti-aliasing filter before sampling the impulse train.
\Cref{fig:ideal-filter-response} shows the frequency response of an ideal anti-aliasing filter is a rectangle function in the frequency interval ($-f_s/2, f_s/2$) and a continuous-time impulse response that is a $\sinc$ function:

\begin{equation}
h(t) = sinc(f_s t) = \frac{\sin(\pi f_s t)}{\pi f_s t}
\label{eq:sinc}
\end{equation}

\begin{figure}
\includegraphics[width=\textwidth]{imgs/ideal-filter-plot.pdf}
\caption{(a) Impulse response and (b) frequency response of an ideal anti-aliasing (lowpass) filter where $f_{Ny}=f_s/2$ is the frequency of the Nyquist limit.}
\label{fig:ideal-filter-response}
\end{figure}

Applying the ideal filter $h(t)$ to the unit-amplitude impulse train of period $T_1$

\begin{equation}
x(t) = \sum^\infty_{k=-\infty} \delta(t=k T_1)
\label{eq:unit-impulse-train}
\end{equation}

by means of convolution gives a bandlimited signal $x_f(t) = (x \ast h)(t)$.
Thus, $x_f$ can now be sampled without aliasing which gives

\begin{equation}
y[n] = x_f(n T_s) = \sum^\infty_{k=-\infty} \sinc(n + k p)
\label{eq:blit}
\end{equation}

where $T_s = 1 / f_s$ is the sample period and $p=f_s/f$ as defined above.
The bandlimited discrete-time signal $y[n]$ can be interpreted as time-aliased $\sinc$ functions \cite[p.~5]{Stilson1996a}, i.e. every impulse is replaced with $\sinc$ response of the ideal filter.
Furthermore, \citeauthor{Stilson1996a} provided a closed-form expression for the *sampled bandlimited impulse train*:

\begin{equation}
y[n] = \frac{M}{P} \sinc_M\left(\frac{M}{P}n\right)
\label{eq:blit-closed-form}
\end{equation}

where

\begin{equation}
\sinc_M(x) = \frac{\sin(\pi x)}{M\sin\left(\pi x/M\right)}
\end{equation}

and $M$ is the number harmonics.
It is convenient to relate the number of harmonics $M$ to the period in samples $p$ as

\begin{equation}
M = 2 \lfloor P/2 \rfloor + 1
\label{eq:blit-harmonics-to-period}
\end{equation}

that is the largest odd integer smaller than the period \cite[p.~6]{Stilson1996a}.
However, the BLIT method cannot be implemented as is because the $\sinc$ function is infinitely long.

### BLIT-SWS

A realizable approximation that was proposed in the original paper by \citeauthor{Stilson1996a} is called *Sum of Windowed Sincs* (SWS) and will be discussed in more detail.
Since then, a lot more methods were developed but it is not in the scope of this thesis to give an overview about all of them.
For a very thorough analysis of alternative methods refer to  \cite{Otalvar2015}.

The difference between this realizable approach and the theoretical BLIT method in the previous section is that a window is applied to the ideal filter's impulse response to make it finite.
Hence, \cref{eq:blit} becomes

\begin{equation}
y_w[n] = \sum^\infty_{k=-\infty} w(n)\sinc(n + k p)
\label{eq:blit-sws}
\end{equation}

where $w(n)$ is a window function.
The choice of the window function determines the attenuation of harmonics in the spectrum.
This is advantageous for frequency sweeps in contrast to exactly bandlimited methods where harmonics pop in and out which can cause unwanted transients.
Aliasing is reduced by increasing the window length, in fact, a doubling in length approximately halves the transition band where most of the aliasing occurs.
\citeauthor{Stilson1996a} used a blackman window that spanned 32 zero crossings\footnote{The $\sinc$ function has zeros at $n*\pi, n \in \mathbb{Z}$} of the $\sinc$ function \cite[fig.~10]{Stilson1996a} which attenuated aliasing to about $\SI{-90}{\decibel}$ for 80% of the spectrum.
The paper also proposed to use some oversampling to get a guard band in which the transition band can be moved by using a appropriate window length, e.g. for a sampling rate of $\SI{48}{\kilo\hertz}$ and a window length of 64 $\sinc$ zero crossings the transition band would span 10% of the spectrum which gives a nearly alias-free frequency range up to $0.9 \cdot\SI{24}{\kilo\hertz} = \SI{21.6}{\kilo\hertz}$.

A disadvantage of BLIT-SWS is that the CPU consumption is proportional to the frequency because for each period a impulse must be inserted and replaced by the impulse response of the windowed filter ($\sinc$).
Furthermore, the $\sinc$ is centered on the impulse and must be mixed in several samples before the actual impulse arrives, thus lookahead is required \cite[p.~20]{Huovilainen2010}.
By controlling the window length a tradeoff between CPU usage and quality can be achieved.

### BLEPs

- bandlimited step
- \citeauthor{Brandt2001} \cite{Brandt2001}
- minimum-phase impulse and pre-integrating it -> minimum-phase bandlimited step
- method can be used to produce bandlimited version of any waveform with discontinuities as long as all derivatives of the waveform are continuous
- integration step of BLIT can be skipped -> reduces numerical error as leaky integrators are no longer needed
- BLEP pulse can simply be mixed with the non-bandlimited waveform any time there is a discontinuity
- integration before sampling decreases aliasing by another 6dB per octave bandlimitation compared to BLIT-SWS
- CPU cost still proportional to frequency and a division for each period required

## Ideally Bandlimited Waveform Synthesis

### Wavetables

- one cycle of a periodic waveform
- pitch: read wavetable at different speeds (index increments)
- increment = (length * frequency)/sampling_frequency
- zero-degree polynomial floor (round), linear interpolation (first-degree polynomial)
     - $y_\text{Lin}(x) = y[x_0] + \left(y[x_0 + 1]-y[x_0]\right)\cdot(x-x_0)$
     - two error components: change in amplitude (barely perceptible) and distortion of waveshape (very perceptible)

\begin{figure}
\includegraphics[width=\textwidth]{imgs/table-lookup.pdf}
\caption{Wavetable for a single period of a sine wave sampled at 24 equidistant points. The bottom x-axis shows the index of the sample in the wavetable and the top shows angle in radians.}
\label{fig:table-lookup}
\end{figure}

\begin{figure}
\includegraphics[width=.9\textwidth]{imgs/saw-table.pdf}
\caption{Harmonic spectra and time domain representation for a sawtooth waveform}
\label{fig:saw-table}
\end{figure}

\begin{figure}
\includegraphics[width=.9\textwidth]{imgs/sqr-table.pdf}
\caption{Harmonic spectra and time domain representation for a square waveform}
\label{fig:sqr-table}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{imgs/interpolation-error.pdf}
\caption{Error of zero-degree (floor) and first-degree (linear) interpolation for a wavetable of 64 samples}
\label{fig:interpolation-error}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{imgs/harmonic-spectrum.pdf}
\includegraphics[width=\textwidth]{imgs/inharmonic-spectrum.pdf}
\caption{Spectrum of a signal with only harmonic content (a) and solely inharmonic content (b)}
\label{fig:spectra}
\end{figure}

## Polyphony

## Latency
<!-- TODO: unsure where to put this chapter -->

The *responsiveness* of an electronic musical instrument is mainly determined by its latency.

\theoremstyle{definition}
\begin{definition}
\label{def:latency}
Latency is the time delay between two causally connected events.
\end{definition}

An instrument is the more responsive, the less latency between an input event and the corresponding sound output it has.
The latency is imperceptible for the user if the delay between the input event and audio output stimuli is $< 24ms$ \cite{NasaLatency}.
Furthermore, there is an even stronger latency limit of $\approx 2$ms, that is the *temporal resolution* of the human hearing, as shown by \cite[p.\~294]{FastlZwicker} using psycho acoustic measurements.
However, it is not realistic to use this as an upper limit for the synthesizer's system latency, considering that the sound propagation delay from a speaker to a listener at a speed of sound $v = 343.2 m/s$ and a common listening distance of $d = 2m$ is $\approx$ 3 times larger than the temporal resolution: $d/v = 2m/343.2\frac{m}{s} = 5.82ms$.
Therefore, achieving a system latency of less than $24ms$ is favorable.

Latency (see \nameref{latency}) is the time difference between an input action and the corresponding output from a system, in this case the synthesizer.

- reduce latency with custom kernel (linux-ck)

\begin{definition}{Latency} is a time interval between the stimulation and response, or, from a more general point of view, a time delay between the cause and the effect of some physical change in the system being observed
\end{definition}

> For any audio system, may it be, for example, an analogue electronic circuit, a digital circuit, a whole computer or a physical wave-guide, there will be some time lag between the instant at which the signal enters the audio system and the one at which the signal exits.
> For a lot of reasons:
> - finite propagation speed of sound waves
> - AD/DA conversion times

- temporal resolution of human hearing is $\approx 2\,ms$ \cite{FastlZwicker}
- only control over some aspects, buffer sizes of the application or the sound backend
- user of the application should not be able to hear any possible delay between his input action/event and the generated output signal

Spectral artifacts can occur for very low latencies and only when the signal is monitored (comb filter).
Can be ignored because there is no source signal that is monitored.
Temporal issues are to be concerned.
\cite{AES:Latency}

The propagation delay, i.e. time $t$ it takes for a wave of sound to travel from the speaker to the ear of the listener, over a distance of $d = 1.5\,m$, and by a speed of sound of $v = 343.2\,m/s$ at normal temperature [^stdTemp] can be calculated like this:

[^stdTemp]: The *normal temperature* is defined by the National Institute of Standards and Technology (NIST) as $20°C$ at $1\,atm$ absolute pressure.

\begin{align}
t &= \frac{d}{v}\cdot 1\,s\\
  &= \frac{1.5\,m}{343.2\,m}\cdot 1\,s\\
  &= 0.00437\,s\\
  &= 4.37\,ms
\end{align}

The overall latency of the synthesizer is the sum of different latency portions, introduced through a variety of components in the audio output chain.
At first there is the input latency, that is either $l_{midi}$ or $l_{osc}$ depending on the used input protocol, where the amount of $l_{osc}$ depends highly on the network connection.
Usually a wifi connection is used to connect an OSC client, like a tablet running [liine's lemur](https://liine.net/de/products/lemur/) or some other OSC capable controller application.
An ad-hoc wifi network or tethering over USB can be used to get reliable network latencies with wireless clients.

The next latency portion, $l_{dsp}$ is introduced through the synthesizers internal audio output buffer.
At the time of writing, a small buffer size of [64 samples](https://github.com/klingtnet/ytterbium/blob/master/src/main.rs#L172), or $\approx 1.3\,ms$, was used.

The audio backends output buffer adds a portion $t_{backend}$ of latency, as well.
It depends on the users buffer size settings how large the added latency is.

At last there is the propagation delay which can be neglected for usual listening distances or is near zero if headphones are used.

\resizebox{\textwidth}{!}{
\begin{tikzpicture}[auto]
\draw
	node [block](Engine){DSP Engine}
	node [block, above left=of Engine, node distance=3cm](MIDI){MIDI}
	node [block, below left=of Engine, node distance=3cm](OSC){OSC}
	node [block, right=of Engine, node distance=4cm](Soundcard){Audio Backend}
	node [block, right=of Soundcard, node distance=5cm](Speaker){Speaker}
	node [block, right=of Speaker, node distance=4cm](Listener){Listener}
;
	\draw[->](MIDI) -- node{$l_{midi}$}(Engine);
	\draw[->](OSC) -- node{$l_{osc}$}(Engine);
	\draw[->](Engine) -- node{$l_{dsp}$}(Soundcard);
	\draw[->](Soundcard) -- node{$l_{backend}$}(Speaker);
	\draw[->](Speaker) -- node{$l_{wave}$}(Listener);
\end{tikzpicture}}

# Implementation Details

\begin{figure}
	\centering
    \resizebox{\textwidth}{!}{
	\begin{tikzpicture}
[
    auto,
    >=latex,
    node distance = 1cm
]
\node[align=center](control){Control\\Input};
\node[matrix, row sep=5mm, right=of control](voices){
    \node[draw](voice1){
        $\text{voice}_1$
        \begin{tikzpicture}
            \node[block](osc){$∿$};
            \node[left=of osc, yshift=4mm](freq){freq};
            \node[draw, left=of osc, yshift=-4mm](adsr){envelope};
            \draw[->](freq)--([yshift=4mm]osc.west);
            \draw[->](adsr)--node[above, midway]{A}([yshift=-4mm]osc.west);
        \end{tikzpicture}
    };\\
    \node[draw](voice2){
        $\text{voice}_2$
        \begin{tikzpicture}
            \node[block](osc){$∿$};
            \node[left=of osc, yshift=4mm](freq){freq};
            \node[draw, left=of osc, yshift=-4mm](adsr){envelope};
            \draw[->](freq)--([yshift=4mm]osc.west);
            \draw[->](adsr)--node[above, midway]{A}([yshift=-4mm]osc.west);
        \end{tikzpicture}
    };\\
    \node[](voiceX){\vdots};\\
    \node[draw](voiceN1){
        $\text{voice}_{n-1}$
        \begin{tikzpicture}
            \node[block](osc){$∿$};
            \node[left=of osc, yshift=4mm](freq){freq};
            \node[draw, left=of osc, yshift=-4mm](adsr){envelope};
            \draw[->](freq)--([yshift=4mm]osc.west);
            \draw[->](adsr)--node[above, midway]{A}([yshift=-4mm]osc.west);
        \end{tikzpicture}
    };\\
    \node[draw](voiceN){
        $\text{voice}_n$
        \begin{tikzpicture}
            \node[block](osc){$∿$};
            \node[left=of osc, yshift=4mm](freq){freq};
            \node[draw, left=of osc, yshift=-4mm](adsr){envelope};
            \draw[->](freq)--([yshift=4mm]osc.west);
            \draw[->](adsr)--node[above, midway]{A}([yshift=-4mm]osc.west);
        \end{tikzpicture}
    };\\
};
\draw[->](control.north)|-(voice1.west);
\draw[->](control.north)|-(voice2.west);
\draw[->](control.south)|-(voiceN1.west);
\draw[->](control.south)|-(voiceN.west);
\node[sum, right=of voices](sum){$+$};
\draw[->](voice1.east)-|(sum.north);
\draw[->](voice2.east)-|(sum.north);
\draw[->](voiceN1.east)-|(sum.south);
\draw[->](voiceN.east)-|(sum.south);
\node[block, right=of sum, align=center](filter){Multi mode\\Filter};
\draw[->](sum)--(filter);
\node[right=of filter,align=center](out){Output\\Device};
\draw[->](filter)--(out);
	\end{tikzpicture}}
	\caption{Building blocks/structure of a basic polyphonic subtractive synthesizer.}
	\label{fig:basic-synth-structure}
\end{figure}


- real-time process one for which the computing time associated with each sampling interval can be completed in a time less than or equal to the sampling interval \cite{Rabiner1972}

\begin{figure}
\includegraphics[width=.8\textwidth,angle=180]{imgs/ytterbium-spectrum-legend.png}
\includegraphics[width=.9\textheight,angle=90,origin=c]{imgs/ytterbium-0.1.0-Saw-sweep.png}
\includegraphics[width=.9\textheight,angle=90,origin=c]{imgs/ytterbium-0.1.0-Square-sweep.png}
\caption{Frequency sweep from 20Hz to 20kHz in the sawtooth and square (below wavetable}
\label{fig:sweep}
\end{figure}

![Sweep using sine modulator and carrier with increasing modulation amount](imgs/ytterbium-0.1.0-fm.png){ width=100% }

## Ring Buffer

- throughput-rate the total rate at which digital information is processed by a discrete-time system, measured in samples per second \cite{Rabiner1972}

## Control Input

### Lemur

![View of the piano section](imgs/lemur-piano.png){ width=100% }

![View of the oscillator contol section](imgs/lemur-oscillator.png){ width=100% }

![View of the fm section](imgs/lemur-fm.png){ width=100% }

![View of the mixer section](imgs/lemur-mix.png){ width=100% }

![View of the mixer section](imgs/lemur-filter.png){ width=100% }

\cite{Bristow:Cookbook}

## Audio Output

# Outlook
<!-- TODO: rename this to future work? -->

## Optimizations

- LFOs
- modulation for oscillator pitch via envelopes
- general amount of modulation sources

# Conclusion

