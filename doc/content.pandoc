# Introduction

## Scope of this Thesis

It's not required for the reader to have any prior knowledge of soft- and hardware tools used in music production environments.
However, a basic knowledge of signal processing and some familiarity with computer programming is beneficial to grasp the presented concepts.

## Objectives

<!-- TODO
Be more specifif
-->

The objective of this thesis is to show how to build a realtime audio software application at the example of an audio synthesizer with frequency modulated oscillators.

The amount of problems that arise when building such an application is quite large.
This includes timing and synchronization problems that occur because the computations made are required to be finished in short time windows.
Computations must be as fast as possible to ensure a responsive feedback for the player of the instrument, and are needed to avoid sound distortion when the generated signals could not be delivered fast enough to the sound card buffer.

Not every programming language is equally suitable for the development of realtime applications, thus it is explained why Rust was chosen as the implementation language.
Libraries for the MIDI and Open Sound Control protocol were implemented and released as open-source software.
Both protocols are supported by the application and are used to polyphonically play notes or remote control all parameters of the synthesizer.
Common synthesizer architectures and signal generation methods are discussed and evaluated.

Furthermore, sound shaping techniques and effects are presented in detail.
The precautions taken to reduce the amount of signal distortion are shown as well.
Lastly, an outlook on possible optimizations and enhancements, as well as a summary of the achieved results is given.

## Overview

<!-- TODO: provide a quick summary of each chapter. -->

Chapter 2 gives an overview about common synthesis concepts and evaluates why Rust was choosen as the implementation language.

## Why Rust?

Realtime audio applications must finish their signal processing in tight time bounds to ensure that the sound card can output a continious audio stream.
Missing such a bound will result in unpleasant sound glitches, and---in the worst case---renders a whole song recording useless.
Additionally, the time bounds must be as tight as possible to reduce the latency between the user input (playing a note) and the output of the calculated signal in the speaker.
Therefore, (memory-)managed languages like [Java](https://en.wikipedia.org/wiki/Java_(programming_language)), [Go](https://golang.org) or [C#](https://en.wikipedia.org/wiki/C_Sharp_(programming_language)) that use a garbage collector, which can produce undeterministic program stops while examining the state of variable references, will not receive any further consideration.
Interpreted languages like [Python](https://www.python.org/) or [Ruby](https://www.ruby-lang.org/en/) face the same problems as compiled managed languages by also having increased runtime costs.
Audio application development in those languages is still possible but commonly requires to write the signal processing as C modules and interface with them through the languages foreign function interface (FFI) (e.g. pyo \cite{pyo} uses this approach). 
Unmanaged languages like [C](https://en.wikipedia.org/wiki/C_(programming_language)) and [C++](https://en.wikipedia.org/wiki/C%2B%2B) offer the control over memory that is needed to make reliable claims about the runtime behaviour while avoiding the overhead of an additional runtime environment.
This comes with a downside in memory safety and introduces a whole new class of possible bugs compared to managed languages.
Those bugs are very likely to cause *undefined behaviour* or to crash the program. Unfortunately, they are also very hard to debug.

One of the major selling points of [Rust](https://rust-lang.org) is guaranteed memory safety *without* garbage collection and *data race freedom*while providing the same level of control over memory as C/C++.
These goals are achieved through a variety of concepts like ownership, lifetimes and borrowing to know at compile when memory can be safely freed, and to enforce that there is only a single mutable access to any variable at any given time in the run of the program.
Explaining these concepts is outside the scope of this thesis and the official [Rust Book](https://doc.rust-lang.org/book/ownership.html) \cite{RustBook} does a great job doing this in detail, hence, this is left as an exercise for the interested reader.
The [Max Planck Institute for Software Systems](http://plv.mpi-sws.org/) has started the \cite{RustBelt} RustBelt research project in 2015 to develop formal foundations for the Rust programming language.
One of the main goals of the research group is to formally investigate if the claims made about data-race freedom, memory and type safety actually hold.
<!-- Furthermore, they want to provide formal tools for verifying safe encapsulation of `unsafe` code blocks. -->
As of the time of writing there is no evidence that the claims are untrue, therefore it's assumed that they're actually hold.

In the following I will describe a number of language features that were crucial in making the decision to use Rust for the implementation of the synthesizer:  

Generics (parametric polymorphism)
:	Generics allow to write a single implementation of a function or method so that it can be used with a number of different types.
	A *generic* type in the declaration is replaced by an *type parameter*.
	Such a parameter can also be constrained so it can only be replaced with types that implement certain traits.

	[Example](https://is.gd/Ob3eqr):
	```rust
	fn sum<T>(a: T, b: T) -> T::Output {
		a + b
	}

	println!(sum(3+4));
	```

Cross compilation
:	Cross compilation is the ability to compile statically linked binaries for different target platforms on a single machine, e.g. one can compile a windows executable on a linux machine.

Abstractions without overhead
:	The Rust compiler will create a separate implementation for each type that a generic function is used with in the code.
	This is called *monomorphization* and allows to statically dispatch calls to generic functions but comes with the cost of slightly increased binary sizes.
	Iterators and other high-level features will be compiled down to simpler constructs like *for* loops.

FFI
:	A foreign function interface allows the calling of functions written in other languages.
	Rust's FFI allows to easily call functions from a variety of languages but, as a systems language, it focuses on C as the default target.
	This is of great value because most audio APIs and I/O libraries, e.g. [portaudio](http://www.portaudio.com/) or [libsoundio](http://libsound.io/), are C libraries.
	The opposite directions is possible as well, because a Rust function can be defined to use the C ABI (application binary interface) so it can be called from C or other languages that provide a C FFI.

Algebraic data types and pattern matching
:	An algebraic data type is formed by combination of other types.
	The term *algebraic* refers to the operations that are used to form the type, which is either *sum*, i.e., the type is $A$ or $B$, or *product*, i.e., it is a combination of $A$ and $B$, where $A$ and $B$ are different types.
	Rust's `enum`s are formed as *sum* types whereas `tuple`s are formed by a *product* of types.
	A classic example for the former kind is the `Option` type, which can be either `None` or have `Some` value of type $T$:

	```rust
	enum Option<T> {
	    None,
		Some(T),
	}
	```

	Destructuring the values of such types is done through *pattern matching* in Rust.
	In contrast to C-like enums, the compiler knows how much variants such an algebraic data type has and won't compile if they aren't matched exhaustively, i.e. there is at least one unmatched variant of the type.

Iterators
:	Rust provides conventional `for` loops but additionally has *iterators*.
	An iterator is something that provides a sequence of things where you can loop over.
	The great benefit of using iterators over index based access in traditional `for` loops is, that it avoids the, often unintended, access of elements that are outside the iterators range.
	In addition, iterators provide a vast amount of methods to combine them, filter elements, perform map-reduce like operations and in many cases even parallelize the computation \cite{Rayon}.

	Example: $\sum_{x\,\in\,v_1, v_2\;\wedge\;x > 3}$

	```rust
	let v1 = [1, 2, 3];
	let v2 = [3, 4, 5];
	let sum = v1.iter().chain(v2.iter())
				.filter(|val| **val > 3)
				.fold(|acc, val| acc + val, 0);
	assert_eq!(sum, 9)
	```

Toolchain
:	Rust has a standard package manager and build tool called *Cargo*, that is typically used to manage the dependencies of a project but also to run tests and benchmarks.

# User Interface

Interaction with the software synthesizer is done through its *user interface*.
The user interface serves three senses, which are sight, touch and hearing.
This section concentrates on the first two, the *visual* and the *haptic* component.
However, a synthesizer can be played solely through haptic controls and audio feedback.
Visual indicators for the synthesizer's parameters, e.g. through a display, are convenient, but not necessary for the playing musician.

The application supports the two most common musical control signal protocols, *MIDI* and *Open Sound Control* (OSC).
Adding MIDI support is highly beneficial, because it enables the synthesizer to be played with any---of the vast amount of available---MIDI hardware controllers (\cref{midi:edirol} shows an example of such a device).
On the other hand, Open Sound Control software like [liine's Lemur](https://liine.net/de/products/lemur/) \cite{LiineLemur} provides an editor to create or customize a software defined controller for a multi-touch device like a smartphone or tablet.

![Edirol PCR-300 MIDI controller keyboard\label{midi:edirol}](imgs/pcr_300_angle.jpg){ width=100% }

<!-- TODO
- OSC: custom controllers, more modern approach
- User interface must work in realtime (latency)

MIDI support is a basic requirement for nearly all types of music hard- and software because it allows to use controller hardware
User input, if it is a played note or a parameter change, must be processed in real-time to give the player direct feedback without perceived *latency* (see \nameref{latency}).
-->

## MIDI

The Musical Instrument Digital Interface (MIDI) specification stipulates a hardware interconnection scheme and a method for data communication \cite[p.~972]{Roads:CMT}, but only the protocol specification is of interest for this work.
Most modern MIDI hardware is connected via USB anyway.
The *MIDI 1.0 Specification* \cite{MIDI10} provides a high level description of the MIDI protocol:

> The Musical Instrument Digital Interface (MIDI) protocol provides a standardized and efficient means of conveying musical performance information as electronic data.
> MIDI information is transmitted in \enquote{MIDI messages}, which can be thought of as instructions which tell a music synthesizer how to play a piece of music. \cite[p.~1]{MIDI10}

Transmitting *control data* is the purpose of the MIDI protocol, and not, like it is sometimes confused, to transmit audio data[^midi-audio].
Control data can be thought of as the press of a key, turning a knob, or an instruction to change the clock speed of a song.

[^midi-audio]: It is possible to transmit audio data over MIDI by using *System Exclusive* (SysEx) messages, but this can not be done in real-time and is often used to replace or update samples or wavetables in hardware synthesizers.

The work on the MIDI specification began in 1981 by a consortium of Japanese and American synthesizer manufacturers, the MIDI Manufacturers Association (MMA).
In August 1983, the version 1.0 was published \cite[p.~974]{Roads:CMT}.
This year, 2016, the MMA established The MIDI Association (TMA).
The TMA should support the global community of MIDI users and establish [midi.org](https://www.midi.org/) \cite{MidiOrg} as a central source for information about MIDI.
MIDI is used in nearly every music electronic device, like synthesizers, samplers, digital audio effects, and music software, due to its simple protocol structure and long time of existence.

### MIDI Protocol

The MIDI protocol specifies a standard transmission rate of 31.250 baud.
This may seem like an unusual choice for the transmission rate, but it was derived by dividing the common clock frequency of 1MHz by 32 \cite[p.~976]{Roads:CMT}.
It uses an 8b/10b encoding, i.e. 8 bits of data are transmitted as a 10 bit word.
A data byte is enclosed by a start- and stop bit which in turn results in the 10 bit encoding.
Asynchronous serial communication is used to transfer *MIDI messages*, thus the start and stop bit.

A MIDI message is composed of a *status byte* which is followed by up to two[^sysex] *data byte*s.
Both types are differentiated by their most significant bit (MSB), `1` for status- and `0` for data bytes.
Consequently, the usable payload size is reduced to 7 bit, in other words, values can range from 0 to 127.

[^sysex]: System Exclusive (SysEx) messages can be made up of more than two data-bytes, in fact they are build by a sequence of data bytes followed by an *End of Exclusive* (EOX) message to mark the end of the stream. This type of message does not contain any musical control data, in general it is used to upload binary data, like firmware updates or samples, to a MIDI device.

\Cref{fig:midi-status} shows the structure of a status byte.
The message type is denoted by three bits (`T`) and the remaining four bits are used to denote the channel number (`C`), hence there are sixteen different channels.
MIDI channels allow to route different logical streams over one physical MIDI connection, e.g. to reach a different, daisy-chained MIDI device or to control different timbres of a multitimbral synthesizer.

\begin{figure}
    \centering
	$0\quad\underbrace{T\quad{}T\quad{}T}_{\text{message type}}\quad\overbrace{C\quad{}C\quad{}C\quad{}C}^{\text{channel number}}$
	\caption{Structure of a MIDI status byte.}
	\label{fig:midi-status}
\end{figure}

MIDI messages are divided in two categories, *channel* and *system* messages.
Only the latter contain musical control information and therefore are of interest for this thesis.
\Cref{fig:midi-classification} illustrates the classification, status byte values are shown as edge labels where \texttt{x} illustrates *don't care*.
*Channel Mode Messages* define the instrument's response to Voice Messages \cite[p.~36]{MIDI10}, i.e. listen on all channels (omni mode), or switch between mono- and polyphonic mode (multiple simultaenous voices).

\begin{figure}
	\centering
	\begin{tikzpicture}
[
	grow = right,
	every node/.style = {font=\footnotesize},
	sibling distance = 1cm,
	level distance = 1cm,
	level 1/.style = {sibling distance = 5cm, level distance = 3cm},
	level 2/.style = {sibling distance = 2cm, level distance = 5cm},
	sloped
]
\node[msg]{MIDI Message}
	child { node [msg]{System Message}
		child { node [msg]{System Exclusive\\Message}
			edge from parent node [above] {\texttt{F0}}
		}
		child { node [msg]{System Common\\Message}
			edge from parent node [above] {\texttt{F1-F7}}
		}
		child { node [msg]{System Real-Time\\Message}
			edge from parent node [above] {\texttt{F8-FF}}
		}
		edge from parent node [above] {\texttt{F0-FF}}
	}
	child { node [msg]{Channel Message}
		child { node [msg]{Channel Voice\\Message}
			edge from parent node [above] {\texttt{8x-Ex}}
		}
		child { node [msg]{Channel Mode\\Message}
			edge from parent node [above] {\texttt{Bx}}
			edge from parent node [below] {\texttt{Data1: 79-7F}}
		}
		edge from parent node [above] {\texttt{8x-Ex}}
	}
	;
	\end{tikzpicture}
	\caption{Classification of MIDI messages.}
	\label{fig:midi-classification}
\end{figure}

### MIDI Pitch

\begin{table}[]
    \centering
    \caption{Types of MIDI Voice Messages.}
    \label{tab:voice-messages}
    \begin{tabular}{@{}llllp{3.5cm}@{}}
    \toprule
    Type                    & Status & Data1      & Data2    & Description                                                \\ \midrule
    Note-Off                & 8x     & Key \#     & Velocity & Key released.                                              \\
    Note-On                 & 9x     & Key \#     & Velocity & Key press from a triggering device.                        \\
    Polyphonic Key Pressure & Ax     & Key \#     & Pressure & Aftertouch event.                                          \\
    Control Change          & Bx     & Ctrl. \#	  & Value    & Move of a controller other than a key (e.g. Knob, Slider). \\
    Program Change          & Cx     & Program \# & ---      & Instruction to load specified preset.                      \\
    Channel Pressure        & Dx     & Pressure   & ---      & Aftertouch event.                                          \\
    Pitch Bend              & Ex     & MSB        & LSB      & Altering pitch (14-bit resolution).                        \\ \bottomrule
    \end{tabular}
\end{table}

\Cref{tab:voice-messages} gives an overview of the types on voice messages.
Corresponding Note-On and Off messages do not necessarily follow one after another, therefore, to relate associated messages, pitch information is contained in the Note-Off as well.
Pitch is encoded as a 7-bit value in note messages, hence there is a range of 128 pitches or about 10 octaves.
MIDI's pitch representation was designed with an *chromatic western music scale* in mind.
A *chromatic scale* has 12 pitches per octave with one semitone difference between each pitch.
An interval of one octave is equivalent to a doubling or halving (in the negative case) in frequency.
Instruments in *western music* are usually *equal-tempered*, i.e. all semitones have the same size.
MIDI pitches are considered to be equal-tempered and range from C0 (*c* in the lowest octave) to a G10 (*g* in the 10th octave).
Middle C, pitch number 60 (C5), is used as reference.

\begin{equation}
	\label{eq:midi-pitch}
	f = f_\text{tune}\cdot 2^{\displaystyle\left(p-p_\text{ref}\right)/12}
\end{equation}

\Cref{eq:midi-pitch} shows how to calculate the frequency $f$ for a given MIDI pitch where $f_\text{tune}$ is the tuning frequency and $p_\text{ref}$ is the reference pitch number.
Musical instruments are commonly tuned to the *Concert A*, the note A above middle C or MIDI pitch 69.
The default tuning of Concert A is 440 Hz \cite{ISO16:1975}.
The following example shows how to calculate the frequency for middle C by using the *Concert A* tuned to 440 Hz as reference pitch in \cref{eq:midi-pitch}:

$$
\begin{aligned}
f	&= 440\,\text{Hz}\cdot 2^{(60-69)/12}\\
	&= 440\,\text{Hz}\cdot 2^{-9/12}\\
	&\approx 261.626\,\text{Hz}
\end{aligned}
$$

### Timing Problems

Playing two or more notes a the same time, i.e. playing a chord, can lead to timing problems because of MIDI's low bandwidth.

$$
\begin{aligned}
t_\text{Note-On}	&= 3\cdot\left(31250\,\frac{\text{bit}}{\text{s}}/10\,{\text{bit}}\right)^{-1}\\
					&= 0.0096\,\text{s} = 0.96\,\text{ms}
\end{aligned}
$$

The time to transmit a single note-on event $t_\text{Note-On}$ takes $\approx 1\,\text{ms}$, this means that the last transmitted note of an $n$-key chord arrives with $n \cdot 0.96\,\text{ms}$ delay, e.g. the last note of a pentachord (5 keys) will be received $5 \cdot 0.96\,\text{ms} = 4.8\,\text{ms}$ later than the first one.
This may result in a *comb filter*[^comb-filter] like distortion of the synthesized chord sound.

[^comb-filter]: A comb filter adds a delayed copy of the signal to itself causing addition or subtraction in the signal. The filters frequency response shows regularly spaced notches, might resemble the shape of a comb.

## Open Sound Control

The *UC Berkeley Center for New Music and Audio Technology* (CNMAT) originally developed, and continues to research, Open Sound Control.
In 2002, OSC's 1.0 specification was released.
It provides the following definition \cite{OSC:10}:

> Open Sound Control (OSC) is an open, transport-independent, message-based protocol developed for communication among computers, sound synthesizers, and other multimedia devices.

The protocol is not limited to being used with audio or multimedia devices, however, it is often used as a high-speed network replacement for MIDI.
Referring to OSC as a *message format* is more accurate, since error-handling, synchronization or negotiation methods are not specified.
Therefore, OSC can be compared to formats like JSON or XML.
A draft of the OSC 1.1 specification was published in a 2009 paper \cite{OSC:11} only adding minor, backward compatible changes.
UDP is often used as the transport layer to avoid the time required to establish a connection by TCP's three-way handshake.
A connectionless transport is sufficient because OSC sender and receiver are almost always in physical proximity and connected through the same LAN.

### OSC Data Types

\begin{table}[]
    \centering
    \caption{Overview of OSC 1.0 and 1.1 data types.}
    \label{tab:osc-data-types}
    \begin{tabular}{@{}cp{7.5cm}cc@{}}
    \toprule
    Tag     & Description                                                      & 1.0 Required & 1.1 Required \\ \midrule
    i       & 32-bit two's complement integer                                  & ✔            & ✔            \\
    f       & IEEE 754 single precision (32-bit)                               & ✔            & ✔            \\
    s       & null-terminated sequence of ASCII characters                     & ✔            & ✔            \\
    b       & binary blob with size information                                & ✔            & ✔            \\
    h       & 64-bit big-endian two's complement integer                       &              &              \\
    t       & OSC-timetag in NTP format                                        &              & ✔            \\
    d       & IEEE 754 double precision (64-bit)                               &              &              \\
    S       & alternate string type                                            &              &              \\
    c       & ASCII character                                                  &              &              \\
    r       & RGBA color (8-bit per channel)                                   &              &              \\
    m       & 4 byte MIDI message. From MSB to LSB: port, status, data1, data2 &              &              \\
    T/F     & true, false boolean values                                       &              & ✔            \\
    N       & Nil                                                              &              & ✔            \\
    I       & Infinitum (1.0)/Impulse(1.1) used as event trigger               &              & ✔            \\
    {[},{]} & Array delimiters                                                 &              &              \\ \bottomrule
    \end{tabular}
\end{table}

An overview of the predefined data types for both, OSC 1.0 and 1.1, is shown in \cref{tab:osc-data-types}.
The byte order of OSC's integer, float and timetags is big-endian.
OSC's unit of transmission is called *OSC Packet*.
The EBNF grammar for OSC packets is described by \cref{osc:grammar}.
Fields of an OSC packet have to be aligned to multiples of 4-byte and are zero-padded, thus the size of such a packet is also a multiple of four.
The packets contents can either be an *OSC Message* or *OSC Bundle*.
An OSC message starts with an *address pattern*  followed by zero or more *arguments* to be applied to the *OSC Method* matched by the pattern.
Address pattern can contain basic regular expression with single-/multi-character `?/*` wildcards, range `[A-Z]` and list matches `{foo, bar}`, hence multiple OSC Methods can be triggered with a single OSC Message.
An OSC Receiver's[^osc-naming] address space forms a tree structure with branch nodes called *OSC Containers* and leaves are named *OSC Methods*.
Methods are *`italicized`* in the tree structure example of \cref{fig:osc-address-space-example}.
The address of an OSC method starts with a `/`, followed by any container name along the path in order from the root of the tree, joined by forward slashes `/` and the method's name, e.g. `/oscillator/1/phase`.

[^osc-naming]:
	The term *OSC Receiver* and *OSC Server* is interchangeable.
	This also applies to *OSC Sender* and *OSC Client*.
	OSC applications often act as server and receiver, hence no clear distinction between those roles can be made.

\begin{figure}
	\centering
	\begin{tikzpicture}
[
	every node/.style = {font=\footnotesize\ttfamily},
	level 1/.style = {sibling distance = 5cm},
	level 2/.style = {sibling distance = 2cm},
	sloped
]
\node{/}
	child { node {oscillator/}
		child { node {1/}
			child { node [osc-method]{freq} }
			child { node [osc-method]{phase} }
		}
		child [sibling distance=20mm]{ node {2/}
			child { node {\ldots} }
		}
	}
	child { node {filter/}
		child { node [osc-method]{mode} }
		child { node [osc-method]{cutoff} }
		child { node [osc-method]{resonance} }
	}
	;
	\end{tikzpicture}
	\caption{OSC Address Space example.}
	\label{fig:osc-address-space-example}
\end{figure}

\begin{figure}
\caption{Grammar of an OSC packet described as EBNF (ISO14977 syntax \cite[p.~14]{ISO14977})}
\label{osc:grammar}
\begin{verbatim}
packet			= size, content ;
size			= (* 4-byte aligned packet content field length *) ;
content			= message | bundle ;
message			= address, ",", { type-tag }, { argument } ;
address			= "/", osc-string - ( "'" | "#" | "*" | "," | "/" |
									  "?" | "[" | "]" | "{" | "}" ) ;
osc-string		= { ASCII }, "0" ;
type-tag		= "i" | "f" | "s" | "b" | "h" | "t" | "d" | "S" |
				  "c" | "r" | "m" | "T" | "F" | "N" | "I" | 
				  "{", {type-tag}, "}" ;
argument		= (* binary representation of the argument *) ;
bundle			= "#bundle", OSC-timetag , { bundle-element } ;
bundle-element	= size, content ;
\end{verbatim}
\end{figure}

### Comparison to MIDI

Both protocols provide a number of benefits and limitations in comparison to each other.
The following list shows them for OSC compared to MIDI:

\begin{enumerate}
	\item[$+$] OSC's data-types allow a much higher resolution for control values.
	They also provide symbolic types like booleans or \emph{Nil} to represent an empty value.
	\item[$+$] The definition of \emph{custom data-types} is allowed, therefore OSC applications must be robusted against unknown ones.
	\item[$+$] The \emph{bandwidth} is orders of magnitudes larger than MIDI's, but it depends on the type of network used.
	A common choice are ad-hoc WiFi connections between OSC receiver and sender because the player (sender) and the instrument (receiver) are in local proximity to each other.
	This, in turn, results in an acceptable network \emph{latency} in the single digit millisecond range.
	\item[$+$] Control events can be send simultaenously as an OSC bundle, e.g. note events of a chord.
	\item[$+$] Events can be timed with an resolution of $\approx 200$ picoseconds \cite{OSC:10}.
	\item[$+$] OSC can be used to tunnel MIDI messages over a network connection.
	\item[$-$] There is no standard for discovering OSC devices in a network, thus addresses must be configured manually which is cumbersome.
	\item[$-$] Unlike MIDI, there is no standard namespace for interfacing with an OSC device, altough, a proposal for a standard exists \cite{synoscopy}.
	\item[$-$] The number of applications that support OSC is very limited.
\end{enumerate}

## Latency
<!-- TODO: unsure where to put this chapter -->

The *responsiveness* of an electronic musical instrument is mainly determined by its latency.

\theoremstyle{definition}
\begin{definition}
\label{def:latency}
Latency is the time delay between two causally connected events.
\end{definition}

An instrument is the more responsive, the less latency between an input event and the corresponding sound output it has.
The latency is imperceptible for the user if the delay between the input event and audio output stimuli is $< 24ms$ \cite{NasaLatency}.
Furthermore, there is an even stronger latency limit of $\approx 2$ms, that is the *temporal resolution* of the human hearing, as shown by \cite[p.\~294]{FastlZwicker} using psychoacoustic measurements.
However, it is not realistic to use this as an upper limit for the synthesizer's system latency, considering that the sound propagation delay from a speaker to a listener at a speed of sound $v = 343.2 m/s$ and a common listening distance of $d = 2m$ is $\approx$ 3 times larger than the temporal resolution: $d/v = 2m/343.2\frac{m}{s} = 5.82ms$.
Therefore, achieving a system latency of less than $24ms$ is favorable.

Latency (see \nameref{latency}) is the time difference between an input action and the corresponding output from a system, in this case the synthesizer.

- reduce latency with custom kernel (linux-ck)

\begin{definition}{Latency} is a time interval between the stimulation and response, or, from a more general point of view, a time delay between the cause and the effect of some physical change in the system being observed
\end{definition}

> For any audio system, may it be, for example, an analogue electronic circuit, a digital circuit, a whole computer or a physical wave-guide, there will be some time lag between the instant at which the signal enters the audio system and the one at which the signal exits.
> For a lot of reasons:
> - finite propagation speed of sound waves
> - AD/DA conversion times

- temporal resolution of human hearing is $\approx 2\,ms$ \cite{FastlZwicker}
- only control over some aspects, buffer sizes of the application or the sound backend
- user of the application should not be able to hear any possible delay between his input action/event and the generated output signal

Spectral artifacts can occur for very low latencies and only when the signal is monitored (comb filter).
Can be ignored because there is no source signal that is monitored.
Temporal issues are to be concerned.
\cite{AES:Latency}

The propagation delay, i.e. time $t$ it takes for a wave of sound to travel from the speaker to the ear of the listener, over a distance of $d = 1.5\,m$, and by a speed of sound of $v = 343.2\,m/s$ at normal temperature [^stdTemp] can be calculated like this:

[^stdTemp]: The *normal temperature* is defined by the National Institute of Standards and Technology (NIST) as $20°C$ at $1\,atm$ absolute pressure.

$$
\begin{aligned}
t &= \frac{d}{v}\cdot 1\,s\\
  &= \frac{1.5\,m}{343.2\,m}\cdot 1\,s\\
  &= 0.00437\,s\\
  &= 4.37\,ms
\end{aligned}
$$

The overall latency of the synthesizer is the sum of different latency portions, introduced through a variety of components in the audio output chain.
At first there is the input latency, that is either $l_{midi}$ or $l_{osc}$ depending on the used input protocol, where the amount of $l_{osc}$ depends highly on the network connection.
Usually a wifi connection is used to connect an OSC client, like a tablet running [liine's lemur](https://liine.net/de/products/lemur/) or some other OSC capable controller application.
An ad-hoc wifi network or tethering over USB can be used to get reliable network latencies with wireless clients.

The next latency portion, $l_{dsp}$ is introduced through the synthesizers internal audio output buffer.
At the time of writing, a small buffer size of [64 samples](https://github.com/klingtnet/ytterbium/blob/master/src/main.rs#L172), or $\approx 1.3\,ms$, was used.

The audio backends output buffer adds a portion $t_{backend}$ of latency, as well.
It depends on the users buffer size settings how large the added latency is.

At last there is the propagation delay which can be neglected for usual listening distances or is near zero if headphones are used.

\resizebox{\textwidth}{!}{
\begin{tikzpicture}[auto]
\draw
	node [block](Engine){DSP Engine}
	node [block, above left of=Engine, node distance=3cm](MIDI){MIDI}
	node [block, below left of=Engine, node distance=3cm](OSC){OSC}
	node [block, right of=Engine, node distance=4cm](Soundcard){Audio Backend}
	node [block, right of=Soundcard, node distance=5cm](Speaker){Speaker}
	node [block, right of=Speaker, node distance=4cm](Listener){Listener}
;
	\draw[->](MIDI) -- node{$l_{midi}$}(Engine);
	\draw[->](OSC) -- node{$l_{osc}$}(Engine);
	\draw[->](Engine) -- node{$l_{dsp}$}(Soundcard);
	\draw[->](Soundcard) -- node{$l_{backend}$}(Speaker);
	\draw[->](Speaker) -- node{$l_{wave}$}(Listener);
\end{tikzpicture}}

# Synthesizer Basics

## Oscillators

### BLITs

### Wavetables

## Synthesizing Methods

### Subtractive

### Additive

### FM

## Envelopes

## Polyphony

# Signal Generation

## Aliasing

## FFT

## Band-Limited Wavetables

## Limiting

## Mixing/Panning

# Sound Shaping

## Filter

## Waveshaper

# Effects

## Delay

## Reverb

# Implementation Details

## Ring Buffer

## Control Input

### Lemur

![View of the piano section](imgs/lemur-piano.png){ width=100% }
![View of the oscillator contol section](imgs/lemur-oscillator.png){ width=100% }
![View of the fm section](imgs/lemur-fm.png){ width=100% }
![View of the mixer section](imgs/lemur-mix.png){ width=100% }

## Audio Output

# Outlook
<!-- TODO: rename this to future work? -->

## Optimizations

# Conclusion
